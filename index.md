
<div id="navwrap">
<div id="nav">
    <p>
      Last Update
    </p>
    <p class="small">
      
        Friday, 02. October 2020 15:01
      
    </p>

    <p>Subscriptions</p>
    <ul class="subscriptions small">
     
      <li>
        <a href="https://www.nazioneindiana.com/feed/"><img src="i/feed-icon-10x10.png"></a>
        <a href="https://www.nazioneindiana.com">Nazione Indiana</a>
      </li>
     
      <li>
        <a href="http://okfnlabs.org/blog/feed.xml"><img src="i/feed-icon-10x10.png"></a>
        <a href="http://okfnlabs.org/blog">Open Knowledge Foundation (OKFN) Labs News</a>
      </li>
     
      <li>
        <a href="http://blog.schema.org/feeds/posts/default"><img src="i/feed-icon-10x10.png"></a>
        <a href="http://blog.schema.org">schema.org News</a>
      </li>
     
      <li>
        <a href="http://blog.wikimedia.org/c/technology/wikidata/feed/"><img src="i/feed-icon-10x10.png"></a>
        <a href="http://blog.wikimedia.org/c/technology/wikidata">Wikidata News</a>
      </li>
     
     </ul>

    <p>Planetarium</p>
    <ul class="planetarium small">
      <li><a href="http://plutolive.herokuapp.com">Planet Ruby</a></li>
      <li><a href="http://viennarb.herokuapp.com">Planet vienna.rb</a></li>
    </ul>

    <p>Meta</p>
    <p class="small">Powered by <a href="https://github.com/feedreader">Pluto</a>; 
       Questions? Comments?
      Send them along to the <a href="https://groups.google.com/group/wwwmake">forum/mailing list</a>. Thanks!
    </p>
</div>
</div>


<div id="opts">
  <div style="width: 100%; text-align: right;">
   <img src="i/view-headlines.png" id="show-headlines" title="Show Headlines Only" width="24" height="24">
   <img src="i/view-snippets.png" id="show-snippets" title="Show Snippets" width="24" height="24">
   <img src="i/view-standard.png" id="show-fulltext" title="Show Full Text" width="24" height="24">
  </div>
  <div style="width: 100%; text-align: right;">
    Style |
      <a href="">Standard</a> â¢
      <a href="planet.cards.html">Cards</a>
  </div>
</div>  


# Il Rivistaio






 
## &#10;  Friday, 02. October 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [InvisibilitÃ  e autorialitÃ  a proposito di Elena Ferrante](https://www.nazioneindiana.com/2020/10/02/invisibilita-e-autorialita-a-proposito-di-elena-ferrante/)&#10;

<div class="item-body">

<div class="item-snippet">


  di Viviana Scarinci
&nbsp;
Il 29 agosto scorso in occasione dellâuscita de La vita bugiarda degli adulti in trentacinque Paesi, Elena Ferrante su Robinson rilascia una delle piÃ¹ lunghe e singolari interviste di sempre. Ferrante risponde alle domande provenienti da alcuni Paesi coinvolti dallâuscita del libro.âŠ Leggi il resto Â» 

</div>

<div class="item-content item-summary">



  <p>di Viviana Scarinci</p>
<p>&nbsp;<br>
Il 29 agosto scorso in occasione dellâuscita de La vita bugiarda degli adulti in trentacinque Paesi, Elena Ferrante su Robinson rilascia una delle piÃ¹ lunghe e singolari interviste di sempre. Ferrante risponde alle domande provenienti da alcuni Paesi coinvolti dallâuscita del libro. Dal Brasile alla Danimarca, da Shangai al Portogallo passando per Formia, traduttori, editor e librai hanno posto allâautrice domande che entrano nello specifico non solo della sua opera ma anche di alcuni aspetti inerenti allâautorialitÃ  come fondamentale presupposto del suo profilo pubblico.</p>
<p>Una delle risposte riguarda per cosÃ¬ dire la cassetta degli attrezzi di chi scrive e il tipo di lavoro di cesello che lâautrice o lâautore compie sul testo. Ã il linguaggio, cioÃš qualcosa che va oltre la sintassi, connettendosi imprevedibilmente al tema trattato, ciÃ² che definisce la resa effettiva di un testo letterario. PerchÃ© riguarda lâesclusivitÃ  della relazione tra chi scrive e chi legge (e spesso anche chi traduce e edita) ma anche lâalteritÃ  di un contenuto e lâimponderabile originalitÃ  che questo puÃ² esprimere rispetto ai parametri letterari in vigore.<br>
âLâediting piÃ¹ pericolosoâ scrive Ferrante in proposito: âÃš quello che vigila sul rispetto del canone vigenteâ. A ben guardare non Ãš unâaffermazione di poco conto se posta in relazione alla filiera editoriale che puÃ² condurre in libreria certi libri e altri no. O stravolgere lâalteritÃ  e lâoriginalitÃ  dei contenuti di libri diciamo âfuori formatoâ se vengono valutati da un punto di vista che si riferisce strettamente a un canone letterario in vigore in un Paese piuttosto che in un altro. Oppure non visti affatto quando lâottica con la quale Ãš valutato un testo si lega a prese di posizione culturali inattuali, marcatamente connotate o semplicemente disinformate.<br>
Domani 3 ottobre nellâambito della manifestazione FEMINISM 3 fiera dellâeditoria delle donne presso la Casa Internazionale delle Donne di Roma la SocietÃ  Italiana delle Letterate dedica il suo incontro nazionale del 2020 a una domanda che affonda il dito nella piaga: âInvisibili?â. Il punto di partenza Ãš il significato simbolico dellâinvisibilitÃ  di Elena Ferrante legato agli aspetti precipui dellâautorialitÃ  femminile. E soprattutto le condizioni di minore visibilitÃ , o non visibilitÃ , di questi aspetti messi a confronto con la visibilitÃ  globale che ha raggiunto lâopera di Elena Ferrante essendone espressione.<br>
Lâincontro continua sotto lâegida dello stesso interrogativo in merito alla visibilitÃ : la SocietÃ  Italiana delle Letterate punta da sempre lâattenzione sul canone letterario e la necessitÃ , oltre che il significato sociale, di unâanalisi critica puntuale e dedicata, posta in essere con strumenti finalmente adeguati allâemersione e allâassimilazione culturale dellâopera, piÃ¹ o meno, ignorata dalla critica ufficiale delle moltissime autrici (romanziere, poete, saggiste e traduttrici) del Novecento, escluse dal canone, e di quelle contemporanee non adeguatamente, o per nulla, affrontate da un punto di vista veramente libero da pregiudizi di genere.<br>
Lâincontro si concluderÃ  tematicamente convergendo sul tema della necessitÃ  della paritÃ  di genere a scuola a partire da una manualistica formativa e didattica finalmente avvertita non solo dei risultati di un lungo percorso di studi e una vasta rosa di pubblicazioni dedicate in Italia alla scrittura delle donne ma anche dei risultati degli studi accademici relativi alle politiche di genere inquadrati in un contesto nazionale e internazionale.<br>
Secondo quanto pubblicato dal sito di Rivista Studio lâ8 settembre scorso ossia a una settimana dallâuscita dellâultimo libro di Ferrante allâestero: âA giudicare dallâentusiasmo incontenibile degli articoli usciti negli ultimi giorni, sembra che il nuovo libro di Elena Ferrante, La vita bugiarda degli adulti, abbia ricevuto unâaccoglienza molto piÃ¹ calorosa negli Usa e in Uk che in Italia, dove Ãš sÃ¬ immediatamente balzato al numero uno (finora ha venduto circa 250.000 copie, Ãš stato in classifica per trentadue settimane) ma non ha ricevuto una tale quantitÃ  di accurate analisi critiche. Se le recensioni italiane tendono a riflettere sul fenomeno Ferrante in generale (non siamo abituati a un caso editoriale di questa portata: continua a stupirci), quelle inglesi e americane si concentrano maggiormente sul libro in sÃ© e sul materiale puro della scritturaâ.<br>
La questione della ricezione dellâopera di Elena Ferrante presso i suoi connazionali mette in luce alcuni problemi rispetto alla gerarchia di valori applicata da alcuni recensori allâanalisi dei libri scritti da donne e nella loro valutazione giornalistica oltre che critica. Lâevidenza di questa questione non Ãš affatto materia nuova di cimento accademico, oltre che di curiositÃ , per chi guarda al panorama letterario italiano dal di fuori.<br>
Un recente esempio ci viene fornito dal saggio di Cecilia Schwartz docente dellâUniversitÃ  di Stoccolma che sul numero 40 della rivista online The italianist pubblicata il 5 maggio del 2020 dedica un intero studio alla ricezione dellâopera di Elena Ferrante, nellâesclusivo ambito della stampa italiana prima e dopo la consacrazione internazionale dellâopera dellâautrice.<br>
Nellâabstract Schwartz dichiara: âLo scopo dello studio Ãš duplice: innanzitutto prende in esame come le opere di Ferrante siano state valutate nella stampa italiana e, dallâaltra, si propone di indagare se la ricezione italiana sia stata influenzata dalla fortuna internazionale, soprattutto quella statunitenseâ.<br>
Oltre alle recensioni sulla versione cartacea e digitale dei principali quotidiani e inserti culturali, lâautrice del saggio prende in esame gli interventi su Elena Ferrante pubblicati dai principali blog di letteratura tra cui Critica letteraria, Doppiozero e Nazione Indiana in un periodo compreso tra il 2011 e il 2014 ossia il lasso di tempo interessato dalla pubblicazione della tetralogia de Lâamica geniale in Italia ma anche dal 2012 in poi dalla pubblicazione con la traduzione di Ann Goldstein della stessa negli USA. âLâanalisi rivelaâ continua Schwartz âche la ricezione delle prime due parti della tetralogia di Ferrante ha somiglianze con gli schemi paternalistici rilevati da Williams, mentre le recensioni pubblicate dopo il successo internazionale sono piÃ¹ elogiative. Questi risultati dellâanalisi sono anche discussi in rapporto ai processi di consacrazione e canonizzazione letteraria mettendo in risalto il discorso su Ferrante in alcuni volumi recenti sulla letteratura italiana contemporaneaâ.<br>
Per inciso: a proposito di autorialitÃ  e punto di vista femminile, su Nazione Indiana ricordiamo tra gli altri lâimportante intervento di Tiziana de Rogatis dellâottobre del 2016 (ndr: presente insieme a Viviana Scarinci e Isabella Pinto allâincontro su Elena Ferrante del 3 ottobre organizzato da SIL, Leggendaria e Letterate Magazine).<br>
Tra gli altri elementi di estremo interesse che il saggio di Schwartz mette in luce, non ci sono solo quelli relativi alla deficitaria comprensione dellâimportanza dellâopera di Ferrante in Italia, in quanto Schwartz illustra anche gli orientamenti di quella critica letteraria italiana che si Ãš dimostrata fin dal principio, pubblicando articoli non accademici diffusi attraverso quotidiani e blog, bene attrezzata per la lettura di unâopera indubbiamente cosÃ¬ complessa come quella dellâautrice di cui stiamo parlando. ComplessitÃ  riferita anche al modo in cui le donne ritratte nellâopera di Ferrante si inseriscono nella storia della societÃ  del loro Paese a partire dal secondo dopoguerra. E ancora di piÃ¹ alla genealogia del romanzo italiano scritto da donne, campo che di fatto Ãš illustrato da un discreto numero di studi e pubblicazioni italiane la cui visibilitÃ  o invisibilitÃ , spesso dipende dallo stesso motivo della mancata o falsata ricezione dellâopera letteraria di Elena Ferrante presso i media italiani.<br>
Nellâarco di venticinque anni infatti la SocietÃ  Italiana delle Letterate ha elaborato e prodotto, attraverso convegni, seminari, focus delle riviste un ricco bagaglio di studi e pubblicazioni che sono state fondamentali per lâelaborazione di quella genealogia delle produzioni artistiche femminili a cui la stessa Elena Ferrante fa riferimento ne Lâinvenzione occasionale quando scrive nel capitolo âLibertÃ  creativaâ: âla posta in gioco non Ãš piÃ¹ essere cooptate allâinterno della lunga, autorevole tradizione estetica creativa degli uomini. La posta in gioco Ãš piÃ¹ alta: contribuire a rafforzare una nostra genealogia artisticaâ (p.82). Un esempio tangibile di questo contributo dato dalla SocietÃ  Italiana delle Letterate nel tempo: la collana Workshop di Iacobelli in cui testi come Oltrecanone, Dellâambivalenza, Lâinvenzione delle Personagge sono risultati fondamentali per lo studio di molte e molti che in Italia hanno approcciato lâopera di Elena Ferrante prima della sua consacrazione internazionale.<br>
Magra consolazione ma va detto: non Ãš un problema che riguarda solo lâItalia quello dellâinadeguata e lacunosa âmisurazioneâ di valore offerta dal canone letterario ufficiale, sulla falsa riga del quale muove la formazione di alcuni esponenti della stampa di un Paese. CiÃ² che a volte viene scritto, o ignorato, sulle pagine culturali dei quotidiani e su alcuni blog letterari, quando si riferisce a scritture âfuori formatoâ, come quelle che sono espressione dellâautorialitÃ  femminile, le relega ancora di piÃ¹ allâinvisibilitÃ .<br>
In un passaggio del ricco saggio di Schwartz leggiamo: âLâispirazione metodologica per questa indagine Ãš principalmente tratta dallo studio diacronico di Anna Williams sulla descrizione e la valutazione delle scrittrici nei libri di storia della letteratura svedese pubblicati nel ventesimo secolo. Sebbene lo studio di Williams copra quasi un intero secolo, mostra che lâapproccio alle scrittrici nei libri di storia della letteratura Ãš sorprendentemente statico per tutto questo periodo: tendono ad essere associate a generi di basso rango, non sono paragonate ad altri scrittori e, cosa cruciale, non sono collocate in una tradizione letteraria. In quasi tutte le storie della letteratura svedese, le poche scrittrici che effettivamente compaiono sono âstelle senza costellazioniâ, una metafora usata nel titolo svedese dello studio di Williamsâ. Cosa aggiungere. Mal comune mezzo gaudio?  </p>

  
</div>

<div class="item-footer">
   Published
   04:00 â¢
   11 hours ago

</div>

</div>

</article>








 
## &#10;  Thursday, 01. October 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Il piccolo (grande) teatro filosofico di Aldo Masullo](https://www.nazioneindiana.com/2020/10/01/il-piccolo-grande-teatro-filosofico-di-aldo-masullo/)&#10;

<div class="item-body">

<div class="item-snippet">


  
PROLOGO
di Lucio Saviani
Oggi si parla molto di dialogo, in ogni dove. Ne parlano in tanti. E anche troppo. Soprattutto i politici, quelli della burattinata politica televisiva, fatta di smorfie in camera, di voce grossa e di par condicio. Parlo io, poi parli tu, stessi minuti, stessi secondi.âŠ Leggi il resto Â» 

</div>

<div class="item-content item-summary">



  <p>âŠ</p>
<p>PROLOGO</p>
<p>di Lucio Saviani</p>
<p>Oggi si parla molto di dialogo, in ogni dove. Ne parlano in tanti. E anche troppo. Soprattutto i politici, quelli della burattinata politica televisiva, fatta di smorfie in camera, di voce grossa e di par condicio. Parlo io, poi parli tu, stessi minuti, stessi secondi. Ma quello, si sa, non Ãš dialogo, Ãš un parlare da soli a turno, senza nemmeno ascoltarsi.</p>
<p>Aldo Masullo ci ha insegnato che dialogo, prima ancora che parlare in due, Ãš pensare in due, pensare insieme.</p>
<p>Quel pensare insieme che Ãš come un viaggio verso qualcosa che da soli non si conosce e che da soli non si raggiunge.</p>
<p>Il dialogo Ãš fatto naturalmente di parola; ma, prima ancora, di ascolto. E dunque di silenzio. Ma per dialogare non basta stare in silenzio: câÃš bisogno di attenzione, apertura, e cioÃš ospitalitÃ , accoglienza di unâaltra visone delle cose. CâÃš bisogno della forza misteriosa â come per lo schiavo della caverna platonica â di chi si scioglie, di chi abbassa le difese e apre la porta di casa. Necessaria Ãš una disposizione allâapertura, allâospitalitÃ , allâaccoglienza dellâaltro. Ma soprattutto significa disporsi ad accettare lâeventualitÃ  di cambiare, non solo idea, ma di sentirsi diverso, anche diventare un altro. Proprio per questo Ãš cosÃ¬ difficile fare davvero esperienza del dialogo, e invece cosÃ¬ facile ritrovarselo cosÃ¬, come in maschera.</p>
<p>Dialogare insomma Ãš la forza di mettersi in gioco, di accettare la possibilitÃ  di ritrovarsi diverso da prima che ci si aprisse al dialogo. In cerca di quella veritÃ  che o Ãš un bene comune o non Ãš. Per questo, Aldo Masullo ci ha insegnato che la filosofia nasce con la democrazia, due facce di una stessa medaglia: la filosofia con la sua vocazione agoretica, vocazione alla piazza, allâagorÃ , alla piazza degli scambi di merci e di opinioni.</p>
<p>Il dialogo perciÃ² Ãš esperienza. Masullo ricordava sempre la parola di Hegel: la vera esperienza Ãš quella che modifica colui che la fa. Dopo unâesperienza non si Ãš piÃ¹ gli stessi di prima. E cosÃ¬ anche con il dialogo. E con il viaggio: dopo un vero viaggio non si torna mai uguali a come si era alla partenza.</p>
<p>Il Patico, termine e concetto cosÃ¬ centrali nel cammino di pensiero di Masullo, hanno origine proprio in questo: esperire, esperienza vissuta, un passare, un attraversare, passare una prova, un provare, quel vissuto che non appartiene al piano della comunicazione dei concetti e dei significati.</p>
<p>Nel libro PaticitÃ  e indifferenza Masullo si chiede quale puÃ² essere ancora il ruolo della filosofia. La filosofia, risponde, Ãš Â«saper assaporare i sapori della vita, gustare a fondo i sensi vissuti, âŠÃš la âsapienza del paticoâ ovvero, se si ricalca interamente lâetimo greco, Ãš la âpatosofiaâÂ».</p>
<p>Nei suoi lavori ricorre spesso il verbo greco&nbsp;pÃ¡skein, che significa sÃ¬ ââvivereâ, ma indica il ââvivereâ in senso transitivo. Indica cioÃš la vita come capacitÃ  di provare, avvertire, vivere lâesperienza: Â«PaticitÃ  Ãš vivere provando, vivere assaporandoÂ».</p>
<p>Ã il senso, diceva Masullo.</p>
<p>E lo diceva con il senso che ha il dialogo: che Ãš soprattutto attenzione, ascolto, apertura, (curiositas, come dicevano gli antichi), cura di sÃ©, degli altri e anche delle parole: questo lo comprendono bene tutti quelli che ricordano, di Aldo Masullo, la costruzione, la sempre felice ricerca della giusta parola, lâesposizione del pensiero durante le sue lezioni, i suoi discorsi, le conferenze. E anche i suoi dialoghi.</p>
<p>Cura delle parole come cura di sÃ© e degli altri. Attenzione e ascolto: per te, ma anche per le persone a te care, che per lui era come la stessa cosa; mi chiedeva di Luna, parlando del suo amore per i cani, e non ricordo una volta, nemmeno una, che non mi abbia chiesto di Ruzenka e lasciato i saluti per lei, fino a quellâultima telefonata che ho fatto a lui per il suo compleanno, che era anche il giorno di Pasqua.</p>
<p>La filosofia, ci ricordava Masullo, Ãš lâesercizio che ogni uomo Ãš chiamato a fare dalla sua umanitÃ  per comprendere meglio non solo se stesso, ma il rapporto tra sÃ© e il mondo, tra sÃ© e gli altri. Insomma: âConosci te stessoâ. Ma Masullo ebbe modo di chiarire una interpretazione meno nota del motto di Delfi. E cioÃš: conosci te stesso perchÃ© solo cosÃ¬ puoi sapere quale Ãš la domanda piÃ¹ giusta da fare al dio Apollo. Lo scrisse ne La libertÃ  e le occasioni. Lo chiamai subito dopo aver letto quel capitolo, ne parlammo a lungo. E io lo chiamavo proprio da Delfi, dove mi ero portato il suo libro. Ma questo glielo dissi solo alla fine.</p>
<p>Aldo Masullo ci lascia risposte che sono poi, come il lascito socratico, quelle che, contro la morte, danno vita: e cioÃš danno vita a tante domande.</p>
<p>Ã proprio in questo senso che possiamo dire che esistere Ãš un essere in dialogo.</p>
<p>Un dialogo che continua anche chi non Ãš piÃ¹ in vita: Aldo Masullo di risposte ne ha date tante, fino a quelle date ai giornali che gli chiedevano una riflessione sulla pandemia; ha dato tante risposte con le sue opere, con la sua opera: la sua vita, la sua vitalitÃ , la sua esistenza.</p>
<p>Una lezione che Ãš rimasta sempre inquietudine teoretica, pratica di libertÃ  e di filosofia come libertÃ .</p>
<p>Ricordo i pomeriggi trascorsi insieme nel suo studio, verso la metÃ  degli anni â90, perchÃ© curavo la bibliografia da aggiornare dopo quasi ventâanni per la nuova edizione del suo fondamentale libro Metafisica, che avevo letto da studente e che sarebbe diventato un libro di riferimento per i miei studenti per molti anni. Masullo ha sempre seguito con passione i destini della scuola italiana: ancora mi ricordo la sua espressione di sconcerto, di divertita amarezza, quando qualche tempo fa ebbi a dirgli che oggi spesso nei documenti ufficiali i docenti sono chiamati âfornitori di docenzaâ e gli studenti sono chiamati âutentiâ.</p>
<p>Aldo Masullo Ãš stato per decenni un maestro e una guida per diverse generazioni di studenti, professori, filosofi e un esempio prezioso della vita civile, della politica piÃ¹ nobile e della cultura del nostro Paese.</p>
<p>Ne sono ricca e sempre presente testimonianza i suoi lavori degli anni sessanta e settanta sulla âintersoggettivitÃ â e sul âfondamentoâ e gli studi sul âtempoâ, sul âsensoâ e sulla âpaticitÃ â degli anni ottanta e novanta.</p>
<p>Agli inizi degli anni novanta, Masullo aveva rappresentato un momento di grande speranza, di forza di rinnovamento, anzi di discontinuitÃ  nel panorama della politica nazionale italiana. Lo ricordiamo tutti protagonista delle âAssise di Palazzo Mariglianoâ e poi della inedita esperienza della âgiunta del sindacoâ, primo caso in assoluto in Italia. Poco tempo fa ebbe a dirmi che Napoli ama mettersi in mascheraâŠ E allora io mi ricordai di quella volta a Parigi. Alla fine di quel decennio di speranze e prime delusioni organizzai a Parigi, presso lâIstituto Italiano di Cultura, un convegno su Napoli dal titolo âPorosâ. Al convegno di Parigi invitai filosofi, scrittori e artisti partenopei. A chiudere i lavori del convegno, invitai naturalmente Aldo Masullo.</p>
<p>Lui pronunciÃ² un discorso di grande sensibilitÃ  politica e di autentica speranza per la cittÃ , per la cultura e per il futuro di Napoli. Ma soprattutto lasciÃ² stupito il pubblico italiano e parigino iniziando cosÃ¬:</p>
<p>âE per dare a voi tutti, per lo meno ai non napoletani, unâimmagine direi pre-filosofica dellâessere napoletano, dellâesistenza napoletana, vorrei evocare una figura che in genere nelle accademie filosofiche non trova posto: la straordinaria maschera di Pulcinella. (âŠ)</p>
<p>âSi Ãš fatto scuro, Lucio, ci ritiriamo?â, diceva spesso cosÃ¬, alla fine delle passeggiate che facevamo tra Pantheon, S. Eustachio e Piazza Navona negli anni del suo ultimo mandato da parlamentare, parlando di quella sua esperienza, tra vecchia passione e giovani delusioni. E io, a sentirlo parlare di crepuscolo mi ricordavo allora la salita delle rampe che mi portavano, da studente, al Cortile del Salvatore. Primi anni â80, dopo il terremoto, in una Napoli che si avviava con un disastro a vivere un decennio disastroso. In quel cortile, nella penombra di una grande aula, cominciava di mattina presto la lezione di Masullo. Lui diceva che la filosofia vive sempre nella penombra. Io in quella penombra mattutina ho imparato ad amare la filosofia. A me ora piace pensare che quelle lezioni di Masullo siano continuate e arrivate fino a questa bella piazza e al nostro dialogo di questa sera.</p>
<p>âŠ</p>
<p>âIl Parco in Mascheraâ â Rassegna a cura del Parco Archeologico dei Campi Flegrei</p>
<p>&nbsp;</p>
<p>CASTEL DI BAIA</p>
<p>MartedÃ¬ 4 agosto 2020, h. 19.00</p>
<p>PICCOLO TEATRO FILOSOFICO</p>
<p>In memoria di Aldo Masullo</p>
<p>&nbsp;</p>
<p>Un prologo e un dialogo di e con</p>
<p>PASQUALE PANELLA e LUCIO SAVIANI</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

  
</div>

<div class="item-footer">
   Published
   05:00 â¢
   a day ago

</div>

</div>

</article>








 
## &#10;  Wednesday, 30. September 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Del fraintendimento: Gruppo 93 e avanguardie](https://www.nazioneindiana.com/2020/09/30/del-fraintendimento-gruppo-93-e-avanguardie/)&#10;

<div class="item-body">

<div class="item-snippet">


  
di Bianca Coluccio
Nel maggio del 2003 lâAlma Mater di Bologna predisponeva una serie di tavole rotonde organizzate nellâambito dellâiniziativa âGruppo 63 â Quarantâanni dopoâ. Sul magazine dellâUniversitÃ  si legge che lâintento principale Ãš quello di âdimostrare che i lavori del Gruppo 63 avevano una potenzialitÃ  di sviluppo ulterioreâ.âŠ Leggi il resto Â» 

</div>

<div class="item-content item-summary">



  <p>âŠ</p>
<p>di Bianca Coluccio</p>
<p>Nel maggio del 2003 lâAlma Mater di Bologna predisponeva una serie di tavole rotonde organizzate nellâambito dellâiniziativa âGruppo 63 â Quarantâanni dopoâ. Sul magazine dellâUniversitÃ  si legge che lâintento principale Ãš quello di âdimostrare che i lavori del Gruppo 63 avevano una potenzialitÃ  di sviluppo ulterioreâ.[1] Che lâavessero Ãš indubbio, ma che i lavori del Gruppo 93 potessero ritenersi uno sviluppo della neoavanguardia aveva bisogno di una puntualizzazione precisa, che sciogliesse lâequivoco che era andato tessendosi. GiÃ  Biagio Cepollaro, nellâintervento âLa compresenza conflittualeâ, uscito su Baldus nel 1991,[2] metteva in chiaro come ritenere il Gruppo 93 unâavanguardia, alla stregua del Gruppo â63, non fosse che uno dei quattro equivoci che toccavano il gruppo.</p>
<p>Nello stesso senso si snoda lâintervento di Lello Voce a Bologna. In questa occasione Voce introduce una nuova forma di avanguardia, che non Ãš piÃ¹ âquella storica, quella neo, quella neo-neo. [âŠ] esiste poi un tipo di avanguardia particolare, che io purtroppo conosco molto bene, che Ãš lâavanguardia mio malgradoâ.[3] Lâequivoco quindi ritorna, anche a distanza di anni, e continua ad avere bisogno di chiarificazioni precise. Prima delle teorizzazioni fatte sopra i lavori del Gruppo, ci sono gli stessi autori che nella consapevolezza delle proprie posizioni non hanno mai voluto o creduto di essere un movimento dâavanguardia.</p>
<p>Lo stesso argomento aprirÃ , nel 2016, la conversazione svolta a Milano tra Angelo Petrella e alcuni poeti del Gruppo 93.[4] Si richiama ancora in gioco la questione dellâavanguardia: non solo Petrella si domanda se questa esperienza non possa venire considerata come âlâestremo canto del cigno della possibilitÃ  di fare avanguardiaâ, ma se questa considerazione non sia piÃ¹ che altro necessaria a inquadrare un fenomeno storico ponendolo a un estremo di due polaritÃ .</p>
<p>In ognuna di queste occasioni Ãš stato ribadito che i rapporti in termini di scopi ultimi e modalitÃ  del loro raggiungimento non sono sovrapponibili rispetto a quello dellâavanguardia, non se ne traggono le mosse. Posto che lâavanguardia sia impossibile a reiterarsi, ciÃ² a cui si tende Ãš piÃ¹ che altro un dialogo, poichÃ© le condizioni che hanno favorito lâavanguardia, âstorica, neo, neo-neoâ,[5] sono ormai esaurite e irreplicabili.</p>
<p>Come si spiega, dunque, questo fraintendimento? E fino a che punto Ãš normale che si verifichi lâequivoco? Ritenere avanguardia il Gruppo 93 significherebbe allo stesso tempo dover riconsiderare alcuni dei caratteri imprescindibili che cooperano alla formazione di uno statuto dâavanguardia: una condizione storico-politica favorevole, una opposizione antagonista rispetto alla tradizione tout court, un unico scopo ultimo perseguito secondo le medesime modalitÃ .</p>
<p>Riguardo lâimpossibilitÃ  di riprodurre le condizioni teoriche e storiche tipiche dellâavanguardia, Luperini prospetta lâesistenza di tre cerchi di azione concentrici.[6] CâÃš un primo cerchio nutrito da un certo numero di autori non armonizzati che compongono una resistenza sperimentale; di qui il secondo in cui tutti coloro che hanno dimostrato interesse nei confronti del Gruppo 93 hanno anche desiderato che il gruppo si tramutasse in una forma dâavanguardia, per conservare delle avanguardie il carattere oppositivo e agonistico. Un terzo cerchio, infine, Ãš quello in cui lo stesso Luperini si inserisce. In questâultimo si riconosce lâimpossibilitÃ  per unâavanguardia di ricostituirsi e si agisce sugli spazi liminari ed estremi ancora concessi dalla postmodernitÃ . In questo senso si potrebbe allora configurare una possibile risposta alla dinamica postmoderna. Spostare la propria attenzione nelle zone periferiche, operare nel segno della lateralitÃ  per rispondere alle propensioni di parificazione che connotano il postmoderno.</p>
<p>&nbsp;</p>
<p>La postura assunta dal Gruppo 93 Ãš evidentemente discosta rispetto a quella tipica delle avanguardie. Lâantagonismo, il nichilismo, il rivoluzionarismo e il terrorismo, lâautopropaganda violenta e autopubblicitaria e la prevalenza della poetica sullâopera: queste sono alcune delle caratteristiche che Renato Poggioli ritiene essere peculiari di ogni movimento dâavanguardia.[7]</p>
<p>Quale avanguardia, perciÃ², senza antagonismo? Esiste unâavanguardia che non abbia un terminus contra quem, che non mantenga un atteggiamento oppositivo, che non prenda le proprie difese? GiÃ  in questo senso, il Gruppo 93 non puÃ² essere considerato unâavanguardia. Il tempo delle opposizioni binarie e delle polaritÃ  contrastanti Ãš del tutto terminato. Il contesto in cui avviene la produzione culturale Ãš adesso votato alla velocitÃ , della stessa velocitÃ  di cui risente il linguaggio, una piega dellâestetizzazione che deve tutto alla comunicazione massmediatica contemporanea: ritenere possibile una difesa ferma e a spada tratta come quella a cui ha abituato lâavanguardia non Ãš neppure piÃ¹ auspicabile.<br>
La modernitÃ  dilagante e fluida sancisce una saturazione impossibile da far retrocedere. Ã la saturazione dellâetÃ  moderna ad aver segnato il termine dellâavanguardia. Quello che hanno rappresentato le grandi avanguardie novecentesche non puÃ² piÃ¹ venire adoperato come paradigma. Ritenere possibile nella societÃ  postmoderna un qualsiasi movimento avanguardistico significherebbe tentare di sostenere un âvelleitarismo pateticoâ.[8]</p>
<p>Tuttavia, quella che si potrebbe considerare alla stregua di unâabiura va ripensata nei termini di una presa dâatto: lâesperienza avanguardistica, storicizzata e conclusa, non puÃ² fare a meno di inserire sÃ© stessa nella tradizione. Rispetto allâavanguardia e al suo periodo, quella dialettica contraddittoria che rimaneva alla base del rapporto arte-museo Ãš diventata sterile e impossibile a realizzarsi. Lâimpasse neoavanguardistico si verificava laddove, pur ricercando un prodotto artistico che fosse incontaminato e atemporale, lo scontro con la solita logica borghese e di mercato era inevitabile: il prodotto artistico era un prodotto, appunto, che per carica innovativa, distruttiva, audacia, era in grado di superare gli altri e di batterli sul piano della concorrenza. Lâavanguardia desiderava produrre unâarte imbattibile e imbattuta e che allo stesso tempo non rimanesse impigliata nelle dinamiche di un certo stringente algoritmo.</p>
<p>Diversamente, invece, il Gruppo 93 nÃ© mira a produzioni incontaminate e atemporali, nÃ© intrattiene questo rapporto contraddittorio col mercato. Anzi, alla nascita del Gruppo, il termine dellâesperienza Ãš giÃ  stato deciso ed Ãš il nome stesso che il Gruppo si dÃ , a indicare quando arriverÃ  lo scioglimento.<br>
Ecco quindi che non si puÃ² parlare di avanguardia, almeno poichÃ© non câÃš alcun âassestamento dellâarte allâepoca della tecnicaâ.[9] Per il Gruppo 93, considerate le premesse âtemporaliâ, la possibilitÃ  di assestamento non Ãš contemplata. PiÃ¹ che lâepoca della tecnica, si fanno i conti con lâepoca della comunicazione âtelevisivaâ e massmediatica, nutrita di una velocitÃ  che non puÃ² non interessare anche la dimensione estetica.</p>
<p>In questâottica si capisce meglio come la tradizione rigettata dalle avanguardie sempre a fronte di uno sperimentalismo estenuato non soltanto viene riconsiderata ma assume la postura di chi la fa propria, di chi la adopera ai fini della propria arte. Questo spiega come nel Gruppo 93 il dialogo con la tradizione avvenga in un confronto di voci diverse e in qualche maniera rifunzionalizzate. Mentre Lello Voce annovera nel proprio canone Zanzotto, Leonetti, o âaddirittura un certo Fortini âpoliticoââ ,[10] per Biagio Cepollaro Ãš la lingua del Duecento a costituire il perno e il riferimento principale.</p>
<p>Si legge nellâintervista di Enzo Rega a Biagio Cepollaro:</p>
<p>Jacopone da Todi e il Dante piÃ¹ infernale sono i miei veri maestri. Da loro ho capito come una parola, ogni singola parola puÃ² essere a tal punto âriscaldataâ da diventare incandescente. Ho capito âlâeccessivoâ che si annida nelle consonanti, la materialitÃ  della parola, la sua capacitÃ  di attrito e di resistenza alla banalitÃ  del âpoeteseâ e dello standard. Da maestri del genere si capisce come la poesia medioevale, letta in un certo modo, si avvicini quasi allâultima poesia sonora, come la cosiddetta âtradizioneâ sia in realtÃ  â se grande â un serbatoio infinito di possibili innovazioni e ricerche.[11]</p>
<p>Il terreno comunque del Gruppo 93 Ãš la tradizione non tutta e non canonica, ma ripensata e rifunzionalizzata alla luce di una propria soggettivitÃ . Posto questo come punto di partenza, le possibilitÃ  di declinazione sono molteplici e diversificate. Inoltre, la fluiditÃ  e la soggettivitÃ  di cui si connotano i canoni, sono iscritte allâinterno di un intento preciso, di una scrupolosa volontÃ  di contaminazione. Senza questa idea di tradizione fin qui descritta, non ci sarebbe alcuna possibilitÃ  di contaminazione.</p>
<p>&nbsp;</p>
<p>Come si Ãš detto, il quadro attuale non permette piÃ¹ una separazione netta e oppositiva neanche tra lingua ordinaria e lingua poetica, tra lingua alta e bassa, altezza del linguaggio lirico e statuto periferico della parola dialettale. La contaminazione vorrebbe agire in questo senso come una ibridazione. CiÃ² che si vuole raggiungere Ãš un momento infine creolo in cui allo scambio Ãš seguita una successiva fusione, secondo un movimento che abbandona ogni pretesa di alteritÃ  e affermazione per giungere a un risultato polifonico, unâarmonia mancata che parte dal suo apparente difetto per riorganizzarsi in un coro di voci e un coacervo di orecchi tesi. Contaminare ha quindi una âpronuncia pluraleâ e âproduce una terza identitÃ  che non Ãš equivalente a nessuna delle due che concorrono a formarlaâ.[12] AffinchÃ© un simile processo di contaminazione possa rendersi possibile, il rapporto con la tradizione deve necessariamente essere opposto a quello che si proponeva lâavanguardia. Mentre le avanguardie, infatti, hanno sempre operato un taglio orizzontale di separazione netta e definitiva col passato, quello che fa il Gruppo 93 Ãš, sÃ¬, operare un taglio, ma stavolta verticale, aprire una fessura, provocare la fuoriuscita dei propri riferimenti, far collimare tra loro universi svariati e disomogenei.</p>
<p>Quando su Le Figaro apparve nel 1909 il Fondation et Manifeste du Futurisme di Marinetti, la posizione era frontale e inequivocabile. Nessuna bellezza per le opere che non possiedono un carattere aggressivo: ciÃ² che non si configura come scontro e come sommossa allora non riguarda nÃ© piÃ¹ lâarte nÃ© la bellezza. Non serve a niente girarsi a guardare il passato per lâuomo che si trovi âsul promontorio estremo dei secoliâ. Dichiarata la guerra ad accademie, biblioteche, musei: uno spreco di tempo rivolgersi indietro, giacchÃ© âil Tempo e lo Spazio morirono ieriâ e per loro non câÃš posto nellââeterna velocitÃ  onnipresenteâ. Ma nessuna tra queste velleitÃ  incendiare apparterrÃ  poi al Gruppo 93. E ancora: se lâavanguardia storica Ãš senza passato, la neoavanguardia non sa, nel presente, destreggiarsi tra la logica piccolo borghese alla quale vorrebbe sfuggire e che invece la inghiotte. Il Gruppo 93 un poâ per propria volontÃ  e per contingenza storica, riesce a porsi al di lÃ  di ognuna di queste posizioni. Ã agli antipodi, infatti, rispetto al rigetto della tradizione a cui guardava lâavanguardia storica. Ha superato il vicolo cieco in cui borghesia e museo costringevano la neoavanguardia, per approdare su un nuovo terreno di comunicazione e dialogo. Terminate violenze e antagonismi avanguardistici, lo scopo a questo punto diventa sviluppare nuove possibilitÃ  di creazione di senso, rifunzionalizzare certi elementi assodati per arrivare a una letteratura creola e quindi inedita.</p>
<p>Dialetto, idioletto, pastiche</p>
<p>Una delle strategie tramite cui si determina la contaminazione Ãš lâutilizzo del dialetto. Di nuovo, le motivazioni sottostanti la scelta di un linguaggio dialettale non sono di natura nostalgica, nÃ© viene caricato in alcun modo di valenza regressiva o ancora mitica. Nellâassenza di cariche oppositive e contrastanti, le polaritÃ  interne alla lingua si trovano scariche e inefficaci, comportando una âequivalenza neutralizzante peraltro ampiamente supportata dagli interessi economico-tecnologici che presiedono alla produzione artistica di sempre piÃ¹ numerose tipologie di prodottiâ.[13] La scelta del dialetto non Ãš dettata quindi da alcuna inclinazione purista e da nessun sentimento nostalgico, quanto dalla volontÃ  di creare uno spazio letterario, poetico, che possa essere uno spazio di creazione di nuovi sensi. CiÃ² che prima si trovava ai due estremi di una divaricazione viene reso incontro proficuo, generatore di significati, terreno fertile. Non solo: la riduzione progressiva dei parlanti dialettofoni ha comportato lâevoluzione del poeta dialettale in poeta neodialettale. Vale a dire che il contenitore da cui si attinge per creare il proprio universo linguistico non Ãš piÃ¹ reale, ma virtuale. In una situazione simile, la dimensione territoriale, caratteristica della poesia dialettale, Ãš inevitabilmente spinta a riconsiderare i rapporti tra il centro e il confine della lingua.[14]</p>
<p>Niente da imputare, insomma, nÃ© a un atteggiamento elegiaco nÃ© a una torsione verso lâinfanzia linguistica. La lingua si deve muovere verso una scrittura âanti-istituzionale, anticlassica, anti-simbolista, nemica dellâio lirico gonfio dei privilegi usurpati, la quale intenda stabilire reti di relazioni piuttosto che immedesimazioni, vuol dire viaggiare in una scrittura non garantita che ha bisogno, per poter vivere, di un atteggiamento continuamente autocritico da parte dellâautoreâ.[15]</p>
<p>Si Ãš detto di come allâinterno del Gruppo non vi fossero esattamente orizzonti comuni, quanto piuttosto spazi condivisi, e di quanto su questi spazi i vari esponenti si muovessero liberamente. Il diverso utilizzo del dialetto da parte di Lello Voce e Biagio Cepollaro si puÃ² iscrivere allâinterno di questo libero movimento. Cepollaro, rifacendosi a Jameson, sostiene che il deperimento del pastiche, inteso come una delle strategie di contaminazione, sia da imputare allâinterazione mancata cui vengono sottoposti i materiali. La giustapposizione degli elementi non realizza, secondo lui, alcun apporto di senso. Diversamente, quando la fusione Ãš pressochÃ© completa e a stento si riconoscono le parti che di cui si compone lâidioletto, allora lÃ¬ si verifica un nuovo apporto di senso. Alla funzione parodica del pastiche si sostituisce una funzione âmodellizzanteâ: il luogo della periferia e del confine si pongono come centro di tutta la strategia compositivaâ.[16] Il pastiche idiolettico di Cepollaro conduce a un testo le cui ragioni sono nella compressione e nella presenza interattiva di linguaggi diversi.</p>
<p>Di contro, Voce, stabilisce la sua idea di pastiche a partire dalla volontÃ  di creare un attrito nella poesia: gli elementi che la compongono, le citazioni che ne creano lâossatura, non agiscono al di sotto del testo ma sono manifeste e identificabili. In questo contesto la citazione Ãš un modo di contaminare e dunque un momento di riflessione. E non solo la contaminazione linguistica non puÃ² essere esente dalla contaminazione stilistica, ma contaminare gli stili significa minare la consueta distinzione tra i generi di cui si rende necessario un ripensamento. Diversamente a quanto ritiene Cepollaro, Voce prende le distanze dalla posizione di Jameson:</p>
<p>Il pastiche fonda la sua identitÃ  (la sua individualitÃ ) non sulla perdita di riconoscibilitÃ  (di individualitÃ ) di ogni suo singolo elemento, come se per una sorta di abbassamento di luminositÃ  tutto si omogeneizzasse nella âsintesiâ di una stessa, opaca, patina-tonalitÃ  [âŠ]. La stroncatura feroce che Jameson riserva al pastiche sembra frutto, quanto meno, di un travisamento sineddochico che condanna il tutto per la parte, che deplora la valenza polifonica per colpire, in realtÃ , i suoi usi, per cosÃ¬ dire, retrogradi.[17]</p>
<p>&nbsp;</p>
<p>Processi allegorici, produzione di significati</p>
<p>Il momento in cui si sviluppa la riflessione sullâallegoria coincide necessariamente con quella di riflessione sul cambiamento della condizione e dellâintellettuale e dello scrittore.[18] Gramscianamente, lâintellettuale non puÃ² attendersi di avere un gruppo sociale di riferimento, che sarÃ  invece di volta in volta differente sulla scorta di quale gruppo si trovi al potere. Va da sÃ© che anche la produzione in questo contesto diventa vittima di processi di standardizzazione e appiattimento, in un modo che rende quantomai manifesto il legame di reciproca dipendenza che lega lâintellettuale â e quindi lo scrittore â alla societÃ . Dunque, la condizione intellettuale muta indifferibilmente al mutare della realtÃ  che lo circonda. I processi di reificazione, in questo momento, non sono reversibili.</p>
<p>Questo ha determinato un esaurimento delle possibilitÃ , per il simbolo, di rendersi produttore di significati. Posta una definizione di simbolo come rappresentazione di un valore sul supporto di un corpo transeunte, Ãš nel simbolo che il poeta poteva vedere lâuniversale sopra il particolare. Nella realtÃ  ridefinita dalle logiche moderne, il poeta deve invece cercare il particolare in relazione allâuniversale.</p>
<p>CioÃš a dire che si Ãš creata una opposizione tra lâatteggiamento del vedere e lâatteggiamento piÃ¹ attivo del cercare, oltre che un rovesciamento del processo metonimico. Il simbolo dimostra lâuniversale adoperando sÃ© stesso. Cosa si sostituisce al simbolo? Lâallegoria, che, al contrario, trae le proprie forze da una volontÃ  di indagine e riflessione. CosÃ¬ il simbolo offre una veritÃ  data e iscritta nel corpo dellâoggetto, mentre lâallegoria accusa una distanza da colmare tra universale e particolare. Al mutare del simbolo in allegoria muta anche la narrazione, anzi scalzata dalla descrizione. Lâimpadronirsi di una pratica allegoria Ãš allora da ricondurre allâinterno di un programma di ricerca poetica. Abbandonati i lirismi e messe al bando le mere nostalgie, preso atto di un postmoderno che depotenzia ogni conflitto possibile in favore di una generale neutralizzazione, la letteratura diviene essa stessa allegoria, poichÃ© costituita di una rete di scambi e interrelazioni a partire dalle quali si determinano sensi diversi. Per dire meglio, i significati di un testo letterario diventano sempre relativi agli scambi interni che per allegoria, per dialetto, idioletto o pastiche, producono un senso che rende attivissimo il testo.</p>
<p>&nbsp;</p>
<p>In conclusione, un progetto di stampo avanguardistico, stanti tutte le premesse di cui si Ãš discusso, Ãš di fatto impossibile a verificarsi. I presupposti che costituivano le basi per la nascita e la durata di unâavanguardia sono scomparsi: non câÃš piÃ¹ alcun agonismo da esercitare nei confronti dâuna certa tradizione, poichÃ© lâavanguardia stessa Ãš entrata a pieno titolo a far parte di quella tradizione che ripudiava â lâavanguardia Ãš diventata unâarte da museo. Quei conflitti che rendevano fervente il discorso intorno allâavanguardia e al suo interno sono stati livellati dallâavvento del postmoderno, da non intendersi piÃ¹ come ideologia ma come momento storico dato. Laddove le avanguardie hanno sempre tentato un rovesciamento della tradizione, il Gruppo 93 ha superato la dialettica che le riguardava: piuttosto che trascinare sfibrando un discorso che in veritÃ  non ha piÃ¹ possibilitÃ  di durare, si avanza seguendo un movimento che parte dalla periferia della lingua, dai margini. In questo modo âA unâopposizione dialettica interna al centro si oppone una conflittualitÃ  fondata sullo spostamento; allâantagonismo frontale delle avanguardie segue una letteratura della lateralitÃ , giocata sullo scarto, che sottolinea [âŠ] uno sforzo di non appartenenzaâ.[19]</p>
<p>Se questi sono i presupposti che hanno governato il Gruppo 93, la possibilitÃ  che si trattasse di una nuova e ultima avanguardia puÃ² dirsi inconsistente. Ciascuna delle poetiche che nasce sulla base di tali premesse, trova poi, nel proprio autore, un suo modo unico di declinazione. Quello che si presenta come un gruppo, di fatto, non funziona esattamente come un gruppo: non fa fronte comune e non trova la propria forza in una coesione interna e viscerale. Lâidea Ãš piÃ¹ quella di un laboratorio o di una rete, di un organismo che funziona nella cooperazione, ovvero di una letteratura che funziona nella contaminazione.</p>
<p>A conclusione riporto alcune parole del Luperini di Un confronto tra posizioni diverse, significative e chiarificatrici rispetto allo scopo del discorso affrontato fin qui, contenute nellâantologia âGruppo 93 la recente avventuraâ.</p>
<p>Ricordava ieri Sanguineti comâÃš nata la letteratura: non diceva come competenza opposta ad altre competenze, non come la competenza di chi lavora il ferro Ãš opposta alla competenza di chi lavora la seta, Ãš nata invece come competenza opposta allâincompetenza, come sapere e potere separati. Nel ritorno alle origini, e spesso non manca la O maiuscola, câÃš il ritorno anche a questa origine. [âŠ] Se niente Ãš letteratura non vuol dire che Ãš finita la letteratura, ma che forse Ãš possibile solo una letteratura di secondo grado, sonda il vuoto e il nulla che la circonda e di cui essa fa parte. [âŠ] La letteratura di secondo grado Ãš unâallegoria della ricerca di senso.[20]</p>
<p>&nbsp;</p>

<p>[1] magazine.unibo.it/calendario/2003/05/08/gruppo63?d=2003-05-08.</p>
<p>[2] Biagio Cepollaro, La compresenza conflittuale. Quattro equivoci sintomatici sulle vicende del Gruppo 93, in Â«BaldusÂ», anno II, n. 1, agosto 1991.</p>
<p>[3] Lello Voce, Il postmoderno Ãš nostro: giÃ¹ le mani!, www.lellovoce.it/Il-Postmoderno-e-nostro-giu-le. Il testo Ãš la trascrizione di Voce del proprio intervento al convegno di Bologna. Gli atti del convegno sono contenuti in AA.VV., Il Gruppo 63 quarantâanni dopo, a c. di Renato Barilli, Fausto Curi, Patrizia Cuzzani e Niva Lorenzini, Bologna, Pendragon, 2005.</p>
<p>[4] La conversazione tra Angelo Petrella e alcuni poeti del Gruppo 93 svoltasi nellâambito di Tu se sai dire dillo (2016) Ãš reperibile al link www.youtube.com/watch?v=VBQsVi36reo&amp;t=2925s.</p>
<p>[5] Lello Voce, il postmoderno Ãš nostro: giÃ¹ le mani!, cit.</p>
<p>[6] Romanzo Luperini, Un confronto tra posizioni diverse, in Â«AlfabetaÂ», n. 69, 1985.</p>
<p>[7] Renato Poggioli, Teoria dellâarte dâavanguardia, Bologna, il Mulino, 1962. Quello citato non Ãš lâelenco completo che Poggioli stila. Di fatto, poichÃ© alcune tra le caratteristiche individuate potrebbero essere opinabili (si parla ad esempio di gratuitÃ  del fine, che tuttavia non Ãš compatibile con lâavanguardismo russo), solo quelle insindacabili sono state citate in questo testo.</p>
<p>[8] Remo Ceserani, Trentâanni dopo, una convitata di pietra di nome Avanguardia, in il manifesto, 1993. Il documento Ãš disponibile online allâindirizzo:</p>
<p>www.cepollaro.it/rastam2.htm#Remo%20Cesarani,%20Trentanni%20dopo,%20una%20convitata%20di%20pietra%20di%20nome%20Avanguardia,%20Il.</p>
<p>[9] Franco Fortini,&nbsp;Verifica dei poteri: scritti di critica e di istituzioni letterarie, vol. 354, Torino, Einaudi, 1989.</p>
<p>[10] Lello Voce, Il postmoderno Ãš nostroâŠ, cit.</p>
<p>[11] Enzo Rega, dallâintervista a Biagio Cepollaro Oltre il postmodernismo: la parola come esperienza del caos, in Â«Quaderni RadicaliÂ», anno XVI, nn. 33/34, aprile-settembre 1992.</p>
<p>[12] Lello Voce, Appunti di dinamica dellâibrido, Â«BaldusÂ», anno II, n. 1, 1991. Il documento Ãš disponibile allâindirizzo www.lellovoce.it/Appunti-di-dinamica-dell-ibrido.</p>
<p>[13] Biagio Cepollaro, La conoscenza del poeta: metamorfosi del realismo, Â«BaldusÂ», anno II, n. 1, 1991. Il documento Ãš disponibile allâindirizzo www.cepollaro.it/nuova_pagina_50.htm.</p>
<p>[14] Cfr. Biagio Cepollaro, Idioletto, in PerchÃ© i poeti?, disponibile al link www.cepollaro.it/new_page_2.htm pp. 14-16.</p>
<p>[15] Cfr. Mariano Baino, Biagio Cepollaro, Lello Voce, A proposito delle Tesi di Lecce, Â«BaldusÂ», n. 0, 1990, consultabile online allâindirizzo www.absolutepoetry.org/L-editoriale-del-n-o-0-e-i.</p>
<p>[16] Biagio Cepollaro, La conoscenza del poetaâŠ, cit.</p>
<p>[17] Lello Voce, Appunti di dinamica dellâibrido, cit.</p>
<p>[18] Per questa breve ricognizione sullâallegoria e il Gruppo 93 mi rifaccio in particolare al capitolo V del lavoro di Angelo Petrella Avanguardia, postmoderno e allegoria: teoria e poesia nellâesperienza del Gruppo â93, Edizioni Biagio Cepollaro, 2007.</p>
<p>[19] AA. VV., Gruppo â93: la recente avventura del dibattito teorico letterario in Italia, a cura di Filippo Bettini e Francesco Muzzioli, Lecce, Piero Manni, 1990, p. 13.</p>
<p>[20] Romano Luperini, Un confronto tra posizioni diverse, in AA. VV. Gruppo â93: la recente avventuraâŠ cit., 1990, pp. 38-39.</p>

  
</div>

<div class="item-footer">
   Published
   06:00 â¢
   2 days ago

</div>

</div>

</article>








  
- - -



<article class="item">




### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Angelo Ferracuti: âMario, non ci resta che lâamoreâ](https://www.nazioneindiana.com/2020/09/30/angelo-ferracuti-mario-non-ci-resta-che-lamore/)&#10;

<div class="item-body">

<div class="item-snippet">


  &nbsp;

&nbsp;
Â«âNon Ãš che a me le persone interessino per fotografarle,
mi interessano perchÃ© esistono. Diversamente, il fotogiornalismo
sarebbe soltanto una sequenza di scatti senzâanimaâ, dicevaâŠ&nbsp;Â»
&nbsp;
Mario, non ci resta che lâamore&nbsp;di&nbsp;&nbsp;Angelo Ferracuti -dedicato alla figura di Mario Donderoâ&nbsp;Ãš il nono libro dei&nbsp;Cervi Volanti, la collana di scritture poeti

</div>

<div class="item-content item-summary">



  <p>&nbsp;</p>
<p>âŠ</p>
<p>&nbsp;</p>
<p>Â«âNon Ãš che a me le persone interessino per fotografarle,</p>
<p>mi interessano perchÃ© esistono. Diversamente, il fotogiornalismo</p>
<p>sarebbe soltanto una sequenza di scatti senzâanimaâ, dicevaâŠ&nbsp;Â»</p>
<p>&nbsp;</p>
<p>Mario, non ci resta che lâamore&nbsp;di&nbsp;&nbsp;Angelo Ferracuti -dedicato alla figura di Mario Donderoâ&nbsp;Ãš il nono libro dei&nbsp;Cervi Volanti, la collana di scritture poetiche che curo insieme a&nbsp;Giuditta Chiaraluce&nbsp;allâinterno del progetto&nbsp;Edizioni Volatili.</p>
<p>Libri come laboratori, primi confronti, materie pensanti, montaggi e scavi&nbsp;attraverso&nbsp;la carta; libri senza profitto, in tiratura limitata (esoeditoria), evidenti nella loro invisibilitÃ  e indirizzati a chi saprÃ  ospitarne lâimplicita consegna; libri&nbsp;col solo intento di essere vigilie per una geografia del&nbsp;dopo-diluvio.</p>
<p>Pubblico qui alcune pagine in anteprima,&nbsp;insieme a un estratto dal testo.&nbsp;Le partiture visive e i segnalibri sono di&nbsp;Giuditta Chiaraluce. Il ritratto fotografico Ãš un contributo di Marco Cruciani.</p>
<p>âŠ</p>
<p>&nbsp;</p>
<p>Un giorno eravamo insieme a Milano per incontrare Giovanni Pesce, lâeroe della resistenza italiana nella sua casa di Piazza Bonomelli. Era una giornata molto afosa in una Milano semideserta. Mario era arrivato con una bottiglia di prosecco e una vaschetta di gelato, le macchine fotografiche in spalla, e proprio il giorno dopo sarebbe partito per la Russia per realizzare un reportage con il giornalista Astrit Dakli sul post-comunismo, âI rifugi di Leninâ. Ci aveva accolto Â«la compagna SandraÂ», ovvero sua moglie Onorina, in questo appartamento buio dove avevamo conversato per un paio dâore. Volevo da Pesce una testimonianza su Giuseppe Di Vittorio, Nicoletti, per il libro che stavo facendo con Mario, âDi Vittorio a memoriaâ, commissionatoci dalla Cgil, che incontrÃ² prima a Guadalajara e poi a Ventotene. Mi aspettavo un racconto vivido, pieno di aneddoti, come piacciono a me. Quelle piccole storie che messe tutte insieme fanno la Storia. Invece lo trovai stanco, quasi senza piÃ¹ voglia di raccontare, si limitava a rispondere lâessenziale, poche frasi significative ma brevi.</p>
<p>Mario, dopo averli riempiti di attenzione e di affetto, mostrando loro le sue foto scattate proprio in Spagna, una delle sue ripetute ossessioni, chiese se potesse fotografarli. Eravamo in un tinello buio, la poca luce arrivava dalla portafinestra che dava sul balcone, faceva molto caldo, e loro due si misero uno accanto allâaltro in attesa che scattasse, come una coppia di anziani qualunque nel tinello di un appartamento.</p>
<p>Pensavo venisse fuori una foto troppo scura, e temevo per il nostro libro che avrebbe perso una voce importante. Invece, quando dopo qualche mese Mario mi mostrÃ² la foto mâimpressionÃ² moltissimo quel ritratto, e anche oggi continua a colpirmi. Lui aveva visto in macchina quello che io non ero riuscito a vedere, e che tutto quel tempo empatico era riuscito a creare, cioÃš la bellezza nuda di due persone giuste della storia, illuminate da una luce che le rendeva umanissime.</p>
<p>&nbsp;</p>
<p>âŠ</p>
<p>âŠ</p>
<p>&nbsp;</p>
<p>Angelo Ferracuti&nbsp;Ãš nato nel 1960. Ha pubblicato&nbsp;Attenti al cane&nbsp;(Guanda, 2000),&nbsp;Le risorse umane&nbsp;(Feltrinelli, 2006, Premio âSandro Onofriâ),&nbsp;Viaggi da Fermo&nbsp;(2009),&nbsp;Il costo della vita&nbsp;(Einaudi, 2013, Premio âLo Stranieroâ),&nbsp;Andare, camminare, lavorare&nbsp;(Feltrinelli, 2015),&nbsp;Addio&nbsp;(Chiarelettere, 2016). Scrive su âil manifestoâ, âLa Letturaâ del âCorriere della Seraâ, âIl VenerdÃ¬â di Repubblica, e collabora con Radio Tre.</p>

  
</div>

<div class="item-footer">
   Published
   05:00 â¢
   2 days ago

</div>

</div>

</article>








 
## &#10;  Tuesday, 29. September 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Da Scurau](https://www.nazioneindiana.com/2020/09/29/giuseppenibali/)&#10;

<div class="item-body">

<div class="item-snippet">


  di Giuseppe Nibali
Ultima voce chiama il sangue.
Campo cruento gli uomini, altro sangue per le donne
Ãš il giorno. Tutti sono convocati, vecchi e nuovi
viventi aspettano un gesto per sbranarsi. Il rivolo
aspettano, verticale sullo sterno, il morituri stabilito
dalla nascita, nella nascita futura rivelato.âŠ Leggi il resto Â» 

</div>

<div class="item-content item-summary">



  <p>di Giuseppe Nibali</p>
<p>Ultima voce chiama il sangue.</p>
<p>Campo cruento gli uomini, altro sangue per le donne<br>
Ãš il giorno. Tutti sono convocati, vecchi e nuovi<br>
viventi aspettano un gesto per sbranarsi. Il rivolo<br>
aspettano, verticale sullo sterno, il morituri stabilito<br>
dalla nascita, nella nascita futura rivelato. Ã tempo<br>
adesso per il sesso tra gli attori, gambe nude, lividi,<br>
dai piedi fino allâano serpi, piaghe fili lo sfondo fuori<br>
anche case, molte, come in cerca vergognosa della luce.<br>
Altro mai, nemmeno nella voce, nella voce ultima</p>
<p>**</p>
<p>due mesi, niente. La cittÃ  Ãš andata avanti<br>
verso il mondo universo. Vi colpisce fino<br>
ai buchi dentro i nervi fino ai chiodi tra le<br>
costole. La cittÃ  ha dato germe. Resistete <br>
non muovete il braccio: respirare restare <br>
come gli uomini che sono uomini due volte, <br>
una per lâorigine lâaltra per il tempo. </p>
<p>**</p>
<p>Corpi cavi enormi, gonne e questi figli come squarcio.<br>
Crolla la religione, MeroÃš, di chi conosce il tormento<br>
di giocare fino al buco dellâabisso; lo sgravo che ricordi<br>
gli spruzzi di merda sul lenzuolo e dentro lâamigdala<br>
appena lavati macelli, vene scure, osiamo dire:</p>
<p>Cattedrale vuota lâulivo schiacciato contro il greto<br>
i rami le foglie lo schianto lo scantu della scorza<br>
materna sul petto. Matriarcato dei giochi lâikea<br>
i segni, questi, del nuovo potere; parola della madre.</p>
<p>SÃ¬, siamo la madre. La morte la morte. La morte.</p>
<p>**</p>
<p>Dal lato filtra lâacquenere nel cemento, passa rifugi antiaerei, tracce di ferrovia. Ai liquami arriva, alle ossa degli antichi. Di qua un nuovo cimitero: cavi molti, un prato. Per lo scopo i piedi premono sul vetro, le mani stanno in preghiera. Mio e comune il giorno in cui ho pisciato via dalla fica il flare, il colpo del sole sulle labbra. Un silenzio primitivo, il viso Ãš morto, non vedi? Le strade, anche le strade, le gallerie come arterie di donna, le vedi? Le sorveglia unâaltra volontÃ . Allora nulla si Ãš sfatto da quanto siamo, non hai da cercare, nÃ© manca in TV di guardare i fiati sfiniti degli amanti, il collo che si curva di un airone. CosÃ¬ Ãš fino alla matrice, allorchÃ© del maschio e della femmina farete un unico essere sicchÃ© non vi sia piÃ¹ nÃ© maschio nÃ© femmina. CosÃ¬ Ãš fino alla matrice, alla prima carne strappata da uno stomaco.</p>
<p>**</p>
<p>Che bestia sei. Che bestia mentre aspetti col muso lâacqua<br>
battere sul dorso e le zampe arrivare alla nuca, mentre latri<br>
allâerba che spacca in giardino il pezzo vicino di cemento.<br>
Dalle foglie ritorna il grecale, la pioggia passa dalla feritoia<br>
nella casa, dai tendoni che coprono i raggi. Questo e di come<br>
ci siamo dimenticati, di come Ãš successo in fretta. Tenendoti<br>
tu ai miei fianchi io alla maglia stesa accanto. Ora Ãš la mossa.<br>
Fermo. La Bugonia. Solleva le mani dai fianchi, la mossa che<br>
faccio col culo. Svella piano la carcassa mia dalle labbra,<br>
la carcassa qui esplosa, il suo fegato emerso dalle piume.</p>
<p>**</p>
<p>Vi seguirÃ  il male dietro lâedera, e di sopra,<br>
sul balcone in lamiera che avete per rifugio.<br>
Non Ãš il tempo delle corse alla ringhiera<br>
mentre lo sfondo si disossa, e passa dallâarco delle vie<br>
per la montagna. Ã morto anche il vecchio prete<br>
di Ragalna, per la fine del suo giorno una domenica.</p>
<p>ChissÃ  che luce vi assale lÃ¬ dai tetti, dove il sole si<br>
inurba coi pastori fra i negozi e che fatica morire<br>
anche voi nella chiesa col barrito alto della fiera.<br>
Qui nel lontano la nebbia muove la pianura<br>
sopra i ponti, dalla miseria di colline, altre volte<br>
fuori alla finestra si alza lo scheletro di un albero.</p>
<p>**</p>
<p>Sgruma tutto lâosso, spolpa le parti del petto non ti fermare<br>
nei calli; tu scheggia gli incisivi sullâosso. Questo amore<br>
con la carne ci ha fatti bestiame umano di denti e radice<br>
di pianta tutta antica tutta crudo inverno. Sgruma lâosso<br>
anzi ascolta: un fischio convoca allâoggi i cadaveri nati<br>
sotto lâorrore delle dita.</p>
<p>Anche la donna, Vera, che pare tua figlia, la donna che guarda<br>
non un punto ma le case, Ãš carne putrefatta giÃ  nellâatomo<br>
nel ribosio Ãš carne di rovina, carne rischiarata dai fotoni<br>
al virginale. Non le mani guarda ma la felce cresciuta<br>
sul buio, che il buio tormenta col suo verde. Sui tasti supera<br>
comuni di uomini in trofallassi. Crea suono.</p>

  
</div>

<div class="item-footer">
   Published
   05:00 â¢
   3 days ago

</div>

</div>

</article>








 
## &#10;  Monday, 28. September 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [La moltiplicazione del Signor Distruggere](https://www.nazioneindiana.com/2020/09/28/la-moltiplicazione-del-signor-distruggere/)&#10;

<div class="item-body">

<div class="item-snippet">


  di Helena Janeczek
Abbiamo un problema, un problema culturale e politico.
Lâonnipresenza del linguaggio misogino (misogino e non piÃ¹ solo maschilista) che nello spazio dei media (social e tradizionali) Ãš diventato da tempo la norma della violenza verbale.
Normale il tweet di Massimiliano Parente â lâennesimo â che usa come sinonimo di âminchiaâ il cognome di Michela Murgia, mentre a lei

</div>

<div class="item-content item-summary">



  <p>di Helena Janeczek<br>
âŠAbbiamo un problema, un problema culturale e politico.<br>
Lâonnipresenza del linguaggio misogino (misogino e non piÃ¹ solo maschilista) che nello spazio dei media (social e tradizionali) Ãš diventato da tempo la norma della violenza verbale.<br>
Normale il tweet di Massimiliano Parente â lâennesimo â che usa come sinonimo di âminchiaâ il cognome di Michela Murgia, mentre a lei pare cosa di un altro mondo che una rivista americana  la difenda da un troll fascistoide. O Marco Gervasoni che, qualche settimana prima, commentava la copertina dedicata a Elly Schlein con âma questa Ãš nâomo?â<br>
Lo âscandaloâ della modella Armine Harutyunyan scoppiato a un anno dalla sfilata di Gucci e divampato solo in Italia. Le valanghe di veleno su Greta Thunberg, ricorrenti come le anomalie meteorologiche causate dal cambiamento climatico.</p>
<p>Ogni volta che Vittorio Sgarbi urla âtroia, puttanaâ a favore delle telecamere, con i video che poi diventano virali perchÃ© un sacco di ragazzi lo trova divertente, come se fosse un rapper blastatore o il âSignor Distruggereâ con il suo milione di seguaci.<br>
Ecco, il problema Ãš la moltiplicazione â metaforica e reale â  dei âSignor Distruggereâ.<br>
Il fatto che le sparate di uno scrittore o di un docente universitario mirino a una visibilitÃ  equiparabile a quella del personaggio social, per non dire a quella di Sgarbi, Feltri, Cruciani.<br>
E che quindi sarebbe doppiamente raccomandabile che la donna sotto attacco non rispondesse: per non concedere la visibilitÃ  cercata allâattaccabrighe e, oltretutto, perchÃ© le donne non dovrebbero mai abbassarsi a certi toni.<br>
Le donne dovrebbero stare salde come le querce nelle tempeste di shitstorm, ringraziare se gli tocca solo qualche âcessaâ, rispondere semmai con pacatezza e pazienza dialogante.<br>
Cosa che, in realtÃ , non le mette al riparo. Si Ãš visto recentemente con le polemiche intorno a Vera Gheno, la sociolinguistica da cui il presidente dellâAccademia della Crusca, addirittura, si Ãš sentito in dovere di dissociarsi. Vera Gheno ha spiegato fino alla nausea che non pensa affatto di imporre dallâalto lo âshwaâ inclusivo perchÃ© non si cambia cosÃ¬ una lingua e la cultura che veicola. Eppure si Ãš beccata una marea di insulti e dileggi sessisti, reazione incomparabile alle critiche riservate, per esempio, a Federico Faloppa che si occupa di simili tematiche.<br>
Per Vera Gheno, oltretutto, non vale neppure che a scatenare lâaggressivitÃ  sia stata âlâiperesposizioneâ che rende tutti piÃ¹ appetibili per gli odiatori. Con le donne, appunto, scatta anche se non sei âmediaticaâ. In tutti casi, comunque, le donne non vengono prese di mira per ciÃ² che sterotipicamente rappresentano in base alle proprie scelte (buonista, sinistrato, radical-chic ecc.) ma per ciÃ² che non hanno scelto di essere, riducendole a corpi e emotivitÃ  âuterinaâ.<br>
In un contesto del genere Ãš fallace concentrare lâattenzione sul caso del giorno, far valere simpatie, antipatie, scusanti, distinguo. Questi attacchi possono toccare a qualunque donna dica o faccia â anche inconsapevolmente â qualcosa di âsbagliatoâ o, se siete uomini, alle vostre sorelle, madri, figlie, compagne, colleghe, amiche.<br>
Infine si tratta di dinamiche che investono la spazio pubblico, la polis virtuale che non Ãš il bar sotto casa, anche se fa di tutto per somigliarvi. Una dimensione dove non câentra nulla il giudizio sul valore letterario (o accademico) di ciÃ² che hanno pubblicato i contendenti, nÃ© le relazioni private tra le persone.</p>
<p>ps. Non linkare tweet, video e pagine criticate Ãš naturalmente una mia scelta.  </p>

  
</div>

<div class="item-footer">
   Published
   12:39 â¢
   4 days ago

</div>

</div>

</article>








  
- - -



<article class="item">




### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Le forme dellâamore](https://www.nazioneindiana.com/2020/09/28/le-forme-dellamore/)&#10;

<div class="item-body">

<div class="item-snippet">


  di Matteo Quaglia
&nbsp;
Mamma diceva sempre che non esiste solitudine, per chi non apprezza la compagnia. Secondo lei, per soffrire davvero, era necessario conoscere le alternative.
PapÃ  era un tipo piÃ¹ pragmatico. Un campione dellâevoluzionismo darwiniano. Non solo perchÃ© ha modificato la propria caratura umana in modo direttamente proporzionale al lievitare della sua influenza nellâApparato, ma 

</div>

<div class="item-content item-summary">



  <p>di Matteo Quaglia</p>
<p>&nbsp;</p>
<p>Mamma diceva sempre che non esiste solitudine, per chi non apprezza la compagnia. Secondo lei, per soffrire davvero, era necessario conoscere le alternative.</p>
<p>PapÃ  era un tipo piÃ¹ pragmatico. Un campione dellâevoluzionismo darwiniano. Non solo perchÃ© ha modificato la propria caratura umana in modo direttamente proporzionale al lievitare della sua influenza nellâApparato, ma anche perchÃ© Ãš venuto a patti con lo stravolgimento di una vita, pianificata, fino a quel momento, al minimo dettaglio. Questo non significa che sia stato un cattivo padre. Non uno di quelli a cui si chiede di leggerti la storia della buonanotte. Non uno di quelli che ti accompagnano al corso di danza, o al cinema. Il suo pragmatismo lo ha guidato lungo le nostre vite con lâostinazione del sordo. Ha chiuso gli occhi ed Ãš andato avanti per la sua strada.</p>
<p>Lâultimo ricordo che ho, di lui in vita, Ãš di questa mattina. Un uomo con la cornetta in mano e le gambe di burro. Quando hanno telefonato per dirci che mamma era morta, la faccia di papÃ  ha assunto lâespressione di certi quadri di Courbet. Non ha versato una lacrima e non ha detto una parola. Si Ãš chiuso nel suo studio e da quel momento, per lui, ho smesso di esistere.</p>
<p>Sebbene fossimo preparati alla notizia, la scomparsa di qualcuno che ami Ãš sempre un pugno su per il culo.</p>
<p>PapÃ  si Ãš impiccato senza lasciare nemmeno una lettera, o unâaltra forma di addio. Lâennesima dimostrazione dellâeconomicitÃ  di un certo funzionalismo.</p>
<p>Quando ho bussato e non ho ricevuto risposta, ho inspirato e ho chiuso gli occhi. Ho aperto la porta del suo studio ed era lÃ¬, appeso, con una specie di ghigno che ricordava un sorriso. Lâho tirato giÃ¹ e lâho adagiato sul tappeto. Mio padre era un uomo compatto, ma non avrei mai detto che pesasse cosÃ¬ tanto. Era cosÃ¬ compatto che, se mi ha voluto bene, Ãš solo perchÃ© amava mamma. Il suo sentimento, nei miei confronti, una forma di rispetto per la donna che amava cosÃ¬ tanto.</p>
<p>&nbsp;</p>
<p>CosÃ¬, in un colpo solo, ho perso mamma e papÃ . Ho sempre creduto che questo genere di coincidenze capitassero solo nei&nbsp;b movies, o in seguito a qualche cataclisma. La veritÃ  Ãš che la vita sa essere piÃ¹ grottesca di ogni finzione o evento naturale. La veritÃ  Ãš che papÃ  non mi ha mai voluto e, dopo la pensione, mamma era il suo unico motivo di vita. Quando ti dicono che si puÃ² imparare a voler bene alle persone, beâ, Ãš una grande cazzata. Imparare non Ãš da tutti. Voler bene non Ãš da tutti. Dicono che si possa imparare dai propri sbagli, ma non Ãš sempre vero. Ho capito che mio padre si era sempre adattato a tutto, perchÃ© non era in grado di adeguarsi a niente.</p>
<p>Dopo aver tirato giÃ¹ papÃ  e avergli dato quel minimo di compostezza cui aveva sempre aspirato in vita, ho chiamato il portavoce dellâApparato per dargli la notizia. Gli ho detto che papÃ  era morto dâinfarto. Gli ho detto che avrei utilizzato la Macchina, quel giorno stesso, perchÃ© avevo una cosa da fare. Il portavoce dellâApparato ha risposto indicandomi un indirizzo e unâora precisa. Ha detto che mi avrebbero aspettato lÃ¬, con la Macchina e le condoglianze.</p>
<p>&nbsp;</p>
<p>Mamma mi raccontava sempre che, quando ha scoperto di essere in attesa, ha trascorso un pomeriggio pensando al nome da darmi. PapÃ  non le Ãš stato molto dâaiuto. PiÃ¹ per questione di volontÃ , che per mancanza di immaginazione. Se avesse conosciuto le conseguenze che il parto avrebbe avuto sulla salute di mamma, papÃ  lâavrebbe costretta ad abortire. Non voglio dire che, cosÃ¬ facendo, avrebbe risolto due problemi in uno, ma insomma.</p>
<p>Il punto Ãš che papÃ  amava mamma e ha accettato la sua volontÃ  di diventare madre e di consacrare quellâerrore in una forma di amore. Ha accettato la mia nascita come un moscerino che ti si infila nellâocchio.</p>
<p>Mamma mi ha piÃ¹ volte detto che non sono stata proprio il frutto di uno sbaglio, perchÃ© non Ãš mai proprio cosÃ¬. Ha ripetuto che, se anche sono stata uno sbaglio, sono lo sbaglio che lâha resa piÃ¹ felice. Il nome che mamma ha deciso di darmi, quando sono nata, Ãš Mia.</p>
<p>Ho raggiunto lâindirizzo allâora indicata e ho trovato gli uomini dellâApparato ad attendermi, i volti scuri di chi si sforza di dimostrare dolore e comprensione. Ci sono state strette di mano e parole di circostanza. Mi hanno chiesto se sapessi come utilizzare la Macchina. Ho detto di sÃ¬. Mi hanno detto che non ci sarebbe stato bisogno di riportare la Macchina indietro, perchÃ© la Macchina Ãš sempre ovunque.</p>
<p>&nbsp;</p>
<p>CosÃ¬ sono arrivata in quellâaltro indirizzo preciso, a quellâora precisa di diciannove anni fa. Mamma mi ha raccontato che lei e papÃ  si sono conosciuti un pomeriggio piovoso, in un vecchio caffÃš. A mamma era caduto il libro che stava leggendo allâepoca, papÃ  lâaveva raccolto e da lÃ¬ era nato tutto quanto. Da lÃ¬ ero nata io.</p>
<p>Vedo mamma. Ã giovane, una ragazza con la spensieratezza di chi ama le giornate di maggio. Il libro sotto il braccio. Mi hanno sempre detto che sono mia madre da giovane, ora ne ho la prova. Forse Ãš questo il motivo per cui, papÃ , non ha mai utilizzato la Macchina per cambiare il corso degli eventi. Per cancellarmi dalla storia. Forse, questa Ãš la stata la sua personalissima forma di amore.</p>
<p>Ora Ãš il mio turno per fare ciÃ² che va fatto.</p>
<p>&nbsp;</p>
<p>Si dice che si impara dai propri sbagli, ma a volte anche gli sbagli imparano. Ed Ãš per questo che siamo tutti e tre dentro il caffÃš e fuori pioviggina e siamo tre sconosciuti dal destino intrecciato. Da giovane, papÃ  sembra meno duro, meno intagliato nel legno. Forse, Ãš diventato ciÃ² che Ãš diventato in seguito alla mia nascita. A vederlo, ora, pare impossibile che tra diciannove anni si appenderÃ  per il collo.</p>
<p>Mi avvicino a mamma. Lei non mi nota. Ã appoggiata al banco del bar. Ã bellissima. PapÃ  Ãš poco distante. Anche papÃ  si avvicina, per ordinare da bere. Mamma si volta verso di me e il libro le scivola da sotto il braccio e cade al suolo, sollevando un piccolo sbuffo di polvere. Mi chino e raccolgo il libro. Lo porgo a mamma, che mi sorride. PapÃ  nota la scena e mi sorride. Poi abbassa gli occhi sul giornale che sta leggendo.</p>
<p>Mi guardo le mani. Sto iniziando a scomparire, come Marty McFly in Ritorno al Futuro. Certe volte la vita Ãš davvero come un film, a quanto pare.</p>
<p>Ho la testa ovattata, tanto che posso sentire i battiti del mio cuore farsi sempre piÃ¹ leggeri, mentre un raggio di sole bianchissimo attraversa i vetri impolverati del bar e&nbsp; mi bagna di luce. Mentre scompaio per sempre, nel pomeriggio, ripenso a quella frase di mamma, secondo cui, per soffrire davvero, Ãš necessario conoscere le alternative. Mi chiedo se papÃ  soffrirÃ . O se, senza di me, la sua vita sarÃ  piÃ¹ felice. Mamma non mi avrÃ , ma mi ha giÃ  avuta, in unâaltra vita. E vissero tutti felici e contenti, in un modo o nellâaltro, senza di me e con me.</p>
<p>I miei occhi incrociano di nuovo quelli di mamma e le labbra di mamma si aprono e mamma sembra sapere chi sono e mi dice qualcosa, che non riesco a capire, e in un attimo sono solo un ricordo, unâaltra forma dellâamore, e scompaio con uno sbuffo negli angoli piÃ¹ reconditi della vita futura di mamma e papÃ .</p>

  
</div>

<div class="item-footer">
   Published
   05:00 â¢
   4 days ago

</div>

</div>

</article>








 
## &#10;  Sunday, 27. September 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Livraisons](https://www.nazioneindiana.com/2020/09/27/livraisons/)&#10;

<div class="item-body">

<div class="item-snippet">


  Una libreria in inglese
&nbsp;
Di Angelo Vannini
&nbsp;
&nbsp;
A Phyllis Cohen,
e alla sua libreria di sogni
&nbsp;
Non sono mai stato la persona adatta a questo, pensai una volta arrivato davanti alla porta, benchÃ© io a volte sia capace di fare quello che altri non possono fare in un lasso cosÃ¬ breve di tempo, Ãš evidente che non sono mai stato adatto, a questo come a ogni altra cosa del resto, per

</div>

<div class="item-content item-summary">



  âŠ<p>photo by Robert Mack</p>
<p>Una libreria in inglese</p>
<p>&nbsp;</p>
<p>Di Angelo Vannini</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>A Phyllis Cohen,</p>
<p>e alla sua libreria di sogni</p>
<p>&nbsp;</p>
<p>Non sono mai stato la persona adatta a questo, pensai una volta arrivato davanti alla porta, benchÃ© io a volte sia capace di fare quello che altri non possono fare in un lasso cosÃ¬ breve di tempo, Ãš evidente che non sono mai stato adatto, a questo come a ogni altra cosa del resto, perchÃ© tutto quello che ho intrapreso in vita mia Ãš sempre capitato mio malgrado, anche se per idea mia, anche se profondamente voluto da me, come questa idea assurda e terribile di lasciare Perpignano per andare a vivere ad Aix-en-Provence, una cosa che certamente ho voluto io ma che Ãš andata, fin da subito, contro me stesso, perchÃ© era evidente, e lo era fin dallâinizio, che mai sarei stato allâaltezza di fare quello che ad Aix-en-Provence mi era chiesto di fare, scrivere seicento pagine, solo a me poteva venire in mente di lanciarmi in unâimpresa del genere, dato che non avevo tempo la mattina, nÃ© il pomeriggio e tantomeno la sera, e poi anche con tutto il tempo del mondo era perfettamente chiaro che non sarei potuto riuscire a scrivere seicento pagine tutte in francese, perchÃ© io non sono adatto a queste cose, anche se a volte preferisco mentirmi e non voglio riconoscerlo, anche se la gente non vede la mia inadeguatezza per quella che Ãš, la mia insufficienza rispetto alle idee che si fanno di me, o delle mie capacitÃ , che sono del tutto errate.</p>
<p>Sono idee del tutto errate, come errato Ãš questo progetto, pensai davanti alla porta, un progetto per cui io divento un anello assolutamente indispensabile, e che farÃ² certamente fallire, se soltanto questa cosa fosse vera, ma vera, voglio augurarmi, non Ãš, e allora con lâaiuto della sorte, perchÃ© di sfighe ne ho avute tante, una dietro lâaltra, sempre, da quindici anni a questa parte, ininterrottamente, tantochÃ© mi dico che prima o poi questa lunga discesa, per quanto evidentemente senza fondo, come sono tutte le discese quando sono vere discese, e non finte, dovrÃ  incontrare qualche soprassalto, uno o due, una cosa tra mille che va per il verso giusto, almeno una volta, cristo, una dico, forse con una manna dal cielo ce la farÃ², e ci sarÃ  una cosa che riesce come dovrebbe riuscire, pensavo davanti alla porta, almeno una e per cui saranno contenti del mio operato, e penseranno che sia anche merito mio se sono riusciti a raggiungere un obiettivo, come appariva allora, tra i piÃ¹ difficili, anche se Ãš e rimane del tutto improbabile, ma qui Ãš tutto nero, pensavo, non câÃš nemmeno una lucina eppure non possono essere se non qua dentro, almeno cosÃ¬ mi hanno detto, pensai appena vidi tutto chiuso e sbarrato, lâinterno preso dal buio, ma la serranda non era abbassata e in vetrina ancora si vedevano libri come se non fosse chiuso, libri ovunque, da ogni parte, libri come se piovessero, eppure la porta non si apriva, maledettamente, cominciai a pensare, era chiaro che non si sarebbe aperta con la scalogna maledetta che mi porto dietro, e come faccio ora che loro non mi vedono nÃ© possono sentire dato che io non posso urlare, non devo attirare lâattenzione proprio ora, pensavo, dato che qui sul marciapiede la mia situazione Ãš irrimediabilmente illegale, di sicuro mi capiterÃ  qualcosa se rimango ancora in questa strada, le volanti non passavano mai nella rue Delavigne, mi avevano detto loro, ma io di queste cose non mi sono mai fidato in vita mia perchÃ© non so quante volte sono stato controllato in situazioni completamente improbabili, come quella volta ad Ancona mentre passeggiavo con un amico, tranquillamente, nella maniera piÃ¹ tranquilla del mondo un piede dopo lâaltro sul marciapiede della Via Nazionale, quando appena svoltati a sinistra davanti a un bar tre carabinieri col mitra, pareva che aspettassero proprio noi quei diavoletti, mezzâora per controllare la carta dâidentitÃ  via radio mentre ci tenevano sotto tiro col mitra come fossimo banditi usciti da una rapina, ed eravamo pure vestiti bene quella volta, camicette abbottonate e appena stirate, un primo pomeriggio dâestate, che cazzo ci facevano lÃ¬ col mitra in un pomeriggio dâestate, pensai mentre ero davanti alla porta, con la sfiga che mi ritrovo passerÃ  sicuramente una volante dei gendarmi stanotte, passerÃ  proprio qui se non mi sbrigo ad entrare, ma loro non rispondono, anche quando comincio a bussare, non câÃš nessuno dentro porco cane, mâhanno lasciato qui nella merda, era prevedibile, era assolutamente prevedibile che sarebbe stato un viaggio fatto completamente a vuoto e che mi sarebbe costato caro, avevo pensato mentre nessuno da dentro rispondeva, esposto nel mezzo della notte ad ogni tipo di ispezione, multa, prelevamento e incarceramento, tutto questo era chiaro che sarebbe successo, se allâimprovviso, dopo non so quanto tempo, non mi avessero aperto. Io non ero adatto a quelle cose, a tutte quelle cose voglio dire, quello che facevo ad Aix-en-Provence come quello che avrei dovuto fare lÃ¬ a Parigi, non ci sarei mai riuscito, per non parlare poi di quello che avevo fatto a Perpignano e per cui, quasi, ero morto di fatica e follia, con la schiena a pezzi e otto chili in piÃ¹ che non riuscivo a smaltire, per quanto corressi, per anni, tutta colpa del mio metabolismo, anche con quello sono stato sfigato, una che ne andasse dritta non câera nÃ© ci sarÃ  mai, la merda surgelata, parevo avanzando, se soltanto qualcuno mi avesse guardato accuratamente se ne sarebbe accorto, era evidente, ma nessuno mi guardava accuratamente e la cosa non mi sorprende, perchÃ© io stesso avrei fatto di tutto per tenermi lontano dalla mia vista, se solo avessi potuto, invece non potevo e mi toccava essere di nuovo lÃ¬ con me stesso nel mezzo della notte aspettando che mi aprissero, e anche dopo che mi avevano aperto ero rimasto solo con me stesso, in mezzo a tutti gli altri che a poco a poco si erano alzati e visibilmente non erano per niente contenti del mio arrivo, e come biasimarli del resto, dato che neanche io ero contento, quella volta come ogni volta, dellâarrivo, che evidentemente non ero capace di fare e che fallivo miserabilmente in modi sempre piÃ¹ disastrosi e avvilenti.</p>
<p>A Perpignano, almeno, ero potuto sparire, anche se solo per un lasso limitato di tempo, a Perpignano avevo potuto far finta di non esistere e questo aveva potuto confortarmi, per un poâ, e soltanto relativamente, ma poi da Perpignano decisi di andarmene per tentare questa follia di Aix-en-Provence che pagherÃ² certamente caro, e presto, non appena la mia impossibilitÃ  di adempiere il contratto che ho firmato cinque mesi fa diverrÃ  chiara a tutti, tempo un anno o due al massimo, pensai una volta entrato nella libreria, che non so perchÃ© ma non pareva una libreria anche se era piena di libri, e quella cosa non era certo di buon augurio, pensai, e subito pensai a non pensarlo, non piÃ¹ per tutto il resto del viaggio, tempo un anno o due e si accorgeranno dellâerrore madornale che hanno fatto con me, sarÃ² espulso dal centro di matematica applicata e mi toccherÃ  fuggire da Aix-en-Provence e probabilmente tornare a Perpignano, anche se a Perpignano non ho piÃ¹ niente da fare, ma almeno da Perpignano potrei andare facilmente, si fa per dire, a lavorare in Catalogna, fare avanti e indietro tra la Spagna e la Francia per insegnare a scuola, finchÃ©, pensai, non mi sarebbe esplosa la testa.</p>
<p>A Perpignano sarebbe stato possibile, mentre ad Aix no, ma io in quel momento mi trovavo ad Aix, dove tutto sarebbe, un giorno allâaltro, precipitato, anzi no, non ero piÃ¹ ad Aix, ero appena arrivato a Parigi senza sapere neanche il perchÃ©, imbarcato in un progetto completamente folle per cui non ero certamente allâaltezza, mi trovo completamente allo sbando, pensai una volta entrato, Ãš un miracolo che non mi abbiano giÃ  arrestato e chissÃ  come farÃ², a festa finita, per ritornare a casa, a festa finita avevo pensato, anche se era chiaro che non era una festa la ragione per cui mi avevano voluto lÃ¬, una ragione di estrema ed impressionante urgenza, mi aveva detto ÃlÃ©onore per telefono, fiondati ti prego, aveva detto e subito mi ero fiondato, come se avessero avuto davvero bisogno di me quando era del tutto evidente che io ero e sarei rimasto in ogni senso superfluo, e non si capiva perchÃ© continuassero a chiamare me, dato che non ero certamente il migliore in questo mestiere, che tra lâaltro non era un mestiere perchÃ© non poteva darmi da vivere dal momento che facevo di tutto per restare nella legalitÃ , e io non volevo essere uno illegale, assolutamente, mai avrei accettato di esserlo, e quindi mi toccava farmi assumere di anno in anno dai dipartimenti piÃ¹ svariati delle piÃ¹ svariate universitÃ  per avere di cosa pagare lâaffitto e comprare il pane, io non ero ricco e anche su questo ero stato sfortunato, perchÃ© câÃš chi nasce senza problemi di soldi e senza problemi di soldi finirÃ  per morire, ma io non facevo parte di questa categoria, i miei erano poveri cristi nati e cresciuti a Vaccarile dove pure io ero nato e cresciuto, prima di finire ad Urbino assieme ad altri poveri cristi, quanti crocifissi, mio dio, pensai una volta finito dentro alla libreria, quanti ne ho visti in tutto e quanti ne sarÃ² ancora condannato a vedere. E questi qua pure erano poveri cristi, pensai, quello lÃ  da Buenos Aires Ãš dovuto fuggire e da anni si nasconde a Parigi sotto falso nome, questa che da Chicago Ãš venuta a ripercorrere le orme della Resistenza francese, quellâaltro che si spaccia per greco ma in veritÃ  Ãš apolide, dove cazzo sono finito, pensai, e soprattutto perchÃ©, per quale dannata ragione mi sono imbarcato in questa impresa chiaramente destinata a fallire, e per di piÃ¹ in un momento come questo, in cui sarei dovuto rimanere ad Aix-en-Provence a fare quello che stavo facendo, cioÃš niente, perchÃ© niente ero in grado di fare ad Aix-en-Provence dal momento che anche lÃ¬ mi ero imbarcato in unâimpresa impossibile, seicento pagine di formule, e per di piÃ¹ in francese, formule che mai sarei riuscito a scrivere, dovendo lavorare mattina pomeriggio e sera soltanto per tirare a campare, questo mondo Ãš un mondo di santi, pensai una volta tolto il giaccone, tutto un sacrificio e nessuna redenzione, almeno non in questa parte della vita, e io non credevo allâaltra parte, non sono uno che crede facilmente, pensai, non ho mai creduto alla befana per esempio, o a babbo natale, quando nonna e nonno ci venivano a trovare dicendo questo te lo manda babbo natale io sparavo giÃ  allora una pernacchia, e correvo via, perchÃ© capivo, se non vedo non credo, ma quello che vedo credo, e vedevo tutti i santi, i sacrificati, i matti, le bollette da pagare e le madonne da tirare, e chissenefrega, dicevo al prete ogni volta che mi pronosticava lâinferno, e avevo ragione io, perchÃ© lâinferno Ãš in questa terra, non in quella. Togliti la merda dalle ossa, vedi se puzza ancora, dicevo ogni volta a mia sorella, se puzza ancora Ãš perchÃ© tutto Ãš dentro, rogna pure nel sangue, aveva fatto bene lei a lasciare lâEuropa per rifugiarsi in Vietnam, e mi dicevo spesso che anche io avrei dovuto raggiungerla, mettere una croce sopra a tutto quanto e partire, ma poi quando andai in estate mi resi conto che non era meglio, la vita lÃ¬ era uno scatafascio esattamente come qua, esattamente come qua si tribolava per le stesse ragioni per cui tribolavamo qua, cosa ho fatto di male io, pensavo allora da mia sorella, che neanche qui posso stare in pace due minuti, Ãš proprio vero che Ãš nel sangue, mi dicevo, e giÃ  ero tornato via, ero a Perpignano ancora prima di essere tornato a Perpignano, e una volta tornato davvero a Perpignano ci misi poco per andare ad Aix-en-Provence senza esserci ancora andato, pensai mentre guardavo i libri che erano ovunque, per terra e sugli scaffali, tutti in inglese cristo santo, neanche un testo in francese in una libreria del sesto arrondissement, manco fossimo davvero a Berkeley, mi dissi, perchÃ© noi viaggiamo con la mente prima del corpo, e solo dopo il corpo segue, ma a volte Ãš il corpo che va e la mente che tiene, non si muove, e allora chissÃ  se mai mi sono mosso da Vaccarile, pensai davanti agli scaffali tutti in inglese, a Perpignano forse non ci ero mai arrivato e me ne rendevo conto in quel momento stesso, mentre mi preparavo a essere nuovamente inutile per me e per tutti in una faccenda, come disse ÃlÃ©onore, della massima urgenza, che certamente non avrei saputo affrontare nella maniera adeguata, ammesso che una maniera adeguata potesse mai esistere in un mondo come quello in cui siamo stati condannati a vivere.</p>
<p>Ma porca, vociferavo, porca, sempre dentro di me, mentre quelli si muovevano tutti in coro per sistemarmi, era quasi commovente tutto quel giostrare allâunisono attorno al mio materiale, devono tenerci davvero, pensai, se in piena notte ancora non mi hanno mandato a cagare, io mi sarei mandato a cagare molto spesso, se avessi potuto, pensai, e soprattutto per esser piombato dal nulla con cosÃ¬ tanto ritardo, ma cosa mi Ãš saltato in mente, partire da Aix-en-Provence in una situazione di emergenza sanitaria assoluta per andare illegalmente a Parigi, al fine di compiere unâoperazione che mai sarei riuscito a compiere, in pieno lockdown, e questo avrebbe dovuto essere sotto gli occhi di tutti, ma loro probabilmente fingevano di non vedere, era davvero improbabile che pensassero si potesse realizzare grazie a me quello che volevano realizzare, anche per una combriccola di svitati come erano loro, non era verisimile che ci credessero davvero, pensavo mentre mi sistemavo, in ogni caso non sarei mai riuscito a dormire quella notte, questo era evidente, e anche le seguenti, sarebbe stato impossibile dormire in una situazione come quella, in un posto come quello e con tutto quello che stava succedendo fuori, una cittÃ  fantasma, mi era sembrata, mi accorsi in quel momento, Parigi al mio arrivo, e probabilmente non sarei mai riuscito ad arrivare in quella libreria senza risvegliare almeno un fantasma. Lâunica cosa bella, pensai mentre attaccavo i computer alla corrente, Ãš che a quella gente importa di me, almeno apparentemente, mi hanno sempre detto le cose come stanno, che fanno quello che fanno non per soldi, perchÃ© non li hanno e mai li avranno, ma per giustizia, per giustizia fanno quello che fanno perchÃ© quando il mondo Ãš rotto câÃš chi pensa ancora che bisogna aggiustarlo, e questo era per me lâunica cosa bella che perÃ² non poteva darmi sollievo perchÃ© io non ero allâaltezza, non avrei mai potuto far parte del loro gruppo in pianta stabile senza morire, un giorno o lâaltro, di fame, e non capivo come facevano loro, a non morire di fame, uno con meno lavoro dellâaltro, chissÃ  quale era il segreto, chi gli spesava lâaffitto, dato che, ne ero sicuro, non era il commercio perchÃ© non vendevano, non era il Centro di Mediazione Anticoloniale perchÃ© ancora non esisteva, ancora quel centro non era un centro, dato che sarebbero divenuti loro il centro, ognuno di loro e tutti assieme, un passeraio, non avevano neanche un ufficio se non quel buco di libreria in cui si rifugiavano di tanto in tanto da qualche mese, a quanto ÃlÃ©onore al telefono mi aveva detto, ma come si fa, pensai, a essere in una situazione come questa, in un posto come questo, cacciato qui senza nessuna possibilitÃ  di successo, niente, a Parigi non riesco, ad Aix sarÃ  un fallimento, Perpignano ormai per me non esiste piÃ¹, come mi sono ridotto, tra lâaltro non si sa nemmeno se ad Aix riuscirÃ² a tornare senza farmi arrestare, solo a me poteva capitare una situazione cosÃ¬, per cui lâunico posto sicuro Ãš questo qui, una libreria in inglese, chissÃ  come ci sono finiti gli altri, pensai, il quartiere dellâOdÃ©on, dico, roba da matti, voler guarire il mondo a partire da qui, come fosse un sogno o un bisogno, una fisima da bel lunedÃ¬.</p>
<p>&nbsp;</p>

  
</div>

<div class="item-footer">
   Published
   05:00 â¢
   5 days ago

</div>

</div>

</article>








 
## &#10;  Saturday, 26. September 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Antropocene fantastico](https://www.nazioneindiana.com/2020/09/26/antropocene-fantastico/)&#10;

<div class="item-body">

<div class="item-snippet">


  &nbsp;
&nbsp;

&nbsp;
In questi giorni Ãš uscito nelle librerie Antropocene fantastico. Scrivere un altro mondo, il nuovo&nbsp;pamphlet di Matteo Meschiari, pubblicato da&nbsp;Armillaria.
Ne ospito qui un estratto in anteprima, tratto dal capitolo Kairocene.
&nbsp;
KAIROCENE â&nbsp;RIFONDARE IL TEMPO
&nbsp;
Quale passato si annida nel futuro?âŠ Leggi il resto Â» 

</div>

<div class="item-content item-summary">



  <p>&nbsp;</p>
<p>&nbsp;</p>
<p>âŠ</p>
<p>&nbsp;</p>
<p>In questi giorni Ãš uscito nelle librerie Antropocene fantastico. Scrivere un altro mondo, il nuovo&nbsp;pamphlet di Matteo Meschiari, pubblicato da&nbsp;Armillaria.</p>
<p>Ne ospito qui un estratto in anteprima, tratto dal capitolo Kairocene.</p>
<p>&nbsp;</p>
<p>KAIROCENE â&nbsp;RIFONDARE IL TEMPO</p>
<p>&nbsp;</p>
<p>Quale passato si annida nel futuro? In che cosa Paleolitico e Antropocene si somigliano? La parola Antropocene Ãš irritante, unâirritazione che viene essenzialmente dalla sua proteiforme adattabilitÃ  ai contesti, dalla sua eccessiva carica di seduzione e facilitÃ  dâuso. Ma, concettualmente, quello che non convince Ãš la sua perenne atmosfera alla Blade Runner, il suo sapore di futuro a tinte fosche, reale ma banale, come una quinta teatrale fissa, scontata. Lâesperienza di Covid-19 ha smentito ogni visione distopica: lâAntropocene Ãš qui senza mutare la percezione del presente. Anzi. Nei comportamenti e nelle atmosfere il presente Ãš venato piÃ¹ di preistoria che di fantascienza. Filosofi oscurantisti e virologi impotenti ci fanno sentire piÃ¹ in un passato immaginato che in futuro promesso. Ã inquietante, certo, ma si apre una possibilitÃ  inedita allâimmaginario del dopo: un Antropocene dagli attributi diversi, piÃ¹ debitore a J.R.R. Tolkien che a Philip K. Dick. E Tolkien per me Ãš il vero scrittore-guida in questo momento storico, perchÃ© se un Antropocene Fantastico Ãš possibile Ãš solo tornando alla radice di chi ha riflettuto sul fantastico in modo ineguagliato. Come dicevo in precedenza, Tolkien non Ãš il Fantasy, perchÃ© lo scarto Ãš tutto tra i due mondi Ãš tutto nellâidea di studio, nella filologia della parola e dello sguardo, e soprattutto nella credenza: Tolkien ci ha lasciato delle istruzioni per lâuso, a una guida mitopoietica del presente e del dopo che ci attende. Fiaba, subcreazione, storytelling fantastico non sono cose da conoscere sulla carta, non sono il destino di un singolo autore, ma sono pratiche sociali, collettive, performative, che hanno il potere di aiutarci a reimmaginare la realtÃ : Â«Tutte le narrazioni si possono avverare; pure alla fine, redente, possono risultare non meno simili e insieme dissimili dalle forme da noi date loro, di quanto lâUomo, finalmente redento, sarÃ  simile e dissimile, insieme, allâuomo caduto a noi notoÂ».</p>
<p>&nbsp;</p>
<p>La citazione chiude un testo in cui allâinizio si pongono le basi: Feeria Â«Ãš un reame che contiene molte altre cose accanto a elfi e fate, oltre a gnomi, streghe, trolls, giganti e draghi: racchiude i mari, il sole, la luna, il cielo, e la terra e tutte le cose che sono in essa, alberi e uccelli, acque e sassi, pane e vino, e noi stessi, uomini mortali, quando siamo vittime di un incantesimoÂ». Il tono apparentemente discorsivo, a tratti bonario, del saggio Sulle fiabe, non deve distrarci con la sua apparente semplicitÃ . Tolkien sta leggendo una conferenza (una Andrew Lang Lecture tenutasi allâuniversitÃ  di St Andrews lâ8 marzo 1939) in bilico tra filologia e autopoetica. Proprio la sua natura ambigua, duplice, rende difficile estrapolare delle coordinate âutiliâ a ottantâanni di distanza, ma quello che si dice qui Ãš soprattutto un invito ad aggiustare lo sguardo, una cosa difficile da proporre e da apprendere. Tolkien ci avverte: Feeria non Ãš solo storie di fate o storie di umani tra le fate, Feeria Ãš un luogo, e non dobbiamo smettere di pensare che in quanto luogo Ãš fatta anche di cose âcomuniâ, ânormaliâ, che in realtÃ  comuni e normali non sono. Su posizioni non troppo lontane da quelle di Viktor Å klovskij sullo straniamento, Tolkien sta dicendo che abbiamo perso la vocazione a guardare il mondo âprimarioâ con attitudine meravigliata. Come recuperare allora lo stupore verso un sasso o una foglia uscendo Â«dalla tediosa opacitÃ  del banale o del familiareÂ»?</p>
<p>La strada non Ãš semplice perchÃ© bisognerebbe comprendere e accettare una frase densissima che il filologo e il linguista storico cala nel suo saggio come un fendente: Â«le lingue, soprattutto le europee moderne, sono una malattia della mitologiaÂ». Tolkien, contrariamente a chi dice di eliminarli, elogia la funzione poietica degli aggettivi: Â«La mente che pensÃ² leggero, pesante, grigio, giallo, immobile, veloce, concepÃ¬ anche la magia atta a rendere cose pesanti, leggere e atte a volare, a trasformare il grigio piombo in giallo oro, lâimmobile roccia in acqua veloceÂ». Questo atto di subcreazione Ãš lo stesso che ritroviamo negli inventori del mito: la mitopoiesi Ãš un atto linguistico primario molto piÃ¹ articolato di una mera architettura allegorica. Il mito non Ãš il tuono che diventa un dio o un irascibile contadino dalla barba rossa elevato a rango divino, il mito Ãš la zona di coesistenza di tuono, Thor e contadino, un luogo di simultaneitÃ  narrativa e ontologica che Tolkien chiama appunto Feeria. Feeria Ãš allora la co-possibilitÃ . E dove la co-possibilitÃ  dei piani si interrompe, per stanchezza creativa, per cinismo, per disordine cognitivo, per usura, allora ci troviamo di fronte a una specie di âmalattia del mitoâ, una sfiducia della lingua per cui subcreazione e sospensione dellâincredulitÃ  sono solo giochi temporanei, fittizi, senza la âcredenzaâ profonda di poter âfare mitoâ anche nel quotidiano. Il problema, ovviamente, non Ãš solo un nodo epistemologico del mondo contemporaneo. Sono e saranno sempre molto pochi i portatori di parola disposti a credere in un commercio diretto tra mito e tempo presente, in un reale scambio di fluidi tra Feeria e il mondo primario.</p>
<p>&nbsp;</p>
<p>âŠ</p>
<p>&nbsp;</p>
<p>Ora, che ci piaccia o meno la parola, siamo entrati nellâAntropocene. Possiamo vedere questâepoca di transizione e la futura prossima come unâennesima declinazione distopica, come una serie Netflix da guardare a distanza stando seduti sul divano, oppure possiamo intercettare nellâAntropocene i grandi flussi mitici che, come accade a ogni epoca, lo attraversano e lo alimentano. Tolkien lo dice cosÃ¬: Â«Costruire un Mondo secondario dentro il quale il sole verde risulti credibile, imponendo Credenza Secondaria, richiederÃ  probabilmente fatica e riflessione, e certamente esigerÃ  una particolare abilitÃ , una sorta di facoltÃ  magica. Pochi si cimentano in compiti cosÃ¬ ardui; ma quando li si affronta e li si attua in misura maggiore o minore, si ottiene un risultato artistico senza pari: arte narrativa, insomma, elaborazione di racconti nella forma primaria e piÃ¹ pregnanteÂ». Ã chiaro che chiedersi come sarÃ  la âletteratura del dopoâ ha piÃ¹ a che fare con questo, con un sole verde, che non con potenziali e anodini romanzi su Covid-19, distanziamento sociale, contenzione domestica e mascherine a passeggio. Nel collasso e nella Pandemia, e forse proprio per questo, dovremmo ricordarci di quelle che Tolkien chiamava Â«le cose piÃ¹ permanenti e fondamentaliÂ».</p>
<p>Tolkien concepisce il Silmarillon nel 1917. Suo figlio Christopher lo pubblica postumo nel 1977. Christopher aveva 53 anni e Guy Gavriel Kay, tra il 1974-75, ne aveva appena 20. Kay, canadese a Oxford, aiutÃ² Christopher nella riscrittura delle parti piÃ¹ tardive. Il libro, che Tolkien voleva pubblicare assieme al Signore degli anelli, ha avuto una genesi di 60 anni. E nonostante la riscrittura postuma resta un incompiuto. Al suo interno ci sono tempi narrativi e tempi redazionali che formano un intrico cosÃ¬ complesso da aver immobilizzato il loro stesso autore. Per noi invece sono un invito a riflettere non sul worldbuilding ma sullâetica della parola: dalla cronaca alla cronologia, dalle agenzie stampa agli annali. Un cambio di prospettiva che potrebbe aiutare a stendere un balsamo calmante sulla fretta di correre a registrare tendenze intellettuali e mode sulle testate on line. Il futuro Ãš crollato. Abbiamo tutto il tempo adesso. Tolkien era sintonizzato su Kairos non su Kronos. Noi certamente non siamo Tolkien, ma siamo lettori e scrittori davanti a una scelta. E questa scelta Ãš di vita o di morte.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

  
</div>

<div class="item-footer">
   Published
   05:00 â¢
   6 days ago

</div>

</div>

</article>








 
## &#10;  Friday, 25. September 2020&#10; 



<article class="item">


#### &#10;  [Nazione Indiana](https://www.nazioneindiana.com)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Storia di farfalle e altre metamorfosi&nbsp;di&nbsp;Chiara Pellegrini â recensione](https://www.nazioneindiana.com/2020/09/25/recensione-silvia-morotti/)&#10;

<div class="item-body">

<div class="item-snippet">


  di Silvia Morotti&nbsp;
&nbsp;
Storia di farfalle e altre metamorfosi&nbsp;di&nbsp;Chiara Pellegrini (Robin, 2020)

&nbsp;
âA Vincenzo Consolo, maestro di voce, maestro di memoriaâ: Chiara Pellegrini esordisce con un romanzo, Storia di farfalle e altre metamorfosi, che si apre nel segno di unâeducazione letteraria e morale.âŠ Leggi il resto Â» 

</div>

<div class="item-content item-summary">



  <p>di Silvia Morotti&nbsp;</p>
<p>&nbsp;</p>
<p>Storia di farfalle e altre metamorfosi&nbsp;di&nbsp;Chiara Pellegrini (Robin, 2020)</p>
<p>âŠ</p>
<p>&nbsp;</p>
<p>âA Vincenzo Consolo, maestro di voce, maestro di memoriaâ: Chiara Pellegrini esordisce con un romanzo, Storia di farfalle e altre metamorfosi, che si apre nel segno di unâeducazione letteraria e morale. Un libro polifonico e stilisticamente curato, con una lingua limpida, incisiva e al tempo stesso capace di abbandonarsi al âmessaggio celesteâ (p.9) della natura, di scendere in profonditÃ , anzi, come scrive lo stesso Consolo, di âverticalizzare il linguaggio, spostarlo verso la zona della poesiaâ. Il romanzo inizia con una data fortemente simbolica: 8 marzo mattino. Si tratta di una lettera, la prima di un lungo carteggio: lâautrice Ãš una delicatissima adolescente che ricorda Katherine Mansfield nel nome e, soprattutto, nel sentire, nel suo trovare da subito, piÃ¹ o meno consapevolmente, la propria religione e il proprio mondo nella scrittura. 8 marzo mattino: di quale anno? Non importa. Il tempo del romanzo si dilata: lâadolescente scrive alla se stessa che sarÃ , domanda alla donna se potrÃ  finalmente, un giorno, âriempire fino in fondo ogni spazioâ o se resterÃ  per sempre âun angolo di vuotoâ (p.7). Ed ecco che la donna risponde: non vuole illudere, non vuole nascondere alla se stessa del passato le ferite âche gocciano per molto tempoâ (p.17), vuole che la ragazza impari ad appartenere, a ârimanere diversaâ (p. 23).</p>
<p>La ragazza di ieri e la donna di oggi appaiono al lettore racchiuse in una stanza ideale, riunite in un miracoloso dialogo, ma il romanzo non Ãš privo di un aggancio con lâesterno: possiamo immaginare che nella stanza ci sia una grande finestra, una di quelle finestre che tanto amava anche unâaltra adolescente, Emma Bovary; lo sguardo delle due donne si posa quindi fuori: non Ãš solo uno sguardo sognante, Ãš anche lo sguardo di chi contempla il mondo, un mondo che si lascia cogliere nel momento in cui la primavera si schiude, fino a quando matura, alle soglie dellâestate. Una primavera e unâestate di qualsiasi anno, una primavera e unâestate di una vita che fiorisce e si trasforma, come ogni vita in ogni tempo.</p>
<p>La metamorfosi Ãš talvolta espiazione e percorso di salvezza: âsi resta diversiâ, si deve attraversare il dolore, perdere il sÃ© per poi riconoscersi (o almeno ricomporre qualche frammento). Tra i tanti riferimenti letterari possibili, lâimmagine della farfalla non puÃ² che ricordare Guido Gozzano, lâentomologo, chiuso nel suo eremo, dove silenziose e in attesa dormono le crisalidi. Nel romanzo di Chiara Pellegrini, lâattesa Ãš sicuramente un tema chiave, come Ãš naturale in pagine scritte in gran parte da unâadolescente; lâadolescenza Ãš lâetÃ  dellâattesa ed anche lâetÃ  in cui la vita ti si offre come un ventaglio di infinite possibilitÃ : attesa, quindi, ma anche scelta. Storia di farfalle e altre metamorfosi non Ãš un romanzo crepuscolare: Ãš piÃ¹ forte, alla fine, la voglia di bruciare nella luce, dopo aver passato la vita a evitare di scegliere. Quando la metamorfosi avviene, quando Caterina si scopre farfalla, porta impresso, come lâAcherontia di Gozzano, un segno spaventoso, qualcosa a cui non Ãš riuscita a dar nome per molto tempo, un trauma che ha condizionato, sotterraneo e prepotente, tutta la sua esistenza. Se le voci maschili sono evanescenti â lâamore non goduto della giovinezza o lâamore della maturitÃ - câÃš invece personaggio maschile che, pur restando sullo sfondo, domina lâintera esistenza di Caterina: Ãš la vera ferita, il dolore rimosso, lâincarnazione del male che non ha voce ma ha âmaniâ, âmani caldeâ, odiose e brutali, il cui ricordo ossessiona Caterina. Il trauma avviene quando Caterina sta per sbocciare. La farÃ  sentire âfuori postoâ (p.7) nella sua primavera e nella sua estate. Le renderÃ  indispensabile trovare una strada per ârestare diversaâ, per fiorire, nonostante tutto. Un varco per Caterina sono le piante e i fiori che lei ama. Le piante non possono muoversi, non possono parlare. Le piante le somigliano ancora di piÃ¹ dopo quel trauma che lâha inchiodata e le ha tolto la voce. Eppure, lei continua a fiorire, consapevole di quanto dolore richieda il mutare forma. âFiorire non Ãš uno scherzoâ, scrive Caterina adulta (cfr. lettera del 23 marzo, notte di stelle):</p>
<p>&nbsp;</p>
<p>Fiorire non Ãš uno scherzo. Ã necessario spaccarsi ed Ãš doloroso. La gemma riposa nella fibra del ramo tutto lâinverno. Quando primavera entra e, come sappiamo, non bussa e ha passo sicuro, la gemma erompe dalla scorza ed Ãš una spaccatura. Le fibre si sono tese allo spasimo dentro il ramo per far posto allâingrossarsi di quel grumo composito e duro di vita e quando questo Ãš gonfio abbastanza, ecco che la sua eruzione lacera e apre il varco. Primavera entra e non bussa e ha passo sicuro. Ieri il verde non câera, oggi vibra a ogni soffio sulle punte dei rami. Ma questa esplosione, che sembra avvenuta stanotte, chiamata dal silenzio delle stelle, ha impiegato mesi per aggregarsi, comporsi, strutturarsi, e lo ha fatto a spese delle fibre dellâalbero, piegate, ritorte, compresse e infine strappate, lo ha fatto succhiando, mungendo, spremendo linfe e umori vitali alla pianta tutta. Tutto quel che cresce fa male a tutto cioÌ che racchiude. Tutto cioÌ che cresce lacera tutto cioÌ che lo vorrebbe avvolgere e contenere. Crescere e racchiudere, coraggio e paura. Il movimento della vita. La vita e la morte. Coraggio e paura. </p>
<p>&nbsp;</p>
<p>Anche la letteratura, come la natura, Ãš cosa viva. In un universo frammentato e sfuggente, lâio si perde in un gioco di specchi: Caterina si ritrova nel mondo vegetale e nei libri, in altre voci di donne, poetesse, scrittrici o protagoniste di pagine narrative. Tra le molte storie citate, nel romanzo si ricorda il racconto Rose rosse, della siciliana Maria Messina, una storia dura e violenta: âsostieni anche tu, se ne hai coraggio, che Ãš la solita scrittura femmineaâ, afferma la voce narrante, parlando a se stessa, ma rivolgendosi in realtÃ  a un uditorio piÃ¹ vasto, al pubblico che ancora dibatte sullâannosa questione se esista o meno una scrittura femminile. A tale riguardo, lâautrice di Farfalle e altre metamorfosi rivendica lâesistenza di quello che Sandra Petrignani (Laterza, 2019) chiama âlessico femminileâ: una lingua diversa, espressione di un âpensiero naturalmente autocriticoâ e spesso âinascoltatoâ (ibidem, p.7), una lingua che sa trattare con leggerezza temi pesanti, proprio come avviene per Maria Messina e per la stessa Pellegrini.</p>
<p>Caterina diviene farfalla e, in parte, si libera e si riconosce; non smette di confrontarsi con il dolore che lâha resa quello che Ãš, ma trova una strategia per ârestare diversaâ. Come Marcel, alla fine della Recherche, si scopre scrittore, cosÃ¬ Caterina comprende che ciÃ² che lâaspetta, da sempre, Ãš âun volo di paroleâ (p. 217). Le due donne, la ragazzina e la donna matura, trovano un varco e balzano fuori, fuggono, in un luogo dove non Ãš necessario scegliere. E passare quel varco âÃš rimanere diversiâ, âtrasfigurareâ (cfr. p. 21):</p>
<p>&nbsp;</p>
<p>No, non Ãš una contraddizione: rimanere diversi Ãš un trasfigurare. Sei ancora tu, ma indossi una veste nuova, come dopo una risurrezione, una volta che la pietra del sepolcro Ãš rotolata di lato e si esce dalla tomba come dal grembo materno, scintillanti di luce e rinati. </p>
<p>&nbsp;</p>
<p>Scrivere non imprigiona, scrivere Ãš ârestare diversiâ: Caterina, come Katherine, trova nella scrittura la sua religione, il suo mondo, la sua vita.</p>
<p>&nbsp;</p>
<p>Silvia Morotti</p>

  
</div>

<div class="item-footer">
   Published
   10:10 â¢
   7 days ago

</div>

</div>

</article>








 
## &#10;  Monday, 06. April 2020&#10; 



<article class="item">


#### &#10;  [schema.org News](http://blog.schema.org)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [COVID-19 schema for CDC hospital reporting](http://blog.schema.org/2020/04/covid-19-schema-for-cdc-hospital.html)&#10;

<div class="item-body">

<div class="item-snippet">


  The COVID-19 pandemic requires various medical and government authorities to aggregate&nbsp;data about available resources from a wide range of medical facilities.&nbsp;Clearly standard schemas for this structured data can be very useful.The Centers for Disease Control (CDC) in the U.S. defined a set of data fields to facilitate exchange&nbsp;of this data. We are introducing a Schema.org representa

</div>

<div class="item-content item-summary">



  The COVID-19 pandemic requires various medical and government authorities to aggregate&nbsp;data about available resources from a wide range of medical facilities.&nbsp;Clearly standard schemas for this structured data can be very useful.<br><br>The Centers for Disease Control (CDC) in the U.S. defined a set of data fields to facilitate exchange&nbsp;of this data. We are introducing a Schema.org representation of these data fields.&nbsp;<br><br>The purpose of this schema definition is to provide a standards-based representation that can be used to encode and exchange records that correspond to the CDC format, with usage within the U.S. primarily in mind. While the existence of this schema may provide additional implementation options for those working with US hospital reporting data about COVID-19, please refer to the CDC and other appropriate bodies for authoritative guidance on the latest reporting workflows and data formats.<br><br>Depending upon&nbsp;context, any of the formats and standards that work with Schema.org may be applicable for encoding this data, including the Microdata, RDFa and JSON-LD data formats, as well as related technologies such as W3C SPARQL for data query. JSON-LD is in most cases likely to be the most appropriate format. There is no assumption that data encoded using this schema should necessarily be published on the public Web, nor that it would be used by search engines.<br><br>We will continue to improve this vocabulary in the light of feedback, and welcome suggestions for improvements and additions particularly from US healthcare organizations who are using it.&nbsp;This CDC-based vocabulary follows other recent changes we have made to Schema.org. For details of recent changes see our release notes&nbsp;and our previous post announcing the SpecialAnnouncement markup, which is now supported at both Bing (blog, docs) and Google (blog, docs). As the global response to COVID-19 evolves we will do our best to improve schema.org's vocabularies to represent the changes that Coronavirus is bringing to society, and to assist those using structured data to help with the response.<br><br><br>

  
</div>

<div class="item-footer">
   Published
   16:42 â¢
   6 months ago

   | Updated
   Monday, 06. April 2020 16:42 â¢
   6 months ago

</div>

</div>

</article>








 
## &#10;  Tuesday, 17. March 2020&#10; 



<article class="item">


#### &#10;  [schema.org News](http://blog.schema.org)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Schema for Coronavirus special announcements, Covid-19 Testing Facilities and more](http://blog.schema.org/2020/03/schema-for-coronavirus-special.html)&#10;

<div class="item-body">

<div class="item-snippet">


  The COVID-19 pandemic is causing a large number of âSpecial Announcementsâ pertaining to changes in schedules and other aspects of everyday life. This includes not just closure of facilities and rescheduling of events but also new availability of medical facilities such as testing centers.We have today published Schema.org 7.0, which includes fast-tracked new vocabulary to assist the global respons

</div>

<div class="item-content item-summary">



  The COVID-19 pandemic is causing a large number of âSpecial Announcementsâ pertaining to changes in schedules and other aspects of everyday life. This includes not just closure of facilities and rescheduling of events but also new availability of medical facilities such as testing centers.<br><br>We have today published Schema.org 7.0, which includes fast-tracked new vocabulary to assist the global response to the Coronavirus outbreak.<br><br>It includes a "SpecialAnnouncement" type that provides for simple date-stamped textual updates, as well as markup to associate the announcement with a situation (such as the Coronavirus pandemic), and to indicate URLs for various kinds of update such a school closures, public transport closures, quarantine guidelines, travel bans, and information about getting tested.&nbsp;&nbsp;<br><br>Many new testing facilities are being rapidly established worldwide, to test for COVID-19. Schema.org now has a CovidTestingFacility type to represent these, regardless of whether they are part of long-established medical facilities or temporary adaptations to the emergency.<br><br>We are also making improvements to other areas of Schema.org to help with the worldwide migration to working online and working from home, for example by helping event organizers indicate when an event has moved from having a physical location to being conducted online, and whether the event's "eventAttendanceMode" is online, offlline or mixed.&nbsp;<br><br>We will continue to improve this vocabulary in the light of feedback (github; doc), and welcome suggestions for improvements and additions particularly from organizations who are publishing such updates.&nbsp; <br><br> Dan Brickley, R.V.Guha, Google.<br> Tom Marsh, Microsoft.

  
</div>

<div class="item-footer">
   Published
   03:16 â¢
   7 months ago

   | Updated
   Tuesday, 17. March 2020 03:23 â¢
   7 months ago

</div>

</div>

</article>








 
## &#10;  Monday, 24. February 2020&#10; 



<article class="item">


#### &#10;  [schema.org News](http://blog.schema.org)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Schema.org 6.0](http://blog.schema.org/2020/01/schemaorg-60.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Schema.org version 6.0 has been released. See the release notes for full details. &nbsp;As always, the release notes have full details and links (including previous releases e.g.&nbsp;5.0&nbsp;and&nbsp;4.0).We are now aiming to release updated schemas on an approximately monthly basis (with longer gaps around vacation periods). Typically, new terms are first added to our "Pending" area to give time

</div>

<div class="item-content item-summary">



  Schema.org version 6.0 has been released. See the release notes for full details. &nbsp;As always, the release notes have full details and links (including previous releases e.g.&nbsp;5.0&nbsp;and&nbsp;4.0).<br><br>We are now aiming to release updated schemas on an approximately monthly basis (with longer gaps around vacation periods). Typically, new terms are first added to our "Pending" area to give time for the definitions to benefit from implementation experience before they are added to the "core" of Schema.org. As always, many thanks to everyone who has contributed to this release of Schema.org.<br><br>--<br>Dan Brickley, for Schema.org.<br><br>

  
</div>

<div class="item-footer">
   Published
   16:13 â¢
   8 months ago

   | Updated
   Monday, 24. February 2020 19:54 â¢
   7 months ago

</div>

</div>

</article>








 
## &#10;  Thursday, 01. August 2019&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Introduction to Statistics With Data Packages and Gonum](http://okfnlabs.org/blog/2019/08/01/intro-statistics-datapackage-gonum.html)&#10;

<div class="item-body">

<div class="item-snippet">


  After 6 years at Google, Daniel Fireman is currently a Ph.D. student, professor and activist for government transparency and accountability in the Northeast of Brazil. He was one of the 2017âs Frictionless Data Tool Fund grantees and implemented the core Frictionless Data specification in the Go programming language: datapackage and tableschema, which he still maintains. You can read more about thi

</div>

<div class="item-content item-summary">



  <p>After 6 years at Google, Daniel Fireman is currently a Ph.D. student, professor and activist for government transparency and accountability in the Northeast of Brazil. He was one of the 2017âs Frictionless Data Tool Fund grantees and implemented the core Frictionless Data specification in the Go programming language: datapackage and tableschema, which he still maintains. You can read more about this in his grantee profile.</p>

<p>Since its first release in 2017, weâve been improving datapackage and tableschema packages. Besides fixing bugs, we tried to make it easier to use data packages together with statistical/plotting libraries like Gonum. This post shows an example of such usage and was inspired in this post, from Sebastian Binet.</p>



<p>Our goal in this tutorial is to load a data package from the web and use Gonum to calculate some basic statistics.</p>

Go, Data Packages &amp; Gonum

<p>datapackage is âa package for working with Data Packagesâ. A Data Package consists of:</p>

<ul>
  <li>Metadata that describes the structure and contents of the package</li>
  <li>Resources such as data files that form the contents of the package</li>
</ul>

<p>Gonum  is  âa set of packages designed to make writing numeric and scientific algorithms productive, performant and scalable.â</p>

<p>Before being able to use <code>datapackage</code> and  <code>Gonum</code>, we need to install  Go. We can download and install the  <code>Go</code>  toolchain for a variety of platforms and operating systems from  golang.org/dl. This post assumes the installation of version 11 or newer.</p>

<p>After installing Go, the runtime will download <code>Gonum</code>, <code>datapackage</code> and all its dependencies as part of running the go script.</p>

Reading Datapackage

<p>In this post, we are using a Tabular Data Package containing the periodic table. The package descriptor (datapackage.json) and contents (data.csv) are stored on GitHub. This dataset includes the atomic number, symbol, element name, atomic mass, and the metallicity of the element. Letâs start by taking a quick look at the header and the first rows.</p>

<pre><code>// file: stats.go

package main

import (
    "fmt"

    "github.com/frictionlessdata/datapackage-go/datapackage"
)

func main() {
    pkg, err := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
    if err != nil {
        panic(err)
    }
    res := pkg.GetResource("data")
    table, err := res.ReadAll()
    if err != nil {
        panic(err)
    }
    for i := 0; i &lt; 4; i++ {
        fmt.Println(table[i])
    }
}
</code></pre>

Gonum and statistics

<p>Gonum provides many statistical functions. Letâs use it to calculate the mean, median, standard deviation and variance of the atomic masses.</p>

<pre><code>// file: stats.go

package main

import (
        "fmt"
        "math"
        "sort"

        "github.com/frictionlessdata/datapackage-go/datapackage"
        "github.com/frictionlessdata/tableschema-go/csv"
        "gonum.org/v1/gonum/stat"
)

func main() {
        pkg, err := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
        if err != nil {
                panic(err)
        }
        var masses []float64
        res := pkg.GetResource("data")
        if err := res.CastColumn("atomic mass", &amp;masses, csv.LoadHeaders()); err != nil {
                panic(err)
        }
        fmt.Printf("data: %v\n", masses)

        sort.Float64s(masses)
        fmt.Printf("data: %v (sorted)\n", masses)

        // computes the weighted mean of the dataset.
        // we don't have any weights (ie, all weights are 1)
        // so we just pass a nil slice.
        mean := stat.Mean(masses, nil)

        // computes the median of the dataset.
        // here as well, we pass a nil slice as weights.
        median := stat.Quantile(0.5, stat.Empirical, masses, nil)

        variance := stat.Variance(masses, nil)
        stddev := math.Sqrt(variance)

        fmt.Printf("mean=     %v\n", mean)
        fmt.Printf("median=   %v\n", median)
        fmt.Printf("variance= %v\n", variance)
        fmt.Printf("std-dev=  %v\n", stddev)
}
</code></pre>

<p>The program above performs some basic statistical operations on our dataset:</p>

<pre><code>$&gt; go run stats.go
... dependency download logs ...
data: [1.00794 4.002602 6.941 9.012182 10.811 12.0107 14.0067 15.9994 18.9984032 20.1797 22.98976928 24.305 26.9815386 28.0855 30.973762 32.065 35.453 39.948 39.0983 40.078 44.955912 47.867 50.9415 51.9961 54.938045 55.845 58.933195 58.6934 63.546 65.38 69.723 72.64 74.9216 78.96 79.904 83.798 85.4678 87.62 88.90585 91.224 92.90638 95.96 98 101.07 102.9055 106.42 107.8682 112.411 114.818 118.71 121.76 127.6 126.90447 131.293 132.9054519 137.327 138.90547 140.116 140.90765 144.242 145 150.36 151.964 157.25 158.92535 162.5 164.93032 167.259 168.93421 173.054 174.9668 178.49 180.94788 183.84 186.207 190.23 192.217 195.084 196.966569 200.59 204.3833 207.2 208.9804 209 210 222 223 226 227 232.03806 231.03588 238.02891 237 244 243 247 247 251 252 257 258 259 262 267 268 271 272 270 276 281 280 285 284 289 288 293 294 294]
data: [1.00794 4.002602 6.941 9.012182 10.811 12.0107 14.0067 15.9994 18.9984032 20.1797 22.98976928 24.305 26.9815386 28.0855 30.973762 32.065 35.453 39.0983 39.948 40.078 44.955912 47.867 50.9415 51.9961 54.938045 55.845 58.6934 58.933195 63.546 65.38 69.723 72.64 74.9216 78.96 79.904 83.798 85.4678 87.62 88.90585 91.224 92.90638 95.96 98 101.07 102.9055 106.42 107.8682 112.411 114.818 118.71 121.76 126.90447 127.6 131.293 132.9054519 137.327 138.90547 140.116 140.90765 144.242 145 150.36 151.964 157.25 158.92535 162.5 164.93032 167.259 168.93421 173.054 174.9668 178.49 180.94788 183.84 186.207 190.23 192.217 195.084 196.966569 200.59 204.3833 207.2 208.9804 209 210 222 223 226 227 231.03588 232.03806 237 238.02891 243 244 247 247 251 252 257 258 259 262 267 268 270 271 272 276 280 281 284 285 288 289 293 294 294] (sorted)
mean=     146.43746355915252
median=   140.90765
variance= 8026.634755570227
std-dev=  89.59148818704948
</code></pre>

<p>Thanks for reading!</p>

<p>We welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-go repository.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Tuesday, 02. April 2019&#10; 



<article class="item">


#### &#10;  [schema.org News](http://blog.schema.org)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Schema.org 3.5: Simpler extension model, projects, grants and funding schemas, and new terms for describing educational and occupational credentials](http://blog.schema.org/2019/04/schemaorg-35-simpler-extension-model.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Schema.org version 3.5 has been released. This release moves a number of terms from  the experimental "Pending" area into the Schema.org core. It also simplifies and clarifies the Schema.org extension model, reducing our emphasis on using named subdomains for topical groups of schemas. New terms introduced in Pending area include improvements for describing projects, grants and funding agencies; fo

</div>

<div class="item-content item-summary">



  Schema.org version 3.5 has been released. This release moves a number of terms from  the experimental "Pending" area into the Schema.org core. It also simplifies and clarifies the Schema.org extension model, reducing our emphasis on using named subdomains for topical groups of schemas. New terms introduced in Pending area include improvements for describing projects, grants and funding agencies; for describing open-ended date ranges (e.g. datasets); and a substantial vocabulary for Educational and Occupational Credentials. Many thanks to all who contributed! 

  
</div>

<div class="item-footer">
   Published
   16:19 â¢
   over a year ago

   | Updated
   Tuesday, 02. April 2019 16:19 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Thursday, 18. October 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Announcing datapackage-pipelines version 2.0](http://okfnlabs.org/blog/2018/10/18/announcing-datapackage-pipelines-v2.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Today weâre releasing a major version for datapackage-pipelines, version 2.0.0.

This new version marks a big step forward in realizing the Data Factory concept and framework. We integrated datapackage-pipelines with its younger sister dataflows, and created a set of common building blocks you can now use interchangeably between the two frameworks.

âŠ

figure 1: diagram showing the relationship bet

</div>

<div class="item-content item-summary">



  <p>Today weâre releasing a major version for datapackage-pipelines, version 2.0.0.</p>

<p>This new version marks a big step forward in realizing the Data Factory concept and framework. We integrated datapackage-pipelines with its younger sister dataflows, and created a set of common building blocks you can now use interchangeably between the two frameworks.</p>

<p>âŠ
<br>
figure 1: diagram showing the relationship between dataflows and datapackage-pipelines</p>

<p>Itâs now possible to bootstrap and develop flows using dataflows, and then run these flows as-is on a datapackage-pipelines server - or effortlessly convert them to the declarative yaml syntax.</p>

<p>Install datapackage-pipelines using <code>pip</code>:</p>

<pre><code>pip install datapackage-pipelines
</code></pre>

What Changed?

New Low-level API and stdout Redirect

<p>One big change (and a long time request) is that processors are now allowed to print from inside their processing code, without interfering with the correct operation of the pipeline. All prints are automatically converted to logging.info(âŠ) calls.This behaviour is enabled when using the new low-level API. The main change weâve introduced is that ingest() is now a context manager. This means that you now should run:</p>

<pre><code># New style for ingest and spew
with ingest() as ctx:
 # Do stuff with datapackage and resource_iterator
 spew(ctx.datapackage,
 ctx.resource_iterator,
 ctx.stats)
</code></pre>

<p>Backward compatibility is maintained for the old way of using ingest(), so you donât have to update all your code immediately.</p>

<pre><code># This still works, but wonât handle print()s
parameters, datapackage, resource_iterator = ingest()
spew(datapackage, resource_iterator)
</code></pre>

Dataflows integration

<p>Thereâs a new integration with dataflows which allows running Flows directly from the <code>pipeline-spec.yaml</code> file.
You can integrate dataflows within pipeline specs using the <code>flow</code> attribute instead of <code>run</code>. For example, given the following flow file, saved under <code>my-flow.py</code>:</p>
<pre><code>from dataflows import Flow, dump_to_path, load, update_package
â
def flow(parameters, datapackage, resources, stats):
  stats[âmultiplied_fieldsâ] = 0
 â
  def multiply(field, n):
    def step(row):
      row[field] = row[field] * n
      stats[âmultiplied_fieldsâ] += 1
      return step
â
    return Flow(update_package(name=âmy-datapackageâ),
                load((datapackage, resources),
                multiply(âmy-fieldâ, 2))
</code></pre>

<p>And a <code>pipeline-spec.yaml</code> in the same directory:</p>
<pre><code>my-flow:
 pipeline:
   â run: load_resource
 parameters:
   url: example.com/my-datapackage/datapackage.json
   resource: my-resource
     â flow: my-flow
     â run: dump.to_path
</code></pre>

<p>You can run the pipeline using <code>dpp run my-flow</code>.</p>

<p>If you want to wrap a flow inside a processor, you can use the <code>spew_flow</code> helper function:</p>

<pre><code>from dataflows import Flow
from datapackage_pipelines.wrapper import ingest
from datapackage_pipelines.utilities.flow_utils import spew_flow
â
def flow(parameters):
 return Flow(
 # Flow processing comes here
 )
â
â
if __name__ == â__main__â:
 with ingest() as ctx:
 spew_flow(flow(ctx.parameters), ctx)
</code></pre>

Standard Processor Refactoring
<p>We refactored all standard processors to use their counterparts from dataflows, thus removing code duplication and allowing us to move forward quicker. As a result, weâre also introducing a couple of new processors:</p>

<ul>
  <li>
    <p><code>load</code> - Loads and streams a new resource (or resources) into the data package. Itâs based on the dataflows processor with the same name, so it supports loading from local files, remote URL, data packages, locations in environment variables etc. For more information, consult the dataflows documentation.</p>
  </li>
  <li>
    <p><code>printer</code> - Smart printing processor for displaying the contents of the stream - comes in handy for development or monitoring a pipeline.It will not print all rows, but an logarithmically sparse sample - in other words, it will print rows 1-20, 100-110, 1000-1010 etc. It also prints the last 10 rows of the dataset.</p>
  </li>
</ul>

Deprecations

<p>We are deprecating a few processorsâââyou can still use them as usual but they will be removed in the next major version (3.0):</p>

<ul>
  <li><code>add_metadata</code> - was renamed to <code>update_package</code> for consistency</li>
  <li><code>add_resource</code> and <code>stream_remote_resources</code> - are being replaced by the <code>load</code></li>
  <li><code>dump.to_path</code>, <code>dump.to_zip</code>, <code>dump.to_sql</code> - are being deprecated - you should use <code>dump_to_path</code>, <code>dump_to_zip</code> and <code>dump_to_sql</code> instead.
Note that <code>dump_to_path</code> and <code>dump_to_zip</code> lack some features that exist in the current processorsâââfor example, custom file formatters and non-tabular file support. We might introduce some of that functionality into the new processors as well in the next versions - in the meantime, please let us know what you think about these features and how badly you need them.</li>
</ul>

The Road Ahead

<p>In the next versions weâre planning to further the integration of dataflows and datapackage-pipelines. Weâre going to work on streamlining development and deployment as well as taking care of naming and documentation to harmonize all aspects of the dataflows ecosystem.
Weâre also working on de-composing datapackage-pipelines into smaller, self contained components. In this version we took apart the standard processor code and some supporting libraries (e.g. <code>kvstore</code>) and delegated it to external libraries.</p>

Links and References

<ul>
  <li>Read more on datapackage-pipelines here: github.com/frictionlessdata/datapackage-pipelines</li>
  <li>Read more on dataflows here: github.com/datahq/dataflows</li>
  <li>Read more on Data Factory here: okfnlabs.org/blog/2018/08/29/data-factory-data-flows-introduction.html</li>
</ul>

Contributors

<p>Thanks to Ori Hoch for contributing code and other invaluable assistance with this release.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Thursday, 30. August 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Data Factory &amp; DataFlows - Tutorial](http://okfnlabs.org/blog/2018/08/30/data-factory-data-flows-tutorial.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Data Factory is an open framework for building and running lightweight data processing workflows quickly and easily. We recommend reading this introductory blogpost to gain a better understanding of underlying Data Factory concepts before diving into the tutorial below.



Learn how to write your own processing flows

Letâs start with the traditional âhello, worldâ example:

from dataflows import F

</div>

<div class="item-content item-summary">



  <p>Data Factory is an open framework for building and running lightweight data processing workflows quickly and easily. We recommend reading this introductory blogpost to gain a better understanding of underlying Data Factory concepts before diving into the tutorial below.</p>



Learn how to write your own processing flows

<p>Letâs start with the traditional âhello, worldâ example:</p>

<pre><code>from dataflows import Flow

data = [
  {'data': 'Hello'},
  {'data': 'World'}
]

def lowerData(row):
	row['data'] = row['data'].lower()

f = Flow(
      data,
      lowerData
)
data, *_ = f.results()

print(data)

# --&gt;
# [
#   [
#     {'data': 'hello'},
#     {'data': 'world'}
#   ]
# ]
</code></pre>

<p>This very simple flow takes a list of <code>dict</code>s and applies a row processing function on each one of them.</p>

<p>We can load data from a file instead:</p>

<pre><code>from dataflows import Flow, load

# beatles.csv:
# name,instrument
# john,guitar
# paul,bass
# george,guitar
# ringo,drums

def titleName(row):
    row['name'] = row['name'].title()

f = Flow(
      load('beatles.csv'),
      titleName
)
data, *_ = f.results()

print(data)

# --&gt;
# [
#   [
#     {'name': 'John', 'instrument': 'guitar'},
#     {'name': 'Paul', 'instrument': 'bass'},
#     {'name': 'George', 'instrument': 'guitar'},
#     {'name': 'Ringo', 'instrument': 'drums'}
#   ]
# ]
</code></pre>

<p>The source file can be a CSV file, an Excel file or a JSON file. You can use a local file name or a URL for a file hosted somewhere on the web.</p>

<p>Data sources can be generators and not just lists or files. Letâs take as an example a very simple scraper:</p>

<pre><code>from dataflows import Flow

from xml.etree import ElementTree
from urllib.request import urlopen

# Get from Wikipedia the population count for each country
def country_population():
    # Read the Wikipedia page and parse it using etree
    page = urlopen('en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population').read()
    tree = ElementTree.fromstring(page)
    # Iterate on all tables, rows and cells
    for table in tree.findall('.//table'):
        if 'wikitable' in table.attrib.get('class', ''):
            for row in table.findall('tr'):
                cells = row.findall('td')
                if len(cells) &gt; 3:
                    # If a matching row is found...
                    name = cells[1].find('.//a').attrib.get('title')
                    population = cells[2].text
                    # ... yield a row with the information
                    yield dict(
                        name=name,
                        population=population
                    )

f = Flow(
      country_population(),
)
data, *_ = f.results()

print(data)
# ---&gt;
# [
#   [
#     {'name': 'China', 'population': '1,391,090,000'},
#     {'name': 'India', 'population': '1,332,140,000'},
#     {'name': 'United States', 'population': '327,187,000'},
#     {'name': 'Indonesia', 'population': '261,890,900'},
#     ...
#   ]
# ]
</code></pre>

<p>This is nice, but we do prefer the numbers to be actual numbers and not strings.</p>

<p>In order to do that, letâs simply define their type to be numeric:</p>

<pre><code>from dataflows import Flow, set_type

def country_population():
    # same as before
	...

f = Flow(
	country_population(),
    set_type('population', type='number', groupChar=',')
)
data, *_ = f.results()

print(data)
# --&gt;
# [
#   [
#     {'name': 'China', 'population': Decimal('1391090000')},
#     {'name': 'India', 'population': Decimal('1332140000')},
#     {'name': 'United States', 'population': Decimal('327187000')},
#     {'name': 'Indonesia', 'population': Decimal('261890900')},
#     ...
#   ]
# ]

</code></pre>

<p>Data is automatically converted to the correct native Python type.</p>

<p>Apart from data-types, itâs also possible to set other constraints to the data. If the data fails validation (or does not fit the assigned data-type) an exception will be thrown - making this method highly effective for validating data and ensuring data quality.</p>

<p>What about large data files? In the above examples, the results are loaded into memory, which is not always preferable or acceptable. In many cases, weâd like to store the results directly onto a hard drive - without having the machineâs RAM limit in any way the amount of data we can process.</p>

<p>We do it by using dump processors:</p>

<pre><code>from dataflows import Flow, set_type, dump_to_path

def country_population():
    # same as before
	...

f = Flow(
	country_population(),
    set_type('population', type='number', groupChar=','),
    dump_to_path('country_population')
)
*_ = f.process()

</code></pre>

<p>Running this code will create a local directory called <code>county_population</code>, containing two files:</p>

<pre><code>âââ country_population
â   âââ datapackage.json
â   âââ res_1.csv
</code></pre>

<p>The CSV file - <code>res_1.csv</code> - is where the data is stored. The <code>datapackage.json</code> file is a metadata file, holding information about the data, including its schema.</p>

<p>We can now open the CSV file with any spreadsheet program or code library supporting the CSV format - or using one of the data package libraries out there, like so:</p>

<pre><code>from datapackage import Package
pkg = Package('country_population/res_1.csv')
it = pkg.resources[0].iter(keyed=True)
print(next(it))
# prints:
# {'name': 'China', 'population': Decimal('1391110000')}
</code></pre>

<p>Note how using the data package meta-data, data-types are restored and thereâs no need to âre-parseâ the data. This also works with other types too, such as dates, booleans and even <code>list</code>s and <code>dict</code>s.</p>

<p>So far weâve seen how to load data, process it row by row, and then inspect the results or store them in a data package.</p>

<p>Letâs see how we can do more complex processing by manipulating the entire data stream:</p>

<pre><code>from dataflows import Flow, set_type, dump_to_path

# Generate all triplets (a,b,c) so that 1 &lt;= a &lt;= b &lt; c &lt;= 20
def all_triplets():
    for a in range(1, 20):
        for b in range(a, 20):
            for c in range(b+1, 21):
                yield dict(a=a, b=b, c=c)

# Yield row only if a^2 + b^2 == c^1
def filter_pythagorean_triplets(rows):
    for row in rows:
        if row['a']**2 + row['b']**2 == row['c']**2:
            yield row

f = Flow(
    all_triplets(),
    set_type('a', type='integer'),
    set_type('b', type='integer'),
    set_type('c', type='integer'),
    filter_pythagorean_triplets,
    dump_to_path('pythagorean_triplets')
)
_ = f.process()

# --&gt;
# pythagorean_triplets/res_1.csv contains:
# a,b,c
# 3,4,5
# 5,12,13
# 6,8,10
# 8,15,17
# 9,12,15
# 12,16,20
</code></pre>

<p>The <code>filter_pythagorean_triplets</code> function takes an iterator of rows, and yields only the ones that pass its condition.</p>

<p>The flow framework knows whether a function is meant to handle a single row or a row iterator based on its parameters:</p>

<ul>
  <li>if it accepts a single <code>row</code> parameter, then itâs a row processor.</li>
  <li>if it accepts a single <code>rows</code> parameter, then itâs a rows processor.</li>
  <li>if it accepts a single <code>package</code> parameter, then itâs a package processor.</li>
</ul>

<p>Letâs see a few examples of what we can do with a package processors.</p>

<p>First, letâs add a field to the data:</p>

<pre><code>from dataflows import Flow, load, dump_to_path


def add_is_guitarist_column_to_schema(package):
	# Add a new field to the first resource
    package.pkg.resources[0]
               .descriptor['schema']['fields']
               .append(dict(
            name='is_guitarist',
            type='boolean'
    ))
    # Must yield the modified datapackage
    yield package.pkg
    # And its resources
    yield from package

def add_is_guitarist_column(row):
	row['is_guitarist'] = row['instrument'] == 'guitar'
    return row

f = Flow(
    # Same one as above
    load('beatles.csv'),
    add_is_guitarist_column_to_schema,
    add_is_guitarist_column,
    dump_to_path('beatles_guitarists')
)
_ = f.process()

</code></pre>

<p>In this example we create two steps - one for adding the new field (<code>is_guitarist</code>) to the schema and another step to modify the actual data.</p>

<p>We can combine the two into one step:</p>

<pre><code>from dataflows import Flow, load, dump_to_path


def add_is_guitarist_column(package):

    # Add a new field to the first resource
    package.pkg.resources[0].descriptor['schema']['fields'].append(dict(
        name='is_guitarist',
        type='boolean'
    ))
    # Must yield the modified datapackage
    yield package.pkg

    # Now iterate on all resources
    resources = iter(package)
    # Take the first resource
    beatles = next(resources)

    # And yield it with with the modification
    def f(row):
        row['is_guitarist'] = row['instrument'] == 'guitar'
        return row

    yield map(f, beatles)

f = Flow(
    # Same one as above
    load('beatles.csv'),
    add_is_guitarist_column,
    dump_to_path('beatles_guitarists')
)
_ = f.process()
</code></pre>

<p>The contract for the <code>package</code> processing function is simple:</p>

<p>First modify <code>package.pkg</code> (which is a <code>Package</code> instance) and yield it.</p>

<p>Then, yield any resources that should exist on the output, with or without modifications.</p>

<p>In the next example weâre removing an entire resource in a package processor - this next one filters the list of Academy Award nominees to those who won both the Oscar and an Emmy award:</p>

<pre><code>    from dataflows import Flow, load, dump_to_path

    def find_double_winners(package):

        # Remove the emmies resource -
        #    we're going to consume it now
        package.pkg.remove_resource('emmies')
        # Must yield the modified datapackage
        yield package.pkg

        # Now iterate on all resources
        resources = iter(package)

        # Emmies is the first -
        # read all its data and create a set of winner names
        emmy = next(resources)
        emmy_winners = set(
            map(lambda x: x['nominee'],
                filter(lambda x: x['winner'],
                       emmy))
        )

        # Oscars are next -
        # filter rows based on the emmy winner set
        academy = next(resources)
        yield filter(lambda row: (row['Winner'] and
                                  row['Name'] in emmy_winners),
                     academy)

    f = Flow(
        # Emmy award nominees and winners
        load('emmy.csv', name='emmies'),
        # Academy award nominees and winners
        load('academy.csv', encoding='utf8', name='oscars'),
        find_double_winners,
        dump_to_path('double_winners')
    )
    _ = f.process()

# --&gt;
# double_winners/academy.csv contains:
# 1931/1932,5,Actress,1,Helen Hayes,The Sin of Madelon Claudet
# 1932/1933,6,Actress,1,Katharine Hepburn,Morning Glory
# 1935,8,Actress,1,Bette Davis,Dangerous
# 1938,11,Actress,1,Bette Davis,Jezebel
# ...
</code></pre>

Builtin Processors

<p>DataFlows comes with a few built-in processors which do most of the heavy lifting in many common scenarios, leaving you to implement only the minimum code that is specific to your specific problem.</p>

<p>A complete list, which also includes an API reference for each one of them, can be found in the DataFlows Built-in Processors page.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Wednesday, 29. August 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Data Factory &amp; DataFlows - An Introduction](http://okfnlabs.org/blog/2018/08/29/data-factory-data-flows-introduction.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Today Iâd like to introduce a new library weâve been working on - dataflows. DataFlows is a part of a larger conceptual framework for data processing.

We call it âData Factoryâ - an open framework for building and running lightweight data processing workflows quickly and easily. LAMP for data wrangling!

Most of you already know what Data Packages are. In short, it is a portable format for packagi

</div>

<div class="item-content item-summary">



  <p>Today Iâd like to introduce a new library weâve been working on - <code>dataflows</code>. DataFlows is a part of a larger conceptual framework for data processing.</p>

<p>We call it âData Factoryâ - an open framework for building and running lightweight data processing workflows quickly and easily. LAMP for data wrangling!</p>

<p>Most of you already know what Data Packages are. In short, it is a portable format for packaging different resources (tabular or otherwise) in a standard way that takes care of most interoperability problems (e.g. âwhatâs the character encoding of the file?â or âwhat is the data type for this column?â or âwhich date format are they using?â). It also provides rich and flexible metadata, which users can then use to understand what the data is about (take a look at frictionlessdata.io to learn more!).</p>

<p>Data Factory complements the Data Package concepts by adding dynamics to the mix.</p>

<p>While Data Packages are a great solution for describing data sets, these data sets are always static - located in one place. Data Factory is all about transforming Data Packages - modifying their data or meta-data and transmitting them from one location to another.</p>

<p>Data Factory defines standard interfaces for building processors - software modules for mutating a Data Package - and protocols for streaming the contents of a Data Package for efficient processing.</p>

Philosophy and Goals

<p>Data Factory is more pattern/convention than library.</p>

<p>An analogy is with web frameworks. Web frameworks were more about a core pattern plus a set of ready to use components than a library themselves. For example, python frameworks were built around WSGI e.g. Pylons, Flask etc. Or consider ExpressJS for Node.</p>

<p>In this sense these frameworks were about convention over configuration. They attempted to decrease the number of decisions that a developer using the framework is required to make without necessarily losing flexibility.</p>

<p>Like web frameworks, Data Factory uses convention over configuration with the aim of decreasing the number of decisions that a data developer is required to make without necessarily losing flexibility.</p>

<p>By following a standard scheme, developers are able to use a large and growing library of existing, reusable processors. This also increases readability and maintainability of data processing code.</p>

<p>Our focus is on:</p>

<ul>
  <li>Small to medium sized data (KBs to GBs)</li>
  <li>Desktop wrangling - people who start on their desktop</li>
  <li>Easy transition from desktop to âcloudâ</li>
  <li>Heterogeneous data sources</li>
  <li>Process using basic building blocks that are extensible</li>
  <li>Less technical audience</li>
  <li>Limited resources - limit on memory, CPU, etc.</li>
</ul>

<p>What are we not?</p>

<ul>
  <li>Big data processing and machine learning. e.g. if you want to wrangle TBs of data in a distributed setup or want to train a machine learning model with GBs of data, you probably donât want this.</li>
  <li>Processing real-time event data.</li>
  <li>Technical know-how is needed: we arenât a fancy ETL UI â you probably need a bit of technical sophistication</li>
</ul>

Architecture

<p>This new framework is built on the foundations of the Frictionless Data project - both conceptually as well as technically. This project provided us the definition of Data Packages and the software to read and write these packages.</p>

<blockquote>
  <p>On top of this Frictionless Data basis, weâre introducing a few new concepts:</p>
  <ul>
    <li>the Data Stream - essentially a Data Package in transit;</li>
    <li>the Data Processor, which manipulates a Data Package, receiving one Data Stream as its input and producing a new Data Stream as its output.</li>
    <li>A chain of Data Processors is what we call a Data Flow.</li>
  </ul>
</blockquote>

<p>We will be providing a library of such processors: some for loading data from various sources, some for storing data in different locations, services or databases, and some for doing common manipulation and transformation on the processed data.</p>

<p>On top of all that weâre building a few integrated services:</p>

<ul>
  <li><code>dataflows-server</code> (formerly known as <code>datapackage-pipelines</code>) - a server side multi-processor runner for Data Flows.</li>
  <li><code>dataflows-cli</code> - a client library for building and running Data Flows locally</li>
  <li><code>dataflows-blueprints</code> - ready made flow generators for common scenarios (e.g. âI want to regularly pull all my analytics from these X services and dump them in a databaseâ)</li>
  <li>and more to come.</li>
</ul>

<p>âŠ</p>

On Data Wrangling

<p>In our experience, data processing starts simple - downloading and inspecting a CSV, deleting a column or a row. We wanted something that was as fast as the command line to get started but would also provide a solid basis as your pipeline grows. We also wanted something that provided some standardization and conventions over completely bespoke code.</p>

<p>With integration in mind, DataFlows comes with very little environmental requirements, and can be embedded in your existing data processing setup.</p>

<p>In short, DataFlows provides a simple, quick and easy-to-setup, and extensible way to build lightweight data processing pipelines.</p>

Introducing dataflows

<p>The first piece of software weâre introducing today is <code>dataflows</code> and its standard library of processors.</p>

<p><code>dataflows</code> introduces the concept of a <code>Flow</code> - a chain of data processors, reading, transforming and modifying a stream of data and writing it to any location (or loading it to memory for further analysis).</p>

<p><code>dataflows</code> also comes with a rich set of built-in data processors, ready to do most of the heavy-lifting youâll need to reduce boilerplate code and increase your productivity.</p>

A demo is worth a thousand words

<p>Most data processing starts simple: getting a file and having a look.</p>

<p>With <code>dataflows</code> you can do this in a few seconds and youâll have a solid basis for whatever you want to do next.</p>

<p>Bootstrapping a data processing script</p>

<pre><code>$ pip install dataflows
$ dataflows init rawgit.com/datahq/demo/_/first.csv
Writing processing code into first_csv.py
Running first_csv.py
first:
  #  Name        Composed    DOB
     (string)    (string)    (date)
---  ----------  ----------  ----------
  1  George      22          1943-02-25
  2  John        90          1940-10-09
  3  Richard     2           1940-07-07
  4  Paul        88          1942-06-18
  5  Brian       n/a         1934-09-19

Done!

</code></pre>

<p><code>dataflows init</code>  actually does 3 things:</p>

<ul>
  <li>Analyzes the source file</li>
  <li>Creates a processing script for reading it</li>
  <li>Runs that script for you</li>
</ul>

<p>In our case, a script named <code>first_csv.py</code> was created - hereâs what it contains:</p>

<pre><code># ...

def first_csv():
    flow = Flow(
        # Load inputs
        load('rawgit.com/datahq/demo/_/first.csv',
             format='csv', ),
        # Process them (if necessary)
        # Save the results
        add_metadata(name='first_csv', title='first.csv'),
        printer(),
    )
    flow.process()

if __name__ == '__main__':
    first_csv()
</code></pre>

<p>The <code>flow</code> variable contains the chain of processing steps (i.e. the processors). In this simple flow, <code>load</code> loads the source data, <code>add_metadata</code> modifies the fileâs metadata and <code>printer</code> outputs the contents to the standard output.</p>

<p>You can run this script again at any time, and it will re-run the processing flow:</p>

<pre><code>$ python first_csv.py
first:
  #  Name        Composed    DOB
     (string)    (string)    (date)
---  ----------  ----------  ----------
  1  George      22          1943-02-25
...
</code></pre>

<p>This is all very nice, but now itâs time for some real data wrangling. By editing the processing script itâs possible to add more functionality to the flow - <code>dataflows</code> provides a simple, solid basis for building up your pipeline quickly, reliably and repeatedly.</p>

<p>Fixing some bad data</p>

<p>Letâs start by getting rid of that annoying <code>n/a</code> in the last line of the data.</p>

<p>We edit <code>first_csv.py</code> and add to the flow two more steps:</p>

<pre><code>def removeNA(row):
    row['Composed'] = row['Composed'].replace('n/a', '')

f = Flow(
        load('rawgit.com/datahq/demo/_/first.csv'),
		# added here custom processing:
	    removeNa,
	    # now parse column as Integer:
        set_type('Composed', type='integer'),
        printer()
    )
</code></pre>

<p><code>removeNa</code> is a simple function which modifies each row it sees, replacing <code>n/a</code>s with the empty string. After it we call <code>set_type</code>, which declares the <code>Composed</code>column should be an integer - and verifies itâs indeed an integer while processing data.</p>

<p>Writing the cleaned data</p>

<p>Finally, letâs write the output to a file using the <code>dump_to_path</code> processor:</p>

<pre><code>def removeNA(row):
    row['Composed'] = row['Composed'].replace('n/a', '')

f = Flow(
		load('rawgit.com/datahq/demo/_/first.csv'),                	
       add_metadata(
            name='beatles_infoz',
            title='Beatle Member Information',
        ),
 	    removeNa,
        set_type('Composed', type='integer'),
        dump_to_path('first_csv/')
    )
</code></pre>

<p>Now, we re-run our modified processing scriptâŠ</p>

<pre><code>$ python first_csv.py
...
</code></pre>

<p>we get a valid Data Package which we can useâŠ</p>
<pre><code>$ tree
âââ first_csv
â   âââ datapackage.json
â   âââ first.csv
</code></pre>

<p>which contains a normalized and cleaned-up CSV fileâŠ</p>
<pre><code>$ head out/out.csv
Name,Composed,DOB
George,22,1943-02-25
John,90,1940-10-09
Richard,2,1940-07-07
Paul,88,1942-06-18
Brian,,1934-09-19
</code></pre>

<p><code>datapackage.json</code>, a JSON file containing the packageâs metadataâŠ</p>
<pre><code>$ cat first_csv/datapackage.json    # Edited for brevity
{
  "count_of_rows": 5,
  "name": "beatles_infoz",
  "title": "Beatle Member Information",
  "resources": [
    {
      "name": "first",
      "path": "first.csv",
      "schema": {
        "fields": [
          {"name": "Name",     "type": "string"},
          {"name": "Composed", "type": "integer"},
          {"name": "DOB",      "type": "date"}
        ]
      }
    }
  ]
}
</code></pre>

<p>and is very simple to use in Python (or JS, Ruby, PHP and many other programming languages) -</p>
<pre><code>$ python
&gt;&gt;&gt; from datapackage import Package
&gt;&gt;&gt; p = Package('first_csv/datapackage.json')
&gt;&gt;&gt; list(p.resources[0].iter())
[['George', 22, datetime.date(1943, 2, 25)],
 ['John', 90, datetime.date(1940, 10, 9)],
 ['Richard', 2, datetime.date(1940, 7, 7)],
 ['Paul', 88, datetime.date(1942, 6, 18)],
 ['Brian', None, datetime.date(1934, 9, 19)]]
&gt;&gt;&gt;
</code></pre>

More âŠ.

<p>Lots, lots more - there is a whole suite of processors built in plus you can quickly add your own with a few lines of python code.</p>

<p>Dig in at the projectâs GitHub Page or continue reading the in-depth tutorial here.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Monday, 07. May 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Processing Tabular Data Packages in Clojure](http://okfnlabs.org/blog/2018/05/07/datapackages-in-clojure.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Matt Thompson was one of 2017âs Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data data package and table schema libraries in Clojure programming language. You can read more about this in his grantee profile. In this post, Thompson will show you how to set up and use the Clojure libraries for working with Tabular Data Packages.



This tutorial uses 

</div>

<div class="item-content item-summary">



  <p>Matt Thompson was one of 2017âs Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data data package and table schema libraries in Clojure programming language. You can read more about this in his grantee profile. In this post, Thompson will show you how to set up and use the Clojure libraries for working with Tabular Data Packages.</p>



<p>This tutorial uses a worked example of downloading a data package from a remote location on the web, and using the Frictionless Data tools to read its contents and metadata into Clojure data structures.</p>

Setup

<p>First, we need to set up the project structure using the Leiningen tool. If you donât have Leiningen set up on your system, follow the link to download and install it. Once it is set up, run the following command from the command line to create the folders and files for a basic Clojure project:</p>

<pre><code>$ lein new periodic-table</code></pre>

<p>This will create the periodic-table folder. Inside the periodic-table/src/periodic-table folder should be a file named core.clj. This is the file you need to edit during this tutorial.</p>

The Data

<p>For this tutorial, we will use a pre-created data package, the Periodic Table Data Package hosted by the Frictionless Data project. A Data Package is a simple container format used to describe and package a collection of data. It consists of two parts:</p>

<ul>
  <li>Metadata that describes the structure and contents of the package</li>
  <li>Resources such as data files that form the contents of the package</li>
</ul>

<p>Our Clojure code will download the data package and process it using the metadata information contained in the
package. The data package can be found here on GitHub.</p>

<p>The data package contains data about elements in the periodic table, including each elementâs name, atomic number, symbol and atomic weight. The table below shows a sample taken from the first three rows of the CSV file:</p>


  
    
      atomic number
      symbol
      name
      atomic mass
      metal or nonmetal?
    
  
  
    
      1
      H
      Hydrogen
      1.00794
      nonmetal
    
    
      2
      He
      Helium
      4.002602
      noble gas
    
    
      3
      Li
      Lithium
      6.941
      alkali metal
    
  


Loading the Data Package

<p>The first step is to load the data package into a Clojure data structure (a map). The initial step is to require the data package library in our code (which we will give the alias dp). Then we can use the load function to load our data package into our project. Enter the following code into the core.clj file:</p>

<pre><code>(ns periodic-table.core
  (:require [frictionlessdata.datapackage :as dp]
            [frictionlessdata.tableschema :as ts]
            [clojure.spec.alpha :as s]))

(def pkg
  (dp/load "raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json"))</code></pre>

<p>This pulls the data in from the remote GitHub location and converts the metadata into a Clojure map. We can access this metadata by using the <code>descriptor</code> function along with keys such as <code>:name</code> and <code>:title</code> to get the relevant information:</p>

<pre><code>(println (str "Package name:" (dp/descriptor pkg :name)))
(println (str "Package title:" (dp/descriptor pkg :title)))</code></pre>

<p>The package descriptor contains metadata that describes the contents of the data package. What about accessing the data itself? We can get to it using the <code>get-resources</code> function:</p>

<pre><code>(def table (dp/get-resources pkg :data))

(doseq [row table]
  (println row))</code></pre>

<p>The above code locates the data in the data package, then goes through it line by line and prints the contents.</p>

Casting Types with core.spec

<p>We can use Clojureâs spec library to define a schema for our data, which can then be used to cast the types of the data in the CSV file.</p>

<p>Below is a spec description of a periodic element type, consisting of an atomic number, atomic symbol, the elementâs name, its mass, and whether or not the element is a metal or non-metal:</p>

<pre><code>(s/def ::number int?)
(s/def ::symbol string?)
(s/def ::name string?)
(s/def ::mass float?)
(s/def ::metal string?)

(s/def ::element (s/keys :req [::number ::symbol ::name ::mass ::metal]))</code></pre>

<p>The above spec can be used to cast values in our tabular data so that they match the specified schema. The example below shows our tabular data values being cast to fit the spec description. Then the <code>-main</code> function loops through the elements, printing only those with an atomic mass of over 10.</p>

<pre><code>(ns periodic-table.core
  (:require [frictionlessdata.datapackage :as dp]
            [frictionlessdata.tableschema :as ts]
            [clojure.spec.alpha :as s]))

(s/def ::number int?)
(s/def ::symbol string?)
(s/def ::name string?)
(s/def ::mass float?)
(s/def ::metal string?)

(s/def ::element (s/keys :req [::number ::symbol ::name ::mass ::metal]))

(def pkg
  (dp/load "raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json"))

(def resources (dp/get-resources pkg :data))

(def elements (dp/cast resources element))

(defn -main []
  (doseq [e elements]
    (if (&lt; (:mass e) 10)
      (println e))))</code></pre>

<p>When run, the program produces the following output:</p>

<pre><code>$ lein run
{::number 1 ::symbol "H" ::name "Hydrogen" ::mass 1.00794 ::metal "nonmetal"}
{::number 2 ::symbol "He" ::name "Helium" ::mass 4.002602 ::metal "noble gas"}
{::number 3 ::symbol "Li" ::name "Lithium" ::mass 6.941 ::metal "alkali gas"}
{::number 4 ::symbol "Be" ::name "Beryllium" ::mass 9.012182 ::metal "alkaline earth metal"}</code></pre>

<p>This concludes our simple tutorial for using the Clojure libraries for Frictionless Data.</p>



<p>We welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-clj repository.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Wednesday, 02. May 2018&#10; 



<article class="item">


#### &#10;  [schema.org News](http://blog.schema.org)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Datacommons.org and Schema.org](http://blog.schema.org/2018/05/datacommonsorg-and-schemaorg.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Over the past few years we have seen a number of application areas benefit from Schema.org markup. Schema.org discussions have often centered around the importance of ease of use, simplicity and adoption for publishers and webmasters. While those principles will continue to guide our work, it is also important to work to make it easier to consume structured data, by building applications and making

</div>

<div class="item-content item-summary">



  Over the past few years we have seen a number of application areas benefit from Schema.org markup. Schema.org discussions have often centered around the importance of ease of use, simplicity and adoption for publishers and webmasters. While those principles will continue to guide our work, it is also important to work to make it easier to consume structured data, by building applications and making more use of the information it carries. We are therefore happy to welcome the new Data&nbsp;Commons initiative, which is devoted to sharing such datasets, beginning with a corpus of fact check data based on the schema.org ClaimReview markup as adopted by many fact checkers around the world. We expect that this work will benefit the wider ecosystem around structured data by encouraging use and re-use of schema.org related datasets. 

  
</div>

<div class="item-footer">
   Published
   20:41 â¢
   over a year ago

   | Updated
   Wednesday, 02. May 2018 20:46 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Saturday, 28. April 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Processing Tabular Data Packages in Java](http://okfnlabs.org/blog/2018/04/28/datapackages-in-java.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Georges LabrÃšche was one of 2017âs Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Java programming language. You can read more about this in his grantee profile.

In this post, LabrÃšche will show you how to install and use the Java libraries for working with Tabular Data Packages.



Our goal in this tutorial is to load tabular data 

</div>

<div class="item-content item-summary">



  <p>Georges LabrÃšche was one of 2017âs Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Java programming language. You can read more about this in his grantee profile.</p>

<p>In this post, LabrÃšche will show you how to install and use the Java libraries for working with Tabular Data Packages.</p>



<p>Our goal in this tutorial is to load tabular data from a CSV file, infer data types and the tableâs schema.</p>

Setup

<p>First things first, youâll want to grab datapackage-java and the tableschema-java libraries.</p>

The Data

<p>For our example, we will use a Tabular Data Package containing the periodic table. You can find the data package descriptor and the data on GitHub.</p>

Packaging
<p>Letâs start by fetching and packaging the data:</p>

<pre><code>// fetch the data
URL url = new URL("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json");

// package the data
Package dp = new Package(url);</code></pre>

<p>Thatâs it, youâre all set to start playing with the packaged data. There are parameters you can set such as loading a schema or imposing strict validation so be sure to go through the projectâs README for more detail.</p>

Iterating

<p>Now that you have a Data Package instance, letâs see what the data looks like. A data package can contain more than one resource so you have to use the <code>Package.getResource()</code> method to specify which resource youâd like to access.</p>

<p>Letâs iterate over the data:</p>

<pre><code>// Get a resource named data from the data package
Resource resource = pkg.getResource("data");

// Get the Iterator
Iterator&lt;String[]&gt; iter = resource.iter();

// Iterate
while(iter.hasNext()){
	String[] row = iter.next();
   	String atomicNumber = row[0];
   	String symbol = row[1];
   	String name = row[2];
  	String atomicMass = row[3];
   	String metalOrNonMetal = row[4];
}</code></pre>

<p>Notice how weâre fetching all values as <code>String</code>. This may not be what you want, particularly for the atomic number and mass. Alternatively, you can trigger data type inference and casting like this:</p>

<pre><code>// Get Iterator.
// Third boolean is the cast flag.
Iterator&lt;Object[]&gt; iter = resource.iter(false, false, true));

// Iterator
while(iter.hasNext()){
	String[] row = iter.next();
   	int atomicNumber = row[0];
   	String symbol = row[1];
   	String name = row[2];
  	float atomicMass = row[3];
   	String metalOrNonMetal = row[4];
}</code></pre>

<p>And thatâs it, your data is now associated with the appropriate data types!</p>

Inferring the Schema

<p>We wouldnât have had to infer the data types if we had included a Table Schema when creating an instance of our Data Package. If a Table Schema is not available, then itâs something that can also be inferred and created with <code>tableschema-java</code>:</p>

<pre><code>URL url = new URL("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/data.csv");
Table table = new Table(url);
Schema schema = table.inferSchema();
schema.write("/path/to/write/schema.json");</code></pre>

<p>The type inference algorithm tries to cast to available types and each successful type casting increments a popularity score for the successful type cast in question. At the end, the best score so far is returned.</p>

<p>The inference algorithm traverses all of the tableâs rows and attempts to cast every single value of the table. When dealing with large tables, you might want to limit the number of rows that the inference algorithm processes:</p>

<pre><code>// Only process the first 25 rows for type inference.
Schema schema = table.inferSchema(25);</code></pre>

<p>Be sure to go through <code>tableschema-java</code>âs README as well to learn more about how to operate with Table Schema.</p>

Contributing
<p>In case you discovered an issue that youâd like to contribute a fix for, or if you would like to extend functionality:</p>

<pre><code># install jabba and maven2
$ cd tableschema-java
$ jabba install 1.8
$ jabba use 1.8
$ mvn install -DskipTests=true -Dmaven.javadoc.skip=true -B -V
$ mvn test -B</code></pre>

<p>Make sure that all tests pass, and submit a PR with your contributions once youâre ready.</p>



<p>We also welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-java repository.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Thursday, 08. March 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Collecting, Analysing and Sharing Twitter Data](http://okfnlabs.org/blog/2018/03/08/open-data-day-tweets.html)&#10;

<div class="item-body">

<div class="item-snippet">


  On March 3, communities around the world marked Open Data Day in over 400 events. Hereâs the dataset for all Open Data Day 2018 events.

In this post, we will harvest Open Data Day affiliated content from Twitter and analyze it using R before packaging and publishing the data and associated resources publicly on GitHub.

Collecting the Data

With over 300million monthly users [source, January 2018]

</div>

<div class="item-content item-summary">



  <p>On March 3, communities around the world marked Open Data Day in over 400 events. Hereâs the dataset for all Open Data Day 2018 events.</p>

<p>In this post, we will harvest Open Data Day affiliated content from Twitter and analyze it using R before packaging and publishing the data and associated resources publicly on GitHub.</p>

Collecting the Data

<p>With over 300million monthly users [source, January 2018], Twitter is a popular social network that I particularly like for its abbreviated messages, known as Tweets. Twitterâs Standard Search API allows users to mine tweets from as far back as a week for free.</p>

<p>R is a popular programming language for data analysis and has an active community of contributors that add to its capabilities by writing custom packages for interacting with different tools and platforms and achieving different tasks. In this post, we will employ two such packages:</p>
<ul>
  <li>twitteR allows us to interact with the Twitter API. We will install this from CRAN, the official packages repository for R.</li>
  <li>Frictionless Dataâs datapackage.r library will allow us to collate our open data day data and associated resources, such as the R script in one place before we publish it. We will install this from GitHub.</li>
</ul>

<p>To get started, create a new application on apps.twitter.com and take note of the API and access tokens. We will need to specify these in our R script.</p>

<pre><code># install and load the twitteR library

install.packages("twitteR")
library(twitteR)


# specify Twitter API and Access Tokens

api_key &lt;- "YOUR_API_KEY"
api_secret &lt;- "YOUR_API_SECRET"
access_token &lt;- "YOUR_ACCESS_TOKEN"
access_secret &lt;- "YOUR_ACCESS_SECRET"

setup_twitter_oauth(api_key, api_secret, access_token, access_secret)</code></pre>

<p>We are now ready to read tweets from the two official Open Data Day hashtags: #opendataday and #odd18. With a maximum number of 100 tweets per request, Twitterâs Search API allows for 180 Requests every 15 minutes. Since we are interested in as many tweets as we can get,  we will specify the upper limit as 18,000, which tells the twitteR library the maximum number of tweets to retrieve for us.</p>

<pre><code># read tweets from the two official hashtags, #opendataday and #odd18

tweets_opendataday &lt;- searchTwitteR("#opendataday", n = 18000)
tweets_odd18 &lt;- searchTwitteR("#odd18", n = 18000)

# view lists of mined tweets from both hashtags

tweets_opendataday
tweets_odd18</code></pre>

<p>Note: run each <code>searchTwitteR()</code> function separately and 15 minutes apart to avoid surpassing the limit.</p>

<p>In the R script snippet above, we assigned results of our search to the variables <code>tweets_opendataday</code> and <code>tweets_odd18</code> and called the two variables to view the entire list of tweets obtained. Lucky for us, the total number of tweets on either hashtag are within Twitterâs 15 minute request limit. Hereâs the feedback we receive:</p>

<pre><code># tweets mined on March 7, 2018

#opendataday
 18000 tweets were requested but the API can only return 11458

#odd18
 18000 tweets were requested but the API can only return 3497</code></pre>

<p>Hereâs a snippet of the list obtained from the #opendataday hashtag:</p>

<pre><code>[[1]]
[1] "OpenDataAnd: Con motivo del pasado #OpenDataDay, @ODIHQ nos recuerda qu&lt;U+00E9&gt; es y para qu&lt;U+00E9&gt; sirven los #DatosAbiertos&lt;U+2026&gt; t.co/Fib4rSukbs"

[[2]]
[1] "johnfaig: RT @ODIHQ: Here's our list of seven weird and wonderful open datasets (nominated by you) t.co/H42bV5oIhw\n\n#opendataday #opendataday&lt;U+2026&gt;"

[[3]]
[1] "SurianoRodrigo: RT @CETGAPue: Desde el auditorio Ing. Antonio Osorio Garc&lt;U+00ED&gt;a de la @fi_buap, se lleva a cabo el BootCamp #OpenDataDay, al que asisten acad&lt;U+00E9&gt;m&lt;U+2026&gt;"

[[4]]
[1] "Carolrozu: RT @CETGAPue: Desde el auditorio Ing. Antonio Osorio Garc&lt;U+00ED&gt;a de la @fi_buap, se lleva a cabo el BootCamp #OpenDataDay, al que asisten acad&lt;U+00E9&gt;m&lt;U+2026&gt;"

[[5]]
[1] "Josefina_Buxade: RT @CETGAPue: Desde el auditorio Ing. Antonio Osorio Garc&lt;U+00ED&gt;a de la @fi_buap, se lleva a cabo el BootCamp #OpenDataDay, al que asisten acad&lt;U+00E9&gt;m&lt;U+2026&gt;"</code></pre>

<p>Since the entire lists are long (~11,500 tweets on the #opendataday hashtag) and hard to comprehend, our best bet is to convert the lists to data frames. In R, data frames allow us to store data in tables and manipulate and analyse them easily. twitteRâs <code>twListToDF</code> function allows us to convert lists to data frames. After scraping data, it is always a good idea to save the original raw data as it provides a good base for any analysis work. We will write our data to a CSV file, so we can publish and it widely. The CSV format is machine-readable and easy to import into any spreadsheet application or advanced tools for analysis.</p>

<pre><code># convert the list of mined tweets from each hashtag to a dataframe

tweets_opendataday_df &lt;- twListToDF(tweets_opendataday)
tweets_odd18_df &lt;- twListToDF(tweets_odd18)

# save scraped data in CSV files

write.csv(tweets_opendataday_df, file="data/opendataday_raw.csv")
write.csv(tweets_odd18_df, file="data/odd18_raw.csv")</code></pre>

<p>Hereâs what the first five rows of our data frame look like:</p>


  
    
      text
      favorited
      favoriteCount
      replyToSN
      created
      truncated
      replyToSID
      id
      replyToUID
      statusSource
      screenName
      retweetCount
      isRetweet
      retweeted
      longitude
      latitude
      &nbsp;
    
  
  
    
      1
      Participan como panelistas de la mesa &lt;U+201C&gt;Datos ma&lt;U+00F1&gt;aneros, qu&lt;U+00E9&gt; son y para qu&lt;U+00E9&gt; sirven los #DatosAbiertos&lt;U+201D&gt;, Karla Ramos&lt;U+2026&gt; t.co/wFBYaUP68n
      FALSE
      3
      NA
      05/03/18 16:29
      TRUE
      NA
      9.70698E+17
      NA
      Twitter for iPhone
      CETGAPue
      2
      FALSE
      FALSE
      NA
      NA
    
    
      2
      RT @Transparen_Xal: A unos minutos de empezar el Open Data Day Xalapa#ODD18 #Xalapa t.co/VH3m0QGeOJ
      FALSE
      0
      NA
      05/03/18 16:28
      FALSE
      NA
      9.70698E+17
      NA
      Hootsuite
      AytoXalapa
      1
      TRUE
      FALSE
      NA
      NA
    
    
      3
      Nos encontramos ya en @ImacXalapa con @AytoXalapa para sumar esfuerzos a favor de la cultura de participaci&lt;U+00F3&gt;n ciuda&lt;U+2026&gt; t.co/VdIcF16Ub4
      FALSE
      0
      NA
      05/03/18 16:22
      TRUE
      NA
      9.70696E+17
      NA
      Twitter for Android
      VERIVAI
      1
      FALSE
      FALSE
      NA
      NA
    
    
      4
      A unos minutos de empezar el Open Data Day Xalapa#ODD18 #Xalapa t.co/VH3m0QGeOJ
      FALSE
      0
      NA
      05/03/18 16:20
      FALSE
      NA
      9.70696E+17
      NA
      Twitter for iPhone
      Transparen_Xal
      1
      FALSE
      FALSE
      NA
      NA
    
    
      5
      El gobierno de @TonyGali promueve el uso de los #DatosAbiertos. Entra al portal t.co/Jz23xpJLAS y consult&lt;U+2026&gt; t.co/UoWP43R8Km
      FALSE
      5
      NA
      05/03/18 16:09
      TRUE
      NA
      9.70693E+17
      NA
      Twitter for iPhone
      CETGAPue
      4
      FALSE
      FALSE
      NA
      NA
    
  


<p>For ease of analysis, and because the two data frames have the same columns, letâs merge the two datasets.</p>

<pre><code># combine dataframes from the two hashtags

alltweets_df &lt;- rbind(tweets_opendataday_df, tweets_odd18_df)
write.csv(alltweets_df, file="data/allopendatadaytweets.csv")</code></pre>

Analysing the Data

<p>Data analysis in R is quite a joy. We will use Râs <code>dplyr</code> package to analyse our data and answer a few questions:</p>

<ul>
  <li>how many open data day attendees tweeted from android phones?</li>
</ul>

<p>We can answer this using dplyrâs select() function, which as the name suggests, allows us to see only data we are interested in, in this case, tweets sent from the Twitter for Android app.</p>

<pre><code># install and load dplyr

install.packages(dplyr)
library(dplyr)

# find out number of open data day tweets from android phones

android_tweets &lt;- filter(alltweets_df, grepl("Twitter for Android", statusSource))
tally(android_tweets)</code></pre>

<pre><code># the result
   n
1 5180</code></pre>

<p>5,180 of the 14,955 (34.6%) #opendataday and #odd18 tweets were sent from android phones.</p>

<ul>
  <li>Naturally, Open Data Day events cut across many topics and disciplines, and some events included hands-on workshop sessions or hackathons. Letâs find out which open data day tweets point to open source projects and resources that are available on GitHub.</li>
</ul>

<pre><code> # open data day tweets that mention resources on GitHub

github_resources &lt;- filter(alltweets_df, grepl("github.com", statusSource))</code></pre>

<pre><code># the result
 n
1 32</code></pre>

<p>Only 32 #opendataday and #odd18 tweets contain GitHub links.</p>

<ul>
  <li>Not all open data day tweets are geotagged, but from the few that are, we can create a very basic map to show where people tweeted from. To do this, we will use the Leaflet library for R.</li>
</ul>

<pre><code># install and load leaflet

install.packages(leaflet)
library(leaflet)

# create basic map
map &lt;- leaflet() %&gt;%
  addTiles() %&gt;%
  addCircles(data = alltweets_df, lat = ~ latitude, lng = ~ longitude)

# view map
map</code></pre>

<p>âŠ
<br>
figure 1: map showing where geotagged #opendataday and #odd18 tweets originated from</p>

Sharing the Data

<p>Due to Twitterâs terms of use, we can only share a stripped-down version of the raw data. Our final dataset contains tweet IDs and retweet count, and will be packaged alongside this R script, so you could download the tweets yourself.</p>

<pre><code># filter out retweets and leave original tweets

  notretweets_df &lt;- dplyr::filter(alltweets_df, grepl("FALSE", isRetweet))

# strip down tweets data to comply with Twitter's terms of use.

  subsetoftweets &lt;- select(notretweets_df, id, retweetCount)
  write.csv(subsetoftweets, file="data/subsetofopendatadaytweets.csv")</code></pre>

Packaging the Data and associated resources

<p>Providing context when sharing data is important, and Frictionless Dataâs Data Package format makes it possible. Using datapackage.r, we can infer a schema for the <code>all tweets</code> CSV file and publish it alongside the other resources.</p>

<pre><code># specify filepath and infer schema
    filepath = '/data/subsetofopendatadaytweets.csv'

    schema = tableschema.r::infer(filepath)</code></pre>

<p>Read more about the datapackage-r package in this post by Open Knowledge Greece.</p>

<p>Alternatively, we can use the Data Package Creator  to package our data and associated resources.</p>

<p>âŠ
<br>
figure 2: creating the data package on Data Package Creator</p>

<p>Read more about the data package creator in this post.</p>

Publishing on Github

<p>Once our data package is ready, we can simply publish it to GitHub. Find the open data day tweets data package here.</p>

Conclusion

<p>Data Packages are a great format for sharing data collections with contextual information i.e. we added metadata and a schema to our final dataset. Read more about Data Packages in Frictionless Data and reach out in our community chat on Gitter.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Friday, 16. February 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Processing Tabular Data Packages in Go](http://okfnlabs.org/blog/2018/02/16/datapackages-in-go.html)&#10;

<div class="item-body">

<div class="item-snippet">


  Daniel Fireman was one of 2017âs Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Go programming language. You can read more about this in his grantee profile.

In this post, Fireman will show you how to install and use the Go libraries for working with Tabular Data Packages.



Our goal in this tutorial is to load a data package from 

</div>

<div class="item-content item-summary">



  <p>Daniel Fireman was one of 2017âs Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Go programming language. You can read more about this in his grantee profile.</p>

<p>In this post, Fireman will show you how to install and use the Go libraries for working with Tabular Data Packages.</p>



<p>Our goal in this tutorial is to load a data package from the web and read its metadata and contents.</p>

Setup
<p>For this tutorial, we will need the datapackage-go and tableschema-go packages, which provide all the functionality to deal with a Data Packageâs metadata and its contents.</p>

<p>We are going to use the dep tool to manage the dependencies of our new project:</p>

<pre><code>$ cd $GOPATH/src/newdataproj
$ dep init</code></pre>

The Periodic Table Data Package

<p>A Data Package is a simple container format used to describe and package a collection of data. It consists of two parts:</p>

<ul>
  <li>Metadata that describes the structure and contents of the package</li>
  <li>Resources such as data files that form the contents of the package</li>
</ul>

<p>In this tutorial, we are using a Tabular Data Package containing the periodic table. The package descriptor (datapackage.json) and contents (data.csv) are stored on GitHub. This dataset includes the atomic number, symbol, element name, atomic mass, and the metallicity of the element. Here are the header and the first three rows:</p>


  
    
      atomic number
      symbol
      name
      atomic mass
      metal or nonmetal?
    
  
  
    
      1
      H
      Hydrogen
      1.00794
      nonmetal
    
    
      2
      He
      Helium
      4.002602
      noble gas
    
    
      3
      Li
      Lithium
      6.941
      alkali metal
    
  


Inspecting Package Metadata

<p>Letâs start off by creating the <code>main.go</code>, which loads the data package and inspects some of its metadata.</p>

<pre><code>package main

import (
    "fmt"

    "github.com/frictionlessdata/datapackage-go/datapackage"
)

func main() {
    pkg, err := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
    if err != nil {
        panic(err)
    }
    fmt.Println("Package loaded successfully.")
}</code></pre>

<p>Before running the code, you need to tell the dep tool to update our project dependencies. Donât worry; you wonât need to do it again in this tutorial.</p>

<pre><code>$ dep ensure
$ go run main.go
Package loaded successfully.</code></pre>

<p>Now that you have loaded the periodic table Data Package, you have access to its <code>title</code> and <code>name</code> fields through the Package.Descriptor() function.  To do so, letâs change our main function to (omitting error handling for the sake of brevity, but we know it is very important):</p>

<pre><code>func main() {
    pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
    fmt.Println("Name:", pkg.Descriptor()["name"])
    fmt.Println("Title:", pkg.Descriptor()["title"])
}</code></pre>

<p>And rerun the program:</p>

<pre><code>$ go run main.go
Name: period-table
Title: Periodic Table</code></pre>

<p>And as you can see, the printed fields match the package descriptor. For more information about the Data Package structure, please take a look at the specification.</p>

Quick Look At the Data

<p>Now that you have loaded your Data Package, it is time to process its contents. The package content consists of one or more resources. You can access Resources via the Package.GetResource() method. Letâs print the periodic table <code>data</code> resource contents.</p>

<pre><code>func main() {
    pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
    res := pkg.GetResource("data")
    table, _ := res.ReadAll()
    for _, row := range table {
        fmt.Println(row)
    }
}</code></pre>

<pre><code>$ go run main.go
[atomic number symbol name atomic mass metal or nonmetal?]
[1 H Hydrogen 1.00794 nonmetal]
[2 He Helium 4.002602 noble gas]
[3 Li Lithium 6.941 alkali metal]
[4 Be Beryllium 9.012182 alkaline earth metal]
...</code></pre>

<p>The Resource.ReadAll() method loads the whole table in memory as raw strings and returns it as a Go <code>[][]string</code>. This can be quick useful to take a quick look or perform a visual sanity check at the data.</p>

Processing the Data Packageâs Content

<p>Even though the string representation can be useful for a quick sanity check, you probably want to use actual language types to process the data. Donât worry, you wonât need to fight the casting battle yourself. Data Package Go libraries provide a rich set of methods to deal with data loading in a very idiomatic way (very similar to encoding/json).</p>

<p>As an example, letâs change our <code>main</code> function to use actual types to store the periodic table and print the elements with atomic mass smaller than 10.</p>

<pre><code>package main

import (
    "fmt"

    "github.com/frictionlessdata/datapackage-go/datapackage"
    "github.com/frictionlessdata/tableschema-go/csv"
)

type element struct {
    Number int     `tableheader:"atomic number"`
    Symbol string  `tableheader:"symbol"`
    Name   string  `tableheader:"name"`
    Mass   float64 `tableheader:"atomic mass"`
    Metal  string  `tableheader:"metal or nonmetal?"`
}

func main() {
    pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
    resource := pkg.GetResource("data")

    var elements []element
    resource.Cast(&amp;elements, csv.LoadHeaders())
    for _, e := range elements {
        if e.Mass &lt; 10 {
            fmt.Printf("%+v\n", e)
        }
    }
}</code></pre>

<pre><code>$ go run main.go
{Number:1 Symbol:H Name:Hydrogen Mass:1.00794 Metal:nonmetal}
{Number:2 Symbol:He Name:Helium Mass:4.002602 Metal:noble gas}
{Number:3 Symbol:Li Name:Lithium Mass:6.941 Metal:alkali metal}
{Number:4 Symbol:Be Name:Beryllium Mass:9.012182 Metal:alkaline earth metal}</code></pre>

<p>In the example above, all rows in the table are loaded into memory. Then every row is parsed into an <code>element</code> object and appended to the slice. The <code>resource.Cast</code> call returns an error if the whole table cannot be successfully parsed.</p>

<p>If you donât want to load all data in memory at once, you can lazily access each row using Resource.Iter and use Schema.CastRow to cast each row into an <code>element</code> object. That would change our main function to:</p>

<pre><code>func main() {
    pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
    resource := pkg.GetResource("data")

    iter, _ := resource.Iter(csv.LoadHeaders())
    sch, _ := resource.GetSchema()
    var e element
    for iter.Next() {
        sch.CastRow(iter.Row(), &amp;e)
        if e.Mass &lt; 10 {
            fmt.Printf("%+v\n", e)
        }
    }
}</code></pre>

<pre><code>$ go run main.go
{Number:1 Symbol:H Name:Hydrogen Mass:1.00794 Metal:nonmetal}
{Number:2 Symbol:He Name:Helium Mass:4.002602 Metal:noble gas}
{Number:3 Symbol:Li Name:Lithium Mass:6.941 Metal:alkali metal}
{Number:4 Symbol:Be Name:Beryllium Mass:9.012182 Metal:alkaline earth metal}</code></pre>

<p>And our code is ready to deal with the growth of the periodic table in a very memory-efficient way :-)</p>



<p>We welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-go repository.</p>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>








 
## &#10;  Thursday, 15. February 2018&#10; 



<article class="item">


#### &#10;  [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)&#10;



### &#10; &#10; <i class="icon-caret-down item-collapse item-close item-opened"></i>&#10; <i class="icon-caret-right item-expand item-open item-closed"></i>&#10; [Frictionless Data Lib - A Design Pattern for Accessing Files and Datasets](http://okfnlabs.org/blog/2018/02/15/design-pattern-for-a-core-data-library.html)&#10;

<div class="item-body">

<div class="item-snippet">


  This document outlines a simple design pattern for a âcoreâ data library "data".

The pattern is focused on access and use of:


  individual files (streams)
  collections of files (âdatasetsâ)


Its primary operation is open:

file = open('path/to/file.csv')
dataset = open('path/to/files/')


It defines a standardized âstream-plus-metadataâ interface for file and dataset objects, along with method

</div>

<div class="item-content item-summary">



  <p>This document outlines a simple design pattern for a âcoreâ data library <code>"data"</code>.</p>

<p>The pattern is focused on access and use of:</p>

<ul>
  <li>individual files (streams)</li>
  <li>collections of files (âdatasetsâ)</li>
</ul>

<p>Its primary operation is <code>open</code>:</p>

<pre><code>file = open('path/to/file.csv')
dataset = open('path/to/files/')
</code></pre>

<p>It defines a standardized âstream-plus-metadataâ interface for file and dataset objects, along with methods for creating these from file or dataset pointers such as file paths or urls.</p>

<pre><code>file = open('path/to/file.csv')
file.stream()
file.rows()
file.descriptor
file.descriptor.path
...
</code></pre>

<p>This pattern derives from many years experience working on data tools and projects like Frictionless Data. Specifically:</p>

<ul>
  <li>Data âplusâ: when you work with data you always find yourself needing the data itself plus a little bit more â things like where the data came from on disk (or is going to), or how large it is. This pattern gives you that information in a standardized way.</li>
  <li>Streams (and strings): streams are the standard way to access data (though strings are useful too) and you should get the same interface whether youâve loaded data from online, on disk or inline; and, finally, we want both raw byte streams and (for tabular data) object/row streams aka iterators.</li>
  <li>Building blocks: most data wrangling, even in simple cases, involves building data processing pipelines. Pipelines need a standard stream-plus-metadata interface to pass data between steps in the pipeline. For example, suppose you want to load a csv file and convert to JSON and write to stdout: thatâs already three steps (load, convert, dump). Then suppose you want to delete the first 3 rows, delete the 2nd column. Now you have a more complex processing pipeline.</li>
</ul>



<p>âŠ</p>
<p>Fig 1: data pipelines and the stream-plus-metadata pattern</p>

<p>The pattern leverages the Frictionless Data specs including Data Resource, Table Schema and Data Package. But it keeps metadata implicit rather than explicit and  focuses on giving users the simplest most direct interface possible (put most crudely: <code>open</code> then <code>stream</code>). You can find more about the connection with the Frictionless Data tooling in the appendix.</p>

<p>Finally, we already have one working implementation of the pattern in javascript:</p>

<p>github.com/datahq/data.js</p>

<p>Work on a python implementation is underway (most of the code is already there in the python Data Package libraries).</p>

<p>Table of Contents</p>

<ul>
  <li>Overview of the Pattern</li>
  <li>The Pattern in Detail    <ul>
      <li><code>open</code> method        <ul>
          <li>File locators</li>
        </ul>
      </li>
      <li>File        <ul>
          <li>Metadata: <code>descriptor</code></li>
          <li>Accessing data            <ul>
              <li><code>stream</code></li>
              <li><code>rows</code>                <ul>
                  <li>Support for TableSchema and CSV Dialect</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Dataset        <ul>
          <li><code>open</code> for datasets            <ul>
              <li>Dataset Locators</li>
            </ul>
          </li>
          <li><code>descriptor</code></li>
          <li><code>identifier</code> (optional)</li>
          <li>README</li>
          <li><code>files</code></li>
          <li>addFile</li>
        </ul>
      </li>
      <li>Operators</li>
    </ul>
  </li>
  <li>Conclusion</li>
  <li>Appendix: Why we need a pattern like this    <ul>
      <li>All data wrangling tools need to load and then pass around âfile-like objectsâ as they process data</li>
      <li>A file is more than a byte stream: the stream may be structured and there is usually the need for associated metadata</li>
      <li>Tool authors find themselves inventing their own âstream-plus-metadataâ objects âŠ but they are all different</li>
      <li>Plus, many tools also need to access collection of files, i.e. datasets</li>
      <li>Having a common API pattern for files (stream-plus-metadata) and datasets would reduce duplication and support plug and play with tooling</li>
    </ul>
  </li>
  <li>Appendix: Design Principles    <ul>
      <li>Orient to the data wrangler workflow</li>
      <li>Zen - maximum viable simplicity        <ul>
          <li>Core objects should be kept as simple as possible (and no simpler)</li>
        </ul>
      </li>
      <li>Use Streams</li>
    </ul>
  </li>
  <li>Appendix: Internal Library Structure Suggestions    <ul>
      <li>Library Components</li>
      <li>Streams</li>
      <li>Loaders/Parsers and Writers</li>
    </ul>
  </li>
  <li>Appendix: API with Data Package terminology</li>
  <li>Appendix: Connection with Frictionless Data    <ul>
      <li>Recommendations for Frictionless Data community</li>
      <li>Why do it?</li>
      <li>Relation to Data Packages</li>
    </ul>
  </li>
</ul>

Overview of the Pattern

<p>The pattern is based on the following principles:</p>

<ul>
  <li>Data wrangler focused: focus on the core data wrangler workflow: open a file and do something with it</li>
  <li>Zen-like: Simplicity and power. As simple as possible: does just what it needs and no more.</li>
  <li>Use Streams: stream focused library, including object streams (aka iterators).</li>
</ul>

<p>A minimal viable interface for the file case:</p>

<pre><code>// this example uses javascript but the example is generic

// data.js is just an illustrative name for the library
const data = require('data.js')

// path can be local or remote
// file is now a data.File object
const file = data.open(pathOrUrl)

// a byte stream
file.stream()

// if this file is tabular this will give me a row stream (iterator)
file.rows()

// descriptor for this file including info like size (if available)
// the descriptor follows the Data Resource specification
// (and if Tabular the Tabular Data Resource spec)
file.descriptor
</code></pre>

<p>For datasets:</p>

<pre><code>// path or url to a directory (or datapackage.json)
// dataset is a data.Dataset object
// note: may rename to openDataset if need to disambiguate from open(file)
const dataset = open(pathOrUrl)

// list of files
dataset.files

// readme (if README.md existed)
dataset.readme

// any metadata (either inferred or from datapackage.json)
// this follows the Data Package spec
dataset.descriptor
</code></pre>

<p>These interfaces can then form the standard basis for lots of additional functionality e.g.</p>

<pre><code>infer(file) =&gt; inferred tableschema (and types) for the column

writer(file) =&gt; stream (for saving to disk)

validate(file) =&gt; validate a file (assumes it has a tableschema)
</code></pre>

<p>NOTE: here we have used <code>file</code> and <code>dataset</code> terminology. If you are more familiar with the package and resource of the Frictionless Data specs please mentally substitute file =&gt; resource and dataset =&gt; package.</p>

The Pattern in Detail

<p>Note: Support for Datasets is optional. Supporting datasets is an added layer of complexity and some implementors MAY choose to support files only. If so, they MUST indicate this clearly.</p>

<code>open</code> method

<p>The library MUST provide a method <code>open</code> which takes a locator to a file and returns a File object:</p>

<pre><code>open(path/to/file.csv, [options]) =&gt; File object
</code></pre>

<p><code>options</code> is a dictionary of keyword argument list of options. The library MUST support an option <code>basePath</code>. <code>basePath</code> is for cases where you want to create a File with a path that is relative to a base directory / path e.g.</p>

<pre><code>file = open('data.csv', {basePath: '/my/base/path'})
</code></pre>

<p>Will open the file: <code>/my/base/path/data.csv</code></p>

<p>This functionality is mainly useful when using Files as part of Datasets where it can be convenient for a  File to have a path relative to the directory of the Dataset. (See also Data Package and Data Resource in the Frictionless Data specs).</p>

File locators

<p>Locators can be:</p>

<ul>
  <li>A file path</li>
  <li>A URL</li>
  <li>Raw data in JSON format</li>
  <li>A Data Resource (in native language structure)</li>
</ul>

<p>Implementors MUST support file paths, SHOULD support URLs and MAY support the last two.</p>

<pre><code>file = open('/path/to/file.csv')

file = open('example.com/data.xls')

// loading raw data
file = open({
  name: 'mydata',
  data: { // can be any javascript - an object, an array or a string or ...
    a: 1,
    b: 2
  }
})

// Loading with a descriptor - this allows more fine-grained configuration
// The descriptor should follow the Frictionless Data Resource model
// specs.frictionlessdata.io/data-resource/
file = open({
  // file or url path
  path: 'example.com/data.csv',
  // a Table Schema - specs.frictionlessdata.io/table-schema/
  schema: {
    fields: [
      ...
    ]
  }
  // CSV dialect - specs.frictionlessdata.io/csv-dialect/
  dialect: {
    // this is tab separated CSV/DSV
    delimiter: '\\t'
  }
})
</code></pre>

File

<p>The File instance MUST have the following properties and methods</p>

Metadata: <code>descriptor</code>

<p>Main metadata is available via the <code>descriptor</code>:</p>

<pre><code>file.descriptor
</code></pre>

<p>The descriptor follows the Frictionless Data Data Resource spec.</p>

<p>The descriptor metadata is a combination of the metadata passed in at File creation (if you created the File with a descriptor object) and auto-inferred information from the File path. This is the info that SHOULD be auto-inferred:</p>

<pre><code>path: path this was instantiated with - may not be same as file.path (depending on basePath)
pathType: remote | local
name:   file name (without extension)
format: the extension
mediatype: mimetype based on file name and extension
</code></pre>

<p>In addition to this metadata there are certain properties which MAY be computed on demand and SHOULD be available as getters on the file object:</p>

<pre><code>// the full path to the file (using basepath)
const path = file.path

const size = file.size

// md5 hash of the file
const hash = file.hash

// file encoding
const encoding = file.encoding
</code></pre>

<p>Note: size, hash are not available for remote Files (those created from urls).</p>

Accessing data

<p>Accessing data in the file:</p>

<pre><code>// byte stream
file.stream()

// if file is tabular
// crude rows - no type casting etc
file.rows(cast=False, keyed=False, ...)

// entire file as a buffer/string (be careful with large files!)
file.buffer()

// (optional)
// if tabular return entire set of rows as an array
file.array()

// EXPERIMENTAL
// file object packed into a stream
// metadata is first line (\n separated)
// motivation: way to send object over single stdin/stdout pipe
file.packed()
</code></pre>

<code>stream</code>

<p>A raw byte stream:</p>

<pre><code>stream()
</code></pre>

<code>rows</code>

<p>Get the rows for this file as an object stream / iterator.</p>

<pre><code>file.rows(cast=False, keyed=False, ...) =&gt;
  iterator with items [val1, val2, val3, ...]
</code></pre>

<ul>
  <li><code>keyed</code>: if <code>false</code> (default) returns rows as arrays i.e. [val1, val2, val3]. If <code>true</code> returns rows as objects i.e.. <code>{col1: val1, col2: val2, ...}</code>.</li>
  <li><code>cast</code>: if <code>false</code> (default) returns values uncast. If true attempts to cast values either using best-effort or TableSchema if available</li>
  <li><code>addRowNumber</code>: default <code>false</code>. Add first value or column <code>_id</code> to resulting rows with row number. [OPTIONAL for implementors]</li>
</ul>

<p>Note: this method assumes underlying data is tabular. Library should raise appropriate error if called on a non-tabular file. It is also up to implementors what tabular formats they support (there are many). At a minimum the library MUST support CSV. It SHOULD support JSON and it MAY (it is desirable) support Excel.</p>

Support for TableSchema and CSV Dialect

<p>The library SHOULD support Table Schema and CSV Dialect in the <code>rows</code> method using metadata provided when the file was <code>open</code>ed:</p>

<pre><code>
// load a CSV with a non-standard dialect e.g. tab separated or semi-colon separated
file = open({
  path: 'mydata.tsv'
  // Full support for specs.frictionlessdata.io/csv-dialect/
  dialect: {
    delimiter: '\\t' // for tabs or ';' for semi-colons etc
  }
})
file.rows() // use the dialect info in parsing the csv

// open a CSV with a Table Schema
file = open({
  path: 'mydata.csv'
  // Full support for Table Schema specs.frictionlessdata.io/table-schema/
  schema: {
    fields: [
      {
        name: 'Column 1',
        type: 'integer'
      },
      ...
    ]
  }
})
</code></pre>

Dataset

<p>A collection of data files with optional metadata.</p>

<p>Under the hood it heavily uses Data Package formats and it natively supports Data Package formats including loading from <code>datapackage.json</code> files. However, it does not require knowledge or use of Data Packages.</p>

<p>A Dataset has two key properties:</p>

<pre><code>// metadata
dataset.descriptor

// files in the dataset
dataset.files
</code></pre>

<code>open</code> for datasets

<p>The library MUST provide a method <code>openDataset</code> that takes a locator to a dataset and returns a Dataset object:</p>

<pre><code>openDataset(path/to/dataset/) =&gt; Dataset object
</code></pre>

<p>The library MAY overload the <code>open</code> method to support datasets as well as files:</p>

<pre><code>open(path/to/dataset/) =&gt; Dataset object
</code></pre>

<p>Note: overloading can be tricky as disambiguating locators for files from locators for datasets is not always trivial.</p>

Dataset Locators

<p><code>path/to/dataset</code> - can be one of:</p>

<ul>
  <li>local path to Dataset</li>
  <li>remote url to Dataset</li>
  <li>descriptor object (i.e. datapackage.json)</li>
</ul>

<code>descriptor</code>

<p>A Dataset MUST have a <code>descriptor</code> which holds the Dataset metadata. The descriptor MUST follow the Data Package spec.</p>

<p>The Dataset SHOULD have the convenience attribute <code>path</code> which is the path (remote or local) to this dataset.</p>

<code>identifier</code> (optional)

<p>A Dataset MAY have a <code>identifier</code> property that encapsulates the location (or origin) of this Dataset. The locator property MUST have the following structure:</p>

<pre><code>{
  name: &lt;name&gt;,   // computed from path
  owner: &lt;owner&gt;, // may be null
  path: &lt;path&gt;,   // computed path
  type: &lt;type&gt;,   // e.g. local, url, github, datahub, ... 
  original: &lt;path&gt;, // path (file or url) as originally supplied
  version: &lt;version&gt; // version as computed
}
</code></pre>

<p>Note: the identifier is parsed from the locator passed into the open method. See the Data Package identifier spec frictionlessdata.io/specs/data-package-identifier/ and implementation in data.js library github.com/datahq/data.js#parsedatasetidentifier</p>

README

<p>The Dataset object MAY support a <code>readme</code> property which returns a string corresponding to the README for this Dataset (if it exists).</p>

<p>The readme content is taken from the README.md file located in the Dataset root directory, or, if that does not exist from the <code>readme</code> property on the descriptor. If neither of those exist the readme will be undefined or null.</p>

<code>files</code>

<p>A Dataset MUST have <code>files</code> property which returns an array of the Files contained in this Dataset:</p>

<pre><code>dataset.files =&gt; Array(&lt;File&gt;)
</code></pre>

addFile

<p>The library SHOULD implement an <code>addFile</code> method to add a <code>File</code> to a Dataset:</p>

<pre><code>dataset.addFile(file)
</code></pre>

<ul>
  <li><code>file</code>: an already instantiated File object or a File descriptor</li>
</ul>

Operators

<p>Finally, we discuss some operators. These SHOULD not be in the core library but it is useful to be aware of them:</p>

<ul>
  <li><code>infer(file) =&gt; TableSchema</code>: infer the Table Schema for a CSV file or other tabular file
    <ul>
      <li><code>inferStructure(file)</code>: infer the structure i.e. CSV Dialect of a CSV or other tabular file. In addition to CSV dialect properties this may include things like <code>skipRows</code> i.e. number of rows to skip</li>
    </ul>
  </li>
  <li><code>validate(file/dataset, metadataOnly=False)</code>: validate the data in a file e.g. against its schema
    <ul>
      <li><code>metadataOnly</code>: only validate the metadata e.g. against the Data Package or Data Resource schemas.</li>
    </ul>
  </li>
  <li><code>write(file/dataset)</code>: write a File or Dataset to disk</li>
</ul>

Conclusion

<p>In this document weâve outlined a âFrictionless Data Libraryâ pattern that standardizes the design of a âcoreâ data library API focused on accessing files and datasets.</p>

<p>Almost all data wrangling work involves opening data streams and passing them between processes. Standardizing the API would have major benefits for tool creators and users, making it quicker and easier to develop tooling as well as making tooling more âplug and playâ.</p>

Appendix: Why we need a pattern like this

All data wrangling tools need to load and then pass around âfile-like objectsâ as they process data

<p>All data tools need to access files/streams:</p>

<pre><code>data = open(path/to/file.csv)
</code></pre>

<p>And so every programming language and every tool have a method for opening a file path and returning a byte-stream.</p>

<p>But âŠ</p>

A file is more than a byte stream: the stream may be structured and there is usually the need for associated metadata

<p>Often we need more than just a byte stream, for example:</p>

<ul>
  <li>We may want the stream to be structured: if it is a CSV file weâre opening weâd like to get a stream of row objects not just a stream of bytes</li>
  <li>We may want file metadata to be available (where did the file come from, how big is the file, when was it last modified)</li>
  <li>We may want schema information: not just the CSV file but type information on its columns (this would allow us to reliably cast the CSV data to proper types when reading)</li>
  <li>And we may even want to add metadata ourselves (perhaps automatedly), for example guessing the types of the columns in a CSV</li>
</ul>

<p>A file is more than a byte stream: the stream may be structured and there is usually the need for associated metadata, at a minimum the name and size of the file but also extending to things like a file schema.</p>

Tool authors find themselves inventing their own âstream-plus-metadataâ objects âŠ but they are all different

<p>Tool authors find themselves inventing their own file-like âstream-plus-metadataâ objects to describe the files they open.</p>

<p>Note: Many languages have âfile-likeâ object that usually consists of a stream plus some metadata (e.g. python <code>file</code> object, Node Streams etc). But this is not standardized and is often inadequate so tool makers end up wrapping or replacing it.</p>

<p>This is not just about opening files but about passing streams around with because most tools, even very simple ones, start to contain implicit mini data pipelines:</p>



<p>âŠ</p>

<p>These stream-plus-metadata objects contain implicit mini-metadata standards for describing files and collections of files (âdatasetsâ). These mini-metadata standards look like Data Resource, Table Schema, Data Package etc.</p>

<p>But these stream-plus-metadata objects and their mini-metadata are all a little different across the various languages and tools.</p>

Plus, many tools also need to access collection of files, i.e. datasets

<p>Many tools want to access collections of files e.g. datasets:</p>

<pre><code>dataset = open(/path/to/dataset)
</code></pre>

<p>Datasets already require some structure to list their collection of files and usually require some additional metadata ranging from where the dataset was loaded from to items such as its license.</p>

<p>You can even have datasets without multiple files when the file you are using is implicitly a dataset. For example, an Excel file is really a dataset if you think of each sheets as a separate file stream or think of an sqlite database.</p>

Having a common API pattern for files (stream-plus-metadata) and datasets would reduce duplication and support plug and play with tooling

<p>Standardizing the structure of these stream-plus-metadata file objects (and dataset objects), and building standard libraries to create them from file/dataset pointers would:</p>

<ul>
  <li>Reduce repetition / allow for reuse across tools: at present, data wrangling tool write this themselves. They now have a common pattern and may even be able to use a common underlying library.</li>
  <li>Support plug and play: new wrangling can operate on these standard file and dataset objects. For example, an inference library that given a file object returns an inferred schema, or a converter that converts xls =&gt; csv.</li>
</ul>

Appendix: Design Principles

<p>The pattern is based on the following principles:</p>

<ul>
  <li>Data wrangler focused: focus on the core data wrangler workflow: open a file and do something with it</li>
  <li>Zen-like: Simplicity and power. As simple as possible: does just what it needs and no more.</li>
  <li>Use Streams: stream focused library, including object streams.</li>
</ul>

Orient to the data wrangler workflow

<p>See motivation section above</p>

<ul>
  <li>Open =&gt; Read / Stream</li>
  <li>[optional] Inspect</li>
  <li>Check</li>
  <li>Operate on</li>
  <li>Write</li>
</ul>

Zen - maximum viable simplicity

<p>As simple as possible. Does just what it needs and no more. Simple and powerful.</p>

<p>Zen =&gt;</p>

<ul>
  <li>âthinâ (vs fat) objects: all complex operators such as infer or dump operate on objects rather than becoming part of them</li>
  <li>a single open method to get data (file or dataset)</li>
  <li>hide metadata by default (data package, data resource etc are in the background)</li>
</ul>

Core objects should be kept as simple as possible (and no simpler)

<p>=&gt; Inversion of control where possible so that we donât end up with âfatâ core classes e.g.</p>

<p>A. save data to disk should be separate objects that operate on the main objects rather than built into them e.g.</p>

<pre><code>const writer = CSVWriter()
writer.writer(dataLibFileObjectInstance, filePath, [options])
</code></pre>

<p>rather than e.g.</p>

<pre><code>dataLibFileObjectInstance.saveToCsv(filePath)
</code></pre>

<p>If there is a simple way to invert dependency (i.e. not have all different dumper in main lib) but have a simple save method that would be fine.</p>

<p>B. Similarly for parsers (though reading is so essential that read needs to be part of of the class)</p>

<p>C. infer, validate etc should operate on Files rather than be part of it âŠ</p>

<pre><code>const tableschema = infer(fileObj)
</code></pre>

<p>Rather than</p>

<pre><code>fileObj.inferSchema()
</code></pre>

Use Streams

<p>Streams are the natural way to handle data and it scales to large datasets.</p>

<p>The library should be stream focused library, including object streams.</p>

Appendix: Internal Library Structure Suggestions

<p>These are some suggestions for how implementors could structure their library internally. They are entirely optional.</p>

Library Components

<p>In top level library just have Dataset and File (+ TabularFile)</p>

<pre><code>graph TD

Dataset[Dataset/Package] --&gt; File[File/Resource]
File --&gt; TabularFile
TabularFile -.-&gt; TableSchema
TableSchema -.-&gt; Field

parsers((Parsers))
dumpers((Writers))
tools((Tools))

tools --&gt; infer
infer --&gt; validate

classDef medium fill:lightblue,stroke:#333,stroke-width:4px;
</code></pre>

<pre><code>graph TD

parsers((Parsers))
dumpers((Writers))

subgraph Parsers - Tabular
  csv["CSV parse(resource) -&gt; row stream"]
  xls["XLS ..."]
end

subgraph Writers - Tabular
  ascii
  csvdump[CSV]
  xlsdump[XLS]
  markdown
end

parsers --&gt; csv
parsers --&gt; xls

dumpers --&gt; ascii
dumpers --&gt; markdown
dumpers --&gt; xlsdump
dumpers --&gt; csvdump
</code></pre>

Streams

<pre><code>graph LR

in1[File,URL,Stream] -- stream--&gt; stream[Byte Stream + Meta]
stream --parse--&gt; objstream[Obj Stream+Meta]
objstream --unparse--&gt; stream2[Byte Stream + Meta]
stream --write--&gt; out
stream2 --writer--&gt; out[file/stream]
</code></pre>

<p>open =&gt; yields descriptor and file stream
parse =&gt; yields file rows (internally uses parsers)
writer =&gt; writers</p>

<pre><code>// aka write
writer(File) =&gt; readable stream

parser(File) =&gt; object stream
</code></pre>

Loaders/Parsers and Writers

<p>Loaders/Parsers and Writers should be be an extensible list.</p>

<p>Inversion of control is important: the core library does not depend directly on parsers (that way we can hot swap and/or extend the list at runtime).</p>

<p>Parsers:</p>

<pre><code>// file is a data.File object
parse(file) =&gt; row stream
</code></pre>

<p>Writers are similar:</p>

<pre><code>// e.g. csv.js

// dump to CSV file
write(file, path) =&gt; null
</code></pre>

<p>Note we may want a writer for datasets as well e.g. a writer to datapackage.json or to sql or âŠ</p>

<pre><code>write(dataset, destination ...)
</code></pre>

Appendix: API with Data Package terminology

<p>In progress</p>

<pre><code>// data.js is just an illustrative name for the library
var data = require('data.js')

// path can be local or remote
const resource = data.open(pathOrUrl)

// a byte stream
resource.stream()

// if this file is tabular this will give me a row stream (iterator)
resource.rows()
</code></pre>

<p>For packages</p>

<pre><code>// path or url to a directory (or datapackage.json)
const package = data.open(pathOrUrl)

// list of files
package.resources

// readme (if README.md exists or there is a description in the metadata)
package.readme

// any metadata (either inferred or from datapackage.json)
package.descriptor
</code></pre>

Appendix: Connection with Frictionless Data

<p>Iâve distilled this pattern out of the work of myself and others who have worked on Frictionless Data specs and tooling.</p>

<p>It is motivated by the following observations about the Data Package suite of libraries and their Table Schema, Data Resource and Data Package interfaces:</p>

<ol>
  <li>These libraries contain functions and metadata that standardize operations that are common to almost all data wrangling tools because almost all data wrangling tools need to handle files/streams and datasets and the core metadata is designed around describing files and datasets â or inferring and validating that.</li>
  <li>BUT: by presenting the underlying metadata such as Data Resource, Data Package front and centre and hiding the common operations (e.g. open this file) they make a rather unnatural interface for data wranglers.</li>
  <li>Most data wranglers start from an immediate need: display this csv on the command line, convert this excel file to csv etc. At the simplest, most data wrangling tools need some function like <code>open(file) =&gt; file-like object</code> where the file-like object can be be used for other tasks</li>
</ol>

<blockquote>
  <p>Metaphorically: the current data package libraries put the skeleton (the metadata) âon the outsideâ and the âfleshâ (the actual methods wranglers want to use) on the âinsideâ (they are implicit or hidden with the overall library)</p>
</blockquote>

<p>What follows from this insight is that we should invert this:</p>

<ul>
  <li>âPut the flesh on the outsideâ: Create a simple interface that addresses the common needs of data wranglers and data wrangler tooling e.g. <code>open(file)</code></li>
  <li>Put the bones on the inside: leverage the Frictionless Data metadata structures but put them on the inside, out of sight but still available if needed.</li>
</ul>

<p>Note: it may be appropriate to continue to have a dedicated Data Package or Table Schema library but keep it really simple</p>

<p>Hereâs how I put this in the original issue github.com/frictionlessdata/tableschema-js/issues/78:</p>

<blockquote>
  <p>People donât care about Data Packages / Resources, they care about opening a data file and doing something with it</p>

  <blockquote>
    <p>Data Packages / Resources come up because they are a nicely agreed metadata structure for all the stuff that comes up in the background when you do that.</p>
  </blockquote>

  <p>Put crudely: Most people are doing stuff with a file (or dataset), and they want to grab it and read it preferably in a structured way e.g. as a row iterator â sometimes inferring or specifying stuff a long the way e.g. encoding, formatting, field types.</p>

  <p>=&gt; Our job is to help users to open that file (or dataset) and stream it as quickly as possible.</p>
</blockquote>

Recommendations for Frictionless Data community

<p>Suggestions:</p>

<ul>
  <li>Users want to do stuff with data fast. This implies that a library like tabulator is more immediately appropriate to end users than data-package or table-schema</li>
  <li>The current set of FD libraries is bewildering and confusing, especially for new users. There are several complementary libraries plus some of the API is pretty confusing (see appendix for more on this)</li>
</ul>

<p>Recommendations:</p>

<ul>
  <li>Have a primary âgatewayâ library oriented around reading and writing data and datasets.</li>
  <li>This can be based around a simplified Package and Resource interface and library
    <ul>
      <li>Move auxiliary functionality to libraries e.g. infer,</li>
      <li>Make parsers / loaders (and writers) to a plugin model so this can be extended easily</li>
    </ul>
  </li>
  <li>Consider renaming Package and Resource to Dataset and File in the simple library as these are more accessible and common terms</li>
</ul>

Why do it?

<ul>
  <li>Massively grow the potential audience: Create an interface non-DP fanatics can use and want to use (and DP ones too)</li>
  <li>Ease of use: easier for us and others to use</li>
  <li>Elegance: do it right - this is the elegant, functional, beautiful way to do this library</li>
</ul>

Relation to Data Packages

<ul>
  <li>We use Data Package and Table Schema as the metadata model for data files and datasets</li>
  <li>Data Package libraries already implement APIs a bit like this and support many features we want (e.g. infer)</li>
</ul>

  
</div>

<div class="item-footer">
   Published
   00:00 â¢
   over a year ago

</div>

</div>

</article>



## Welcome to GitHub Pages

You can use the [editor on GitHub](https://github.com/MarioBarbarino/MarioBarbarino.github.io/edit/master/index.md) to maintain and preview the content for your website in Markdown files.

Whenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.

### Markdown

Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/MarioBarbarino/MarioBarbarino.github.io/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://docs.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we’ll help you sort it out.
