<div id="navwrap">

<div id="nav">

Last Update

Friday, 02\. October 2020 15:01

Subscriptions

*   [![](i/feed-icon-10x10.png)](https://www.nazioneindiana.com/feed/) [Nazione Indiana](https://www.nazioneindiana.com)
*   [![](i/feed-icon-10x10.png)](http://okfnlabs.org/blog/feed.xml) [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)
*   [![](i/feed-icon-10x10.png)](http://blog.schema.org/feeds/posts/default) [schema.org News](http://blog.schema.org)
*   [![](i/feed-icon-10x10.png)](http://blog.wikimedia.org/c/technology/wikidata/feed/) [Wikidata News](http://blog.wikimedia.org/c/technology/wikidata)

Planetarium

*   [Planet Ruby](http://plutolive.herokuapp.com)
*   [Planet vienna.rb](http://viennarb.herokuapp.com)

Meta

Powered by [Pluto](https://github.com/feedreader); Questions? Comments? Send them along to the [forum/mailing list](https://groups.google.com/group/wwwmake). Thanks!

</div>

</div>

<div id="opts">

<div style="width: 100%; text-align: right;">![](i/view-headlines.png "Show Headlines Only") ![](i/view-snippets.png "Show Snippets") ![](i/view-standard.png "Show Full Text")</div>

<div style="width: 100%; text-align: right;">Style | [Standard](planet.html) • [Cards](planet.cards.html)</div>

</div>

# Il Rivistaio

## Friday, 02\. October 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [Invisibilità e autorialità a proposito di Elena Ferrante](https://www.nazioneindiana.com/2020/10/02/invisibilita-e-autorialita-a-proposito-di-elena-ferrante/)

<div class="item-body">

<div class="item-snippet">di Viviana Scarinci   Il 29 agosto scorso in occasione dell’uscita de La vita bugiarda degli adulti in trentacinque Paesi, Elena Ferrante su Robinson rilascia una delle più lunghe e singolari interviste di sempre. Ferrante risponde alle domande provenienti da alcuni Paesi coinvolti dall’uscita del libro.… Leggi il resto »</div>

<div class="item-content item-summary">

di Viviana Scarinci

Il 29 agosto scorso in occasione dell’uscita de La vita bugiarda degli adulti in trentacinque Paesi, Elena Ferrante su Robinson rilascia una delle più lunghe e singolari interviste di sempre. Ferrante risponde alle domande provenienti da alcuni Paesi coinvolti dall’uscita del libro. Dal Brasile alla Danimarca, da Shangai al Portogallo passando per Formia, traduttori, editor e librai hanno posto all’autrice domande che entrano nello specifico non solo della sua opera ma anche di alcuni aspetti inerenti all’autorialità come fondamentale presupposto del suo profilo pubblico.

Una delle risposte riguarda per così dire la cassetta degli attrezzi di chi scrive e il tipo di lavoro di cesello che l’autrice o l’autore compie sul testo. È il linguaggio, cioè qualcosa che va oltre la sintassi, connettendosi imprevedibilmente al tema trattato, ciò che definisce la resa effettiva di un testo letterario. Perché riguarda l’esclusività della relazione tra chi scrive e chi legge (e spesso anche chi traduce e edita) ma anche l’alterità di un contenuto e l’imponderabile originalità che questo può esprimere rispetto ai parametri letterari in vigore.  
“L’editing più pericoloso” scrive Ferrante in proposito: “è quello che vigila sul rispetto del canone vigente”. A ben guardare non è un’affermazione di poco conto se posta in relazione alla filiera editoriale che può condurre in libreria certi libri e altri no. O stravolgere l’alterità e l’originalità dei contenuti di libri diciamo “fuori formato” se vengono valutati da un punto di vista che si riferisce strettamente a un canone letterario in vigore in un Paese piuttosto che in un altro. Oppure non visti affatto quando l’ottica con la quale è valutato un testo si lega a prese di posizione culturali inattuali, marcatamente connotate o semplicemente disinformate.  
Domani 3 ottobre nell’ambito della manifestazione FEMINISM 3 fiera dell’editoria delle donne presso la Casa Internazionale delle Donne di Roma la Società Italiana delle Letterate dedica il suo incontro nazionale del 2020 a una domanda che affonda il dito nella piaga: “Invisibili?”. Il punto di partenza è il significato simbolico dell’invisibilità di Elena Ferrante legato agli aspetti precipui dell’autorialità femminile. E soprattutto le condizioni di minore visibilità, o non visibilità, di questi aspetti messi a confronto con la visibilità globale che ha raggiunto l’opera di Elena Ferrante essendone espressione.  
L’incontro continua sotto l’egida dello stesso interrogativo in merito alla visibilità: la Società Italiana delle Letterate punta da sempre l’attenzione sul canone letterario e la necessità, oltre che il significato sociale, di un’analisi critica puntuale e dedicata, posta in essere con strumenti finalmente adeguati all’emersione e all’assimilazione culturale dell’opera, più o meno, ignorata dalla critica ufficiale delle moltissime autrici (romanziere, poete, saggiste e traduttrici) del Novecento, escluse dal canone, e di quelle contemporanee non adeguatamente, o per nulla, affrontate da un punto di vista veramente libero da pregiudizi di genere.  
L’incontro si concluderà tematicamente convergendo sul tema della necessità della parità di genere a scuola a partire da una manualistica formativa e didattica finalmente avvertita non solo dei risultati di un lungo percorso di studi e una vasta rosa di pubblicazioni dedicate in Italia alla scrittura delle donne ma anche dei risultati degli studi accademici relativi alle politiche di genere inquadrati in un contesto nazionale e internazionale.  
Secondo quanto pubblicato dal sito di Rivista Studio l’8 settembre scorso ossia a una settimana dall’uscita dell’ultimo libro di Ferrante all’estero: “A giudicare dall’entusiasmo incontenibile degli articoli usciti negli ultimi giorni, sembra che il nuovo libro di Elena Ferrante, La vita bugiarda degli adulti, abbia ricevuto un’accoglienza molto più calorosa negli Usa e in Uk che in Italia, dove è sì immediatamente balzato al numero uno (finora ha venduto circa 250.000 copie, è stato in classifica per trentadue settimane) ma non ha ricevuto una tale quantità di accurate analisi critiche. Se le recensioni italiane tendono a riflettere sul fenomeno Ferrante in generale (non siamo abituati a un caso editoriale di questa portata: continua a stupirci), quelle inglesi e americane si concentrano maggiormente sul libro in sé e sul materiale puro della scrittura”.  
La questione della ricezione dell’opera di Elena Ferrante presso i suoi connazionali mette in luce alcuni problemi rispetto alla gerarchia di valori applicata da alcuni recensori all’analisi dei libri scritti da donne e nella loro valutazione giornalistica oltre che critica. L’evidenza di questa questione non è affatto materia nuova di cimento accademico, oltre che di curiosità, per chi guarda al panorama letterario italiano dal di fuori.  
Un recente esempio ci viene fornito dal saggio di Cecilia Schwartz docente dell’Università di Stoccolma che sul numero 40 della rivista online The italianist pubblicata il 5 maggio del 2020 dedica un intero studio alla ricezione dell’opera di Elena Ferrante, nell’esclusivo ambito della stampa italiana prima e dopo la consacrazione internazionale dell’opera dell’autrice.  
Nell’abstract Schwartz dichiara: “Lo scopo dello studio è duplice: innanzitutto prende in esame come le opere di Ferrante siano state valutate nella stampa italiana e, dall’altra, si propone di indagare se la ricezione italiana sia stata influenzata dalla fortuna internazionale, soprattutto quella statunitense”.  
Oltre alle recensioni sulla versione cartacea e digitale dei principali quotidiani e inserti culturali, l’autrice del saggio prende in esame gli interventi su Elena Ferrante pubblicati dai principali blog di letteratura tra cui Critica letteraria, Doppiozero e Nazione Indiana in un periodo compreso tra il 2011 e il 2014 ossia il lasso di tempo interessato dalla pubblicazione della tetralogia de L’amica geniale in Italia ma anche dal 2012 in poi dalla pubblicazione con la traduzione di Ann Goldstein della stessa negli USA. “L’analisi rivela” continua Schwartz “che la ricezione delle prime due parti della tetralogia di Ferrante ha somiglianze con gli schemi paternalistici rilevati da Williams, mentre le recensioni pubblicate dopo il successo internazionale sono più elogiative. Questi risultati dell’analisi sono anche discussi in rapporto ai processi di consacrazione e canonizzazione letteraria mettendo in risalto il discorso su Ferrante in alcuni volumi recenti sulla letteratura italiana contemporanea”.  
Per inciso: a proposito di autorialità e punto di vista femminile, su Nazione Indiana ricordiamo tra gli altri l’importante intervento di Tiziana de Rogatis dell’ottobre del 2016 (ndr: presente insieme a Viviana Scarinci e Isabella Pinto all’incontro su Elena Ferrante del 3 ottobre organizzato da SIL, Leggendaria e Letterate Magazine).  
Tra gli altri elementi di estremo interesse che il saggio di Schwartz mette in luce, non ci sono solo quelli relativi alla deficitaria comprensione dell’importanza dell’opera di Ferrante in Italia, in quanto Schwartz illustra anche gli orientamenti di quella critica letteraria italiana che si è dimostrata fin dal principio, pubblicando articoli non accademici diffusi attraverso quotidiani e blog, bene attrezzata per la lettura di un’opera indubbiamente così complessa come quella dell’autrice di cui stiamo parlando. Complessità riferita anche al modo in cui le donne ritratte nell’opera di Ferrante si inseriscono nella storia della società del loro Paese a partire dal secondo dopoguerra. E ancora di più alla genealogia del romanzo italiano scritto da donne, campo che di fatto è illustrato da un discreto numero di studi e pubblicazioni italiane la cui visibilità o invisibilità, spesso dipende dallo stesso motivo della mancata o falsata ricezione dell’opera letteraria di Elena Ferrante presso i media italiani.  
Nell’arco di venticinque anni infatti la Società Italiana delle Letterate ha elaborato e prodotto, attraverso convegni, seminari, focus delle riviste un ricco bagaglio di studi e pubblicazioni che sono state fondamentali per l’elaborazione di quella genealogia delle produzioni artistiche femminili a cui la stessa Elena Ferrante fa riferimento ne L’invenzione occasionale quando scrive nel capitolo “Libertà creativa”: “la posta in gioco non è più essere cooptate all’interno della lunga, autorevole tradizione estetica creativa degli uomini. La posta in gioco è più alta: contribuire a rafforzare una nostra genealogia artistica” (p.82). Un esempio tangibile di questo contributo dato dalla Società Italiana delle Letterate nel tempo: la collana Workshop di Iacobelli in cui testi come Oltrecanone, Dell’ambivalenza, L’invenzione delle Personagge sono risultati fondamentali per lo studio di molte e molti che in Italia hanno approcciato l’opera di Elena Ferrante prima della sua consacrazione internazionale.  
Magra consolazione ma va detto: non è un problema che riguarda solo l’Italia quello dell’inadeguata e lacunosa “misurazione” di valore offerta dal canone letterario ufficiale, sulla falsa riga del quale muove la formazione di alcuni esponenti della stampa di un Paese. Ciò che a volte viene scritto, o ignorato, sulle pagine culturali dei quotidiani e su alcuni blog letterari, quando si riferisce a scritture “fuori formato”, come quelle che sono espressione dell’autorialità femminile, le relega ancora di più all’invisibilità.  
In un passaggio del ricco saggio di Schwartz leggiamo: “L’ispirazione metodologica per questa indagine è principalmente tratta dallo studio diacronico di Anna Williams sulla descrizione e la valutazione delle scrittrici nei libri di storia della letteratura svedese pubblicati nel ventesimo secolo. Sebbene lo studio di Williams copra quasi un intero secolo, mostra che l’approccio alle scrittrici nei libri di storia della letteratura è sorprendentemente statico per tutto questo periodo: tendono ad essere associate a generi di basso rango, non sono paragonate ad altri scrittori e, cosa cruciale, non sono collocate in una tradizione letteraria. In quasi tutte le storie della letteratura svedese, le poche scrittrici che effettivamente compaiono sono “stelle senza costellazioni”, una metafora usata nel titolo svedese dello studio di Williams”. Cosa aggiungere. Mal comune mezzo gaudio?

</div>

<div class="item-footer">Published 04:00 • 11 hours ago</div>

</div>

</article>

## Thursday, 01\. October 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [Il piccolo (grande) teatro filosofico di Aldo Masullo](https://www.nazioneindiana.com/2020/10/01/il-piccolo-grande-teatro-filosofico-di-aldo-masullo/)

<div class="item-body">

<div class="item-snippet">PROLOGO di Lucio Saviani Oggi si parla molto di dialogo, in ogni dove. Ne parlano in tanti. E anche troppo. Soprattutto i politici, quelli della burattinata politica televisiva, fatta di smorfie in camera, di voce grossa e di par condicio. Parlo io, poi parli tu, stessi minuti, stessi secondi.… Leggi il resto »</div>

<div class="item-content item-summary">

♦

PROLOGO

di Lucio Saviani

Oggi si parla molto di dialogo, in ogni dove. Ne parlano in tanti. E anche troppo. Soprattutto i politici, quelli della burattinata politica televisiva, fatta di smorfie in camera, di voce grossa e di par condicio. Parlo io, poi parli tu, stessi minuti, stessi secondi. Ma quello, si sa, non è dialogo, è un parlare da soli a turno, senza nemmeno ascoltarsi.

Aldo Masullo ci ha insegnato che dialogo, prima ancora che parlare in due, è pensare in due, pensare insieme.

Quel pensare insieme che è come un viaggio verso qualcosa che da soli non si conosce e che da soli non si raggiunge.

Il dialogo è fatto naturalmente di parola; ma, prima ancora, di ascolto. E dunque di silenzio. Ma per dialogare non basta stare in silenzio: c’è bisogno di attenzione, apertura, e cioè ospitalità, accoglienza di un’altra visone delle cose. C’è bisogno della forza misteriosa – come per lo schiavo della caverna platonica – di chi si scioglie, di chi abbassa le difese e apre la porta di casa. Necessaria è una disposizione all’apertura, all’ospitalità, all’accoglienza dell’altro. Ma soprattutto significa disporsi ad accettare l’eventualità di cambiare, non solo idea, ma di sentirsi diverso, anche diventare un altro. Proprio per questo è così difficile fare davvero esperienza del dialogo, e invece così facile ritrovarselo così, come in maschera.

Dialogare insomma è la forza di mettersi in gioco, di accettare la possibilità di ritrovarsi diverso da prima che ci si aprisse al dialogo. In cerca di quella verità che o è un bene comune o non è. Per questo, Aldo Masullo ci ha insegnato che la filosofia nasce con la democrazia, due facce di una stessa medaglia: la filosofia con la sua vocazione agoretica, vocazione alla piazza, all’agorà, alla piazza degli scambi di merci e di opinioni.

Il dialogo perciò è esperienza. Masullo ricordava sempre la parola di Hegel: la vera esperienza è quella che modifica colui che la fa. Dopo un’esperienza non si è più gli stessi di prima. E così anche con il dialogo. E con il viaggio: dopo un vero viaggio non si torna mai uguali a come si era alla partenza.

Il Patico, termine e concetto così centrali nel cammino di pensiero di Masullo, hanno origine proprio in questo: esperire, esperienza vissuta, un passare, un attraversare, passare una prova, un provare, quel vissuto che non appartiene al piano della comunicazione dei concetti e dei significati.

Nel libro Paticità e indifferenza Masullo si chiede quale può essere ancora il ruolo della filosofia. La filosofia, risponde, è «saper assaporare i sapori della vita, gustare a fondo i sensi vissuti, …è la “sapienza del patico” ovvero, se si ricalca interamente l’etimo greco, è la “patosofia”».

Nei suoi lavori ricorre spesso il verbo greco páskein, che significa sì ‘‘vivere”, ma indica il ‘‘vivere” in senso transitivo. Indica cioè la vita come capacità di provare, avvertire, vivere l’esperienza: «Paticità è vivere provando, vivere assaporando».

È il senso, diceva Masullo.

E lo diceva con il senso che ha il dialogo: che è soprattutto attenzione, ascolto, apertura, (curiositas, come dicevano gli antichi), cura di sé, degli altri e anche delle parole: questo lo comprendono bene tutti quelli che ricordano, di Aldo Masullo, la costruzione, la sempre felice ricerca della giusta parola, l’esposizione del pensiero durante le sue lezioni, i suoi discorsi, le conferenze. E anche i suoi dialoghi.

Cura delle parole come cura di sé e degli altri. Attenzione e ascolto: per te, ma anche per le persone a te care, che per lui era come la stessa cosa; mi chiedeva di Luna, parlando del suo amore per i cani, e non ricordo una volta, nemmeno una, che non mi abbia chiesto di Ruzenka e lasciato i saluti per lei, fino a quell’ultima telefonata che ho fatto a lui per il suo compleanno, che era anche il giorno di Pasqua.

La filosofia, ci ricordava Masullo, è l’esercizio che ogni uomo è chiamato a fare dalla sua umanità per comprendere meglio non solo se stesso, ma il rapporto tra sé e il mondo, tra sé e gli altri. Insomma: “Conosci te stesso”. Ma Masullo ebbe modo di chiarire una interpretazione meno nota del motto di Delfi. E cioè: conosci te stesso perché solo così puoi sapere quale è la domanda più giusta da fare al dio Apollo. Lo scrisse ne La libertà e le occasioni. Lo chiamai subito dopo aver letto quel capitolo, ne parlammo a lungo. E io lo chiamavo proprio da Delfi, dove mi ero portato il suo libro. Ma questo glielo dissi solo alla fine.

Aldo Masullo ci lascia risposte che sono poi, come il lascito socratico, quelle che, contro la morte, danno vita: e cioè danno vita a tante domande.

È proprio in questo senso che possiamo dire che esistere è un essere in dialogo.

Un dialogo che continua anche chi non è più in vita: Aldo Masullo di risposte ne ha date tante, fino a quelle date ai giornali che gli chiedevano una riflessione sulla pandemia; ha dato tante risposte con le sue opere, con la sua opera: la sua vita, la sua vitalità, la sua esistenza.

Una lezione che è rimasta sempre inquietudine teoretica, pratica di libertà e di filosofia come libertà.

Ricordo i pomeriggi trascorsi insieme nel suo studio, verso la metà degli anni ’90, perché curavo la bibliografia da aggiornare dopo quasi vent’anni per la nuova edizione del suo fondamentale libro Metafisica, che avevo letto da studente e che sarebbe diventato un libro di riferimento per i miei studenti per molti anni. Masullo ha sempre seguito con passione i destini della scuola italiana: ancora mi ricordo la sua espressione di sconcerto, di divertita amarezza, quando qualche tempo fa ebbi a dirgli che oggi spesso nei documenti ufficiali i docenti sono chiamati “fornitori di docenza” e gli studenti sono chiamati “utenti”.

Aldo Masullo è stato per decenni un maestro e una guida per diverse generazioni di studenti, professori, filosofi e un esempio prezioso della vita civile, della politica più nobile e della cultura del nostro Paese.

Ne sono ricca e sempre presente testimonianza i suoi lavori degli anni sessanta e settanta sulla “intersoggettività” e sul “fondamento” e gli studi sul “tempo”, sul “senso” e sulla “paticità” degli anni ottanta e novanta.

Agli inizi degli anni novanta, Masullo aveva rappresentato un momento di grande speranza, di forza di rinnovamento, anzi di discontinuità nel panorama della politica nazionale italiana. Lo ricordiamo tutti protagonista delle “Assise di Palazzo Marigliano” e poi della inedita esperienza della “giunta del sindaco”, primo caso in assoluto in Italia. Poco tempo fa ebbe a dirmi che Napoli ama mettersi in maschera… E allora io mi ricordai di quella volta a Parigi. Alla fine di quel decennio di speranze e prime delusioni organizzai a Parigi, presso l’Istituto Italiano di Cultura, un convegno su Napoli dal titolo “Poros”. Al convegno di Parigi invitai filosofi, scrittori e artisti partenopei. A chiudere i lavori del convegno, invitai naturalmente Aldo Masullo.

Lui pronunciò un discorso di grande sensibilità politica e di autentica speranza per la città, per la cultura e per il futuro di Napoli. Ma soprattutto lasciò stupito il pubblico italiano e parigino iniziando così:

“E per dare a voi tutti, per lo meno ai non napoletani, un’immagine direi pre-filosofica dell’essere napoletano, dell’esistenza napoletana, vorrei evocare una figura che in genere nelle accademie filosofiche non trova posto: la straordinaria maschera di Pulcinella. (…)

“Si è fatto scuro, Lucio, ci ritiriamo?”, diceva spesso così, alla fine delle passeggiate che facevamo tra Pantheon, S. Eustachio e Piazza Navona negli anni del suo ultimo mandato da parlamentare, parlando di quella sua esperienza, tra vecchia passione e giovani delusioni. E io, a sentirlo parlare di crepuscolo mi ricordavo allora la salita delle rampe che mi portavano, da studente, al Cortile del Salvatore. Primi anni ’80, dopo il terremoto, in una Napoli che si avviava con un disastro a vivere un decennio disastroso. In quel cortile, nella penombra di una grande aula, cominciava di mattina presto la lezione di Masullo. Lui diceva che la filosofia vive sempre nella penombra. Io in quella penombra mattutina ho imparato ad amare la filosofia. A me ora piace pensare che quelle lezioni di Masullo siano continuate e arrivate fino a questa bella piazza e al nostro dialogo di questa sera.

♦

“Il Parco in Maschera” – Rassegna a cura del Parco Archeologico dei Campi Flegrei

CASTEL DI BAIA

Martedì 4 agosto 2020, h. 19.00

PICCOLO TEATRO FILOSOFICO

In memoria di Aldo Masullo

Un prologo e un dialogo di e con

PASQUALE PANELLA e LUCIO SAVIANI

</div>

<div class="item-footer">Published 05:00 • a day ago</div>

</div>

</article>

## Wednesday, 30\. September 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [Del fraintendimento: Gruppo 93 e avanguardie](https://www.nazioneindiana.com/2020/09/30/del-fraintendimento-gruppo-93-e-avanguardie/)

<div class="item-body">

<div class="item-snippet">di Bianca Coluccio Nel maggio del 2003 l’Alma Mater di Bologna predisponeva una serie di tavole rotonde organizzate nell’ambito dell’iniziativa “Gruppo 63 – Quarant’anni dopo”. Sul magazine dell’Università si legge che l’intento principale è quello di “dimostrare che i lavori del Gruppo 63 avevano una potenzialità di sviluppo ulteriore”.… Leggi il resto »</div>

<div class="item-content item-summary">

♦

di Bianca Coluccio

Nel maggio del 2003 l’Alma Mater di Bologna predisponeva una serie di tavole rotonde organizzate nell’ambito dell’iniziativa “Gruppo 63 – Quarant’anni dopo”. Sul magazine dell’Università si legge che l’intento principale è quello di “dimostrare che i lavori del Gruppo 63 avevano una potenzialità di sviluppo ulteriore”.[1] Che l’avessero è indubbio, ma che i lavori del Gruppo 93 potessero ritenersi uno sviluppo della neoavanguardia aveva bisogno di una puntualizzazione precisa, che sciogliesse l’equivoco che era andato tessendosi. Già Biagio Cepollaro, nell’intervento “La compresenza conflittuale”, uscito su Baldus nel 1991,[2] metteva in chiaro come ritenere il Gruppo 93 un’avanguardia, alla stregua del Gruppo ’63, non fosse che uno dei quattro equivoci che toccavano il gruppo.

Nello stesso senso si snoda l’intervento di Lello Voce a Bologna. In questa occasione Voce introduce una nuova forma di avanguardia, che non è più “quella storica, quella neo, quella neo-neo. […] esiste poi un tipo di avanguardia particolare, che io purtroppo conosco molto bene, che è l’avanguardia mio malgrado”.[3] L’equivoco quindi ritorna, anche a distanza di anni, e continua ad avere bisogno di chiarificazioni precise. Prima delle teorizzazioni fatte sopra i lavori del Gruppo, ci sono gli stessi autori che nella consapevolezza delle proprie posizioni non hanno mai voluto o creduto di essere un movimento d’avanguardia.

Lo stesso argomento aprirà, nel 2016, la conversazione svolta a Milano tra Angelo Petrella e alcuni poeti del Gruppo 93.[4] Si richiama ancora in gioco la questione dell’avanguardia: non solo Petrella si domanda se questa esperienza non possa venire considerata come “l’estremo canto del cigno della possibilità di fare avanguardia”, ma se questa considerazione non sia più che altro necessaria a inquadrare un fenomeno storico ponendolo a un estremo di due polarità.

In ognuna di queste occasioni è stato ribadito che i rapporti in termini di scopi ultimi e modalità del loro raggiungimento non sono sovrapponibili rispetto a quello dell’avanguardia, non se ne traggono le mosse. Posto che l’avanguardia sia impossibile a reiterarsi, ciò a cui si tende è più che altro un dialogo, poiché le condizioni che hanno favorito l’avanguardia, “storica, neo, neo-neo”,[5] sono ormai esaurite e irreplicabili.

Come si spiega, dunque, questo fraintendimento? E fino a che punto è normale che si verifichi l’equivoco? Ritenere avanguardia il Gruppo 93 significherebbe allo stesso tempo dover riconsiderare alcuni dei caratteri imprescindibili che cooperano alla formazione di uno statuto d’avanguardia: una condizione storico-politica favorevole, una opposizione antagonista rispetto alla tradizione tout court, un unico scopo ultimo perseguito secondo le medesime modalità.

Riguardo l’impossibilità di riprodurre le condizioni teoriche e storiche tipiche dell’avanguardia, Luperini prospetta l’esistenza di tre cerchi di azione concentrici.[6] C’è un primo cerchio nutrito da un certo numero di autori non armonizzati che compongono una resistenza sperimentale; di qui il secondo in cui tutti coloro che hanno dimostrato interesse nei confronti del Gruppo 93 hanno anche desiderato che il gruppo si tramutasse in una forma d’avanguardia, per conservare delle avanguardie il carattere oppositivo e agonistico. Un terzo cerchio, infine, è quello in cui lo stesso Luperini si inserisce. In quest’ultimo si riconosce l’impossibilità per un’avanguardia di ricostituirsi e si agisce sugli spazi liminari ed estremi ancora concessi dalla postmodernità. In questo senso si potrebbe allora configurare una possibile risposta alla dinamica postmoderna. Spostare la propria attenzione nelle zone periferiche, operare nel segno della lateralità per rispondere alle propensioni di parificazione che connotano il postmoderno.

La postura assunta dal Gruppo 93 è evidentemente discosta rispetto a quella tipica delle avanguardie. L’antagonismo, il nichilismo, il rivoluzionarismo e il terrorismo, l’autopropaganda violenta e autopubblicitaria e la prevalenza della poetica sull’opera: queste sono alcune delle caratteristiche che Renato Poggioli ritiene essere peculiari di ogni movimento d’avanguardia.[7]

Quale avanguardia, perciò, senza antagonismo? Esiste un’avanguardia che non abbia un terminus contra quem, che non mantenga un atteggiamento oppositivo, che non prenda le proprie difese? Già in questo senso, il Gruppo 93 non può essere considerato un’avanguardia. Il tempo delle opposizioni binarie e delle polarità contrastanti è del tutto terminato. Il contesto in cui avviene la produzione culturale è adesso votato alla velocità, della stessa velocità di cui risente il linguaggio, una piega dell’estetizzazione che deve tutto alla comunicazione massmediatica contemporanea: ritenere possibile una difesa ferma e a spada tratta come quella a cui ha abituato l’avanguardia non è neppure più auspicabile.  
La modernità dilagante e fluida sancisce una saturazione impossibile da far retrocedere. È la saturazione dell’età moderna ad aver segnato il termine dell’avanguardia. Quello che hanno rappresentato le grandi avanguardie novecentesche non può più venire adoperato come paradigma. Ritenere possibile nella società postmoderna un qualsiasi movimento avanguardistico significherebbe tentare di sostenere un “velleitarismo patetico”.[8]

Tuttavia, quella che si potrebbe considerare alla stregua di un’abiura va ripensata nei termini di una presa d’atto: l’esperienza avanguardistica, storicizzata e conclusa, non può fare a meno di inserire sé stessa nella tradizione. Rispetto all’avanguardia e al suo periodo, quella dialettica contraddittoria che rimaneva alla base del rapporto arte-museo è diventata sterile e impossibile a realizzarsi. L’impasse neoavanguardistico si verificava laddove, pur ricercando un prodotto artistico che fosse incontaminato e atemporale, lo scontro con la solita logica borghese e di mercato era inevitabile: il prodotto artistico era un prodotto, appunto, che per carica innovativa, distruttiva, audacia, era in grado di superare gli altri e di batterli sul piano della concorrenza. L’avanguardia desiderava produrre un’arte imbattibile e imbattuta e che allo stesso tempo non rimanesse impigliata nelle dinamiche di un certo stringente algoritmo.

Diversamente, invece, il Gruppo 93 né mira a produzioni incontaminate e atemporali, né intrattiene questo rapporto contraddittorio col mercato. Anzi, alla nascita del Gruppo, il termine dell’esperienza è già stato deciso ed è il nome stesso che il Gruppo si dà, a indicare quando arriverà lo scioglimento.  
Ecco quindi che non si può parlare di avanguardia, almeno poiché non c’è alcun “assestamento dell’arte all’epoca della tecnica”.[9] Per il Gruppo 93, considerate le premesse “temporali”, la possibilità di assestamento non è contemplata. Più che l’epoca della tecnica, si fanno i conti con l’epoca della comunicazione “televisiva” e massmediatica, nutrita di una velocità che non può non interessare anche la dimensione estetica.

In quest’ottica si capisce meglio come la tradizione rigettata dalle avanguardie sempre a fronte di uno sperimentalismo estenuato non soltanto viene riconsiderata ma assume la postura di chi la fa propria, di chi la adopera ai fini della propria arte. Questo spiega come nel Gruppo 93 il dialogo con la tradizione avvenga in un confronto di voci diverse e in qualche maniera rifunzionalizzate. Mentre Lello Voce annovera nel proprio canone Zanzotto, Leonetti, o “addirittura un certo Fortini ‘politico’” ,[10] per Biagio Cepollaro è la lingua del Duecento a costituire il perno e il riferimento principale.

Si legge nell’intervista di Enzo Rega a Biagio Cepollaro:

Jacopone da Todi e il Dante più infernale sono i miei veri maestri. Da loro ho capito come una parola, ogni singola parola può essere a tal punto “riscaldata” da diventare incandescente. Ho capito “l’eccessivo” che si annida nelle consonanti, la materialità della parola, la sua capacità di attrito e di resistenza alla banalità del “poetese” e dello standard. Da maestri del genere si capisce come la poesia medioevale, letta in un certo modo, si avvicini quasi all’ultima poesia sonora, come la cosiddetta “tradizione” sia in realtà – se grande – un serbatoio infinito di possibili innovazioni e ricerche.[11]

Il terreno comunque del Gruppo 93 è la tradizione non tutta e non canonica, ma ripensata e rifunzionalizzata alla luce di una propria soggettività. Posto questo come punto di partenza, le possibilità di declinazione sono molteplici e diversificate. Inoltre, la fluidità e la soggettività di cui si connotano i canoni, sono iscritte all’interno di un intento preciso, di una scrupolosa volontà di contaminazione. Senza questa idea di tradizione fin qui descritta, non ci sarebbe alcuna possibilità di contaminazione.

Come si è detto, il quadro attuale non permette più una separazione netta e oppositiva neanche tra lingua ordinaria e lingua poetica, tra lingua alta e bassa, altezza del linguaggio lirico e statuto periferico della parola dialettale. La contaminazione vorrebbe agire in questo senso come una ibridazione. Ciò che si vuole raggiungere è un momento infine creolo in cui allo scambio è seguita una successiva fusione, secondo un movimento che abbandona ogni pretesa di alterità e affermazione per giungere a un risultato polifonico, un’armonia mancata che parte dal suo apparente difetto per riorganizzarsi in un coro di voci e un coacervo di orecchi tesi. Contaminare ha quindi una “pronuncia plurale” e “produce una terza identità che non è equivalente a nessuna delle due che concorrono a formarla”.[12] Affinché un simile processo di contaminazione possa rendersi possibile, il rapporto con la tradizione deve necessariamente essere opposto a quello che si proponeva l’avanguardia. Mentre le avanguardie, infatti, hanno sempre operato un taglio orizzontale di separazione netta e definitiva col passato, quello che fa il Gruppo 93 è, sì, operare un taglio, ma stavolta verticale, aprire una fessura, provocare la fuoriuscita dei propri riferimenti, far collimare tra loro universi svariati e disomogenei.

Quando su Le Figaro apparve nel 1909 il Fondation et Manifeste du Futurisme di Marinetti, la posizione era frontale e inequivocabile. Nessuna bellezza per le opere che non possiedono un carattere aggressivo: ciò che non si configura come scontro e come sommossa allora non riguarda né più l’arte né la bellezza. Non serve a niente girarsi a guardare il passato per l’uomo che si trovi “sul promontorio estremo dei secoli”. Dichiarata la guerra ad accademie, biblioteche, musei: uno spreco di tempo rivolgersi indietro, giacché “il Tempo e lo Spazio morirono ieri” e per loro non c’è posto nell’“eterna velocità onnipresente”. Ma nessuna tra queste velleità incendiare apparterrà poi al Gruppo 93\. E ancora: se l’avanguardia storica è senza passato, la neoavanguardia non sa, nel presente, destreggiarsi tra la logica piccolo borghese alla quale vorrebbe sfuggire e che invece la inghiotte. Il Gruppo 93 un po’ per propria volontà e per contingenza storica, riesce a porsi al di là di ognuna di queste posizioni. È agli antipodi, infatti, rispetto al rigetto della tradizione a cui guardava l’avanguardia storica. Ha superato il vicolo cieco in cui borghesia e museo costringevano la neoavanguardia, per approdare su un nuovo terreno di comunicazione e dialogo. Terminate violenze e antagonismi avanguardistici, lo scopo a questo punto diventa sviluppare nuove possibilità di creazione di senso, rifunzionalizzare certi elementi assodati per arrivare a una letteratura creola e quindi inedita.

Dialetto, idioletto, pastiche

Una delle strategie tramite cui si determina la contaminazione è l’utilizzo del dialetto. Di nuovo, le motivazioni sottostanti la scelta di un linguaggio dialettale non sono di natura nostalgica, né viene caricato in alcun modo di valenza regressiva o ancora mitica. Nell’assenza di cariche oppositive e contrastanti, le polarità interne alla lingua si trovano scariche e inefficaci, comportando una “equivalenza neutralizzante peraltro ampiamente supportata dagli interessi economico-tecnologici che presiedono alla produzione artistica di sempre più numerose tipologie di prodotti”.[13] La scelta del dialetto non è dettata quindi da alcuna inclinazione purista e da nessun sentimento nostalgico, quanto dalla volontà di creare uno spazio letterario, poetico, che possa essere uno spazio di creazione di nuovi sensi. Ciò che prima si trovava ai due estremi di una divaricazione viene reso incontro proficuo, generatore di significati, terreno fertile. Non solo: la riduzione progressiva dei parlanti dialettofoni ha comportato l’evoluzione del poeta dialettale in poeta neodialettale. Vale a dire che il contenitore da cui si attinge per creare il proprio universo linguistico non è più reale, ma virtuale. In una situazione simile, la dimensione territoriale, caratteristica della poesia dialettale, è inevitabilmente spinta a riconsiderare i rapporti tra il centro e il confine della lingua.[14]

Niente da imputare, insomma, né a un atteggiamento elegiaco né a una torsione verso l’infanzia linguistica. La lingua si deve muovere verso una scrittura “anti-istituzionale, anticlassica, anti-simbolista, nemica dell’io lirico gonfio dei privilegi usurpati, la quale intenda stabilire reti di relazioni piuttosto che immedesimazioni, vuol dire viaggiare in una scrittura non garantita che ha bisogno, per poter vivere, di un atteggiamento continuamente autocritico da parte dell’autore”.[15]

Si è detto di come all’interno del Gruppo non vi fossero esattamente orizzonti comuni, quanto piuttosto spazi condivisi, e di quanto su questi spazi i vari esponenti si muovessero liberamente. Il diverso utilizzo del dialetto da parte di Lello Voce e Biagio Cepollaro si può iscrivere all’interno di questo libero movimento. Cepollaro, rifacendosi a Jameson, sostiene che il deperimento del pastiche, inteso come una delle strategie di contaminazione, sia da imputare all’interazione mancata cui vengono sottoposti i materiali. La giustapposizione degli elementi non realizza, secondo lui, alcun apporto di senso. Diversamente, quando la fusione è pressoché completa e a stento si riconoscono le parti che di cui si compone l’idioletto, allora lì si verifica un nuovo apporto di senso. Alla funzione parodica del pastiche si sostituisce una funzione “modellizzante”: il luogo della periferia e del confine si pongono come centro di tutta la strategia compositiva”.[16] Il pastiche idiolettico di Cepollaro conduce a un testo le cui ragioni sono nella compressione e nella presenza interattiva di linguaggi diversi.

Di contro, Voce, stabilisce la sua idea di pastiche a partire dalla volontà di creare un attrito nella poesia: gli elementi che la compongono, le citazioni che ne creano l’ossatura, non agiscono al di sotto del testo ma sono manifeste e identificabili. In questo contesto la citazione è un modo di contaminare e dunque un momento di riflessione. E non solo la contaminazione linguistica non può essere esente dalla contaminazione stilistica, ma contaminare gli stili significa minare la consueta distinzione tra i generi di cui si rende necessario un ripensamento. Diversamente a quanto ritiene Cepollaro, Voce prende le distanze dalla posizione di Jameson:

Il pastiche fonda la sua identità (la sua individualità) non sulla perdita di riconoscibilità (di individualità) di ogni suo singolo elemento, come se per una sorta di abbassamento di luminosità tutto si omogeneizzasse nella “sintesi” di una stessa, opaca, patina-tonalità […]. La stroncatura feroce che Jameson riserva al pastiche sembra frutto, quanto meno, di un travisamento sineddochico che condanna il tutto per la parte, che deplora la valenza polifonica per colpire, in realtà, i suoi usi, per così dire, retrogradi.[17]

Processi allegorici, produzione di significati

Il momento in cui si sviluppa la riflessione sull’allegoria coincide necessariamente con quella di riflessione sul cambiamento della condizione e dell’intellettuale e dello scrittore.[18] Gramscianamente, l’intellettuale non può attendersi di avere un gruppo sociale di riferimento, che sarà invece di volta in volta differente sulla scorta di quale gruppo si trovi al potere. Va da sé che anche la produzione in questo contesto diventa vittima di processi di standardizzazione e appiattimento, in un modo che rende quantomai manifesto il legame di reciproca dipendenza che lega l’intellettuale – e quindi lo scrittore – alla società. Dunque, la condizione intellettuale muta indifferibilmente al mutare della realtà che lo circonda. I processi di reificazione, in questo momento, non sono reversibili.

Questo ha determinato un esaurimento delle possibilità, per il simbolo, di rendersi produttore di significati. Posta una definizione di simbolo come rappresentazione di un valore sul supporto di un corpo transeunte, è nel simbolo che il poeta poteva vedere l’universale sopra il particolare. Nella realtà ridefinita dalle logiche moderne, il poeta deve invece cercare il particolare in relazione all’universale.

Cioè a dire che si è creata una opposizione tra l’atteggiamento del vedere e l’atteggiamento più attivo del cercare, oltre che un rovesciamento del processo metonimico. Il simbolo dimostra l’universale adoperando sé stesso. Cosa si sostituisce al simbolo? L’allegoria, che, al contrario, trae le proprie forze da una volontà di indagine e riflessione. Così il simbolo offre una verità data e iscritta nel corpo dell’oggetto, mentre l’allegoria accusa una distanza da colmare tra universale e particolare. Al mutare del simbolo in allegoria muta anche la narrazione, anzi scalzata dalla descrizione. L’impadronirsi di una pratica allegoria è allora da ricondurre all’interno di un programma di ricerca poetica. Abbandonati i lirismi e messe al bando le mere nostalgie, preso atto di un postmoderno che depotenzia ogni conflitto possibile in favore di una generale neutralizzazione, la letteratura diviene essa stessa allegoria, poiché costituita di una rete di scambi e interrelazioni a partire dalle quali si determinano sensi diversi. Per dire meglio, i significati di un testo letterario diventano sempre relativi agli scambi interni che per allegoria, per dialetto, idioletto o pastiche, producono un senso che rende attivissimo il testo.

In conclusione, un progetto di stampo avanguardistico, stanti tutte le premesse di cui si è discusso, è di fatto impossibile a verificarsi. I presupposti che costituivano le basi per la nascita e la durata di un’avanguardia sono scomparsi: non c’è più alcun agonismo da esercitare nei confronti d’una certa tradizione, poiché l’avanguardia stessa è entrata a pieno titolo a far parte di quella tradizione che ripudiava – l’avanguardia è diventata un’arte da museo. Quei conflitti che rendevano fervente il discorso intorno all’avanguardia e al suo interno sono stati livellati dall’avvento del postmoderno, da non intendersi più come ideologia ma come momento storico dato. Laddove le avanguardie hanno sempre tentato un rovesciamento della tradizione, il Gruppo 93 ha superato la dialettica che le riguardava: piuttosto che trascinare sfibrando un discorso che in verità non ha più possibilità di durare, si avanza seguendo un movimento che parte dalla periferia della lingua, dai margini. In questo modo “A un’opposizione dialettica interna al centro si oppone una conflittualità fondata sullo spostamento; all’antagonismo frontale delle avanguardie segue una letteratura della lateralità, giocata sullo scarto, che sottolinea […] uno sforzo di non appartenenza”.[19]

Se questi sono i presupposti che hanno governato il Gruppo 93, la possibilità che si trattasse di una nuova e ultima avanguardia può dirsi inconsistente. Ciascuna delle poetiche che nasce sulla base di tali premesse, trova poi, nel proprio autore, un suo modo unico di declinazione. Quello che si presenta come un gruppo, di fatto, non funziona esattamente come un gruppo: non fa fronte comune e non trova la propria forza in una coesione interna e viscerale. L’idea è più quella di un laboratorio o di una rete, di un organismo che funziona nella cooperazione, ovvero di una letteratura che funziona nella contaminazione.

A conclusione riporto alcune parole del Luperini di Un confronto tra posizioni diverse, significative e chiarificatrici rispetto allo scopo del discorso affrontato fin qui, contenute nell’antologia “Gruppo 93 la recente avventura”.

Ricordava ieri Sanguineti com’è nata la letteratura: non diceva come competenza opposta ad altre competenze, non come la competenza di chi lavora il ferro è opposta alla competenza di chi lavora la seta, è nata invece come competenza opposta all’incompetenza, come sapere e potere separati. Nel ritorno alle origini, e spesso non manca la O maiuscola, c’è il ritorno anche a questa origine. […] Se niente è letteratura non vuol dire che è finita la letteratura, ma che forse è possibile solo una letteratura di secondo grado, sonda il vuoto e il nulla che la circonda e di cui essa fa parte. […] La letteratura di secondo grado è un’allegoria della ricerca di senso.[20]

[1] magazine.unibo.it/calendario/2003/05/08/gruppo63?d=2003-05-08.

[2] Biagio Cepollaro, La compresenza conflittuale. Quattro equivoci sintomatici sulle vicende del Gruppo 93, in «Baldus», anno II, n. 1, agosto 1991.

[3] Lello Voce, Il postmoderno è nostro: giù le mani!, www.lellovoce.it/Il-Postmoderno-e-nostro-giu-le. Il testo è la trascrizione di Voce del proprio intervento al convegno di Bologna. Gli atti del convegno sono contenuti in AA.VV., Il Gruppo 63 quarant’anni dopo, a c. di Renato Barilli, Fausto Curi, Patrizia Cuzzani e Niva Lorenzini, Bologna, Pendragon, 2005.

[4] La conversazione tra Angelo Petrella e alcuni poeti del Gruppo 93 svoltasi nell’ambito di Tu se sai dire dillo (2016) è reperibile al link www.youtube.com/watch?v=VBQsVi36reo&t=2925s.

[5] Lello Voce, il postmoderno è nostro: giù le mani!, cit.

[6] Romanzo Luperini, Un confronto tra posizioni diverse, in «Alfabeta», n. 69, 1985.

[7] Renato Poggioli, Teoria dell’arte d’avanguardia, Bologna, il Mulino, 1962\. Quello citato non è l’elenco completo che Poggioli stila. Di fatto, poiché alcune tra le caratteristiche individuate potrebbero essere opinabili (si parla ad esempio di gratuità del fine, che tuttavia non è compatibile con l’avanguardismo russo), solo quelle insindacabili sono state citate in questo testo.

[8] Remo Ceserani, Trent’anni dopo, una convitata di pietra di nome Avanguardia, in il manifesto, 1993\. Il documento è disponibile online all’indirizzo:

www.cepollaro.it/rastam2.htm#Remo%20Cesarani,%20Trentanni%20dopo,%20una%20convitata%20di%20pietra%20di%20nome%20Avanguardia,%20Il.

[9] Franco Fortini, Verifica dei poteri: scritti di critica e di istituzioni letterarie, vol. 354, Torino, Einaudi, 1989.

[10] Lello Voce, Il postmoderno è nostro…, cit.

[11] Enzo Rega, dall’intervista a Biagio Cepollaro Oltre il postmodernismo: la parola come esperienza del caos, in «Quaderni Radicali», anno XVI, nn. 33/34, aprile-settembre 1992.

[12] Lello Voce, Appunti di dinamica dell’ibrido, «Baldus», anno II, n. 1, 1991\. Il documento è disponibile all’indirizzo www.lellovoce.it/Appunti-di-dinamica-dell-ibrido.

[13] Biagio Cepollaro, La conoscenza del poeta: metamorfosi del realismo, «Baldus», anno II, n. 1, 1991\. Il documento è disponibile all’indirizzo www.cepollaro.it/nuova_pagina_50.htm.

[14] Cfr. Biagio Cepollaro, Idioletto, in Perché i poeti?, disponibile al link www.cepollaro.it/new_page_2.htm pp. 14-16.

[15] Cfr. Mariano Baino, Biagio Cepollaro, Lello Voce, A proposito delle Tesi di Lecce, «Baldus», n. 0, 1990, consultabile online all’indirizzo www.absolutepoetry.org/L-editoriale-del-n-o-0-e-i.

[16] Biagio Cepollaro, La conoscenza del poeta…, cit.

[17] Lello Voce, Appunti di dinamica dell’ibrido, cit.

[18] Per questa breve ricognizione sull’allegoria e il Gruppo 93 mi rifaccio in particolare al capitolo V del lavoro di Angelo Petrella Avanguardia, postmoderno e allegoria: teoria e poesia nell’esperienza del Gruppo ’93, Edizioni Biagio Cepollaro, 2007.

[19] AA. VV., Gruppo ’93: la recente avventura del dibattito teorico letterario in Italia, a cura di Filippo Bettini e Francesco Muzzioli, Lecce, Piero Manni, 1990, p. 13.

[20] Romano Luperini, Un confronto tra posizioni diverse, in AA. VV. Gruppo ’93: la recente avventura… cit., 1990, pp. 38-39.

</div>

<div class="item-footer">Published 06:00 • 2 days ago</div>

</div>

</article>

* * *

<article class="item">

### [Angelo Ferracuti: “Mario, non ci resta che l’amore”](https://www.nazioneindiana.com/2020/09/30/angelo-ferracuti-mario-non-ci-resta-che-lamore/)

<div class="item-body">

<div class="item-snippet">    «”Non è che a me le persone interessino per fotografarle, mi interessano perché esistono. Diversamente, il fotogiornalismo sarebbe soltanto una sequenza di scatti senz’anima”, diceva… »   Mario, non ci resta che l’amore di  Angelo Ferracuti -dedicato alla figura di Mario Dondero– è il nono libro dei Cervi Volanti, la collana di scritture poeti</div>

<div class="item-content item-summary">

♦

«”Non è che a me le persone interessino per fotografarle,

mi interessano perché esistono. Diversamente, il fotogiornalismo

sarebbe soltanto una sequenza di scatti senz’anima”, diceva… »

Mario, non ci resta che l’amore di  Angelo Ferracuti -dedicato alla figura di Mario Dondero– è il nono libro dei Cervi Volanti, la collana di scritture poetiche che curo insieme a Giuditta Chiaraluce all’interno del progetto Edizioni Volatili.

Libri come laboratori, primi confronti, materie pensanti, montaggi e scavi attraverso la carta; libri senza profitto, in tiratura limitata (esoeditoria), evidenti nella loro invisibilità e indirizzati a chi saprà ospitarne l’implicita consegna; libri col solo intento di essere vigilie per una geografia del dopo-diluvio.

Pubblico qui alcune pagine in anteprima, insieme a un estratto dal testo. Le partiture visive e i segnalibri sono di Giuditta Chiaraluce. Il ritratto fotografico è un contributo di Marco Cruciani.

♦

Un giorno eravamo insieme a Milano per incontrare Giovanni Pesce, l’eroe della resistenza italiana nella sua casa di Piazza Bonomelli. Era una giornata molto afosa in una Milano semideserta. Mario era arrivato con una bottiglia di prosecco e una vaschetta di gelato, le macchine fotografiche in spalla, e proprio il giorno dopo sarebbe partito per la Russia per realizzare un reportage con il giornalista Astrit Dakli sul post-comunismo, “I rifugi di Lenin”. Ci aveva accolto «la compagna Sandra», ovvero sua moglie Onorina, in questo appartamento buio dove avevamo conversato per un paio d’ore. Volevo da Pesce una testimonianza su Giuseppe Di Vittorio, Nicoletti, per il libro che stavo facendo con Mario, “Di Vittorio a memoria”, commissionatoci dalla Cgil, che incontrò prima a Guadalajara e poi a Ventotene. Mi aspettavo un racconto vivido, pieno di aneddoti, come piacciono a me. Quelle piccole storie che messe tutte insieme fanno la Storia. Invece lo trovai stanco, quasi senza più voglia di raccontare, si limitava a rispondere l’essenziale, poche frasi significative ma brevi.

Mario, dopo averli riempiti di attenzione e di affetto, mostrando loro le sue foto scattate proprio in Spagna, una delle sue ripetute ossessioni, chiese se potesse fotografarli. Eravamo in un tinello buio, la poca luce arrivava dalla portafinestra che dava sul balcone, faceva molto caldo, e loro due si misero uno accanto all’altro in attesa che scattasse, come una coppia di anziani qualunque nel tinello di un appartamento.

Pensavo venisse fuori una foto troppo scura, e temevo per il nostro libro che avrebbe perso una voce importante. Invece, quando dopo qualche mese Mario mi mostrò la foto m’impressionò moltissimo quel ritratto, e anche oggi continua a colpirmi. Lui aveva visto in macchina quello che io non ero riuscito a vedere, e che tutto quel tempo empatico era riuscito a creare, cioè la bellezza nuda di due persone giuste della storia, illuminate da una luce che le rendeva umanissime.

♦

♦

Angelo Ferracuti è nato nel 1960\. Ha pubblicato Attenti al cane (Guanda, 2000), Le risorse umane (Feltrinelli, 2006, Premio “Sandro Onofri”), Viaggi da Fermo (2009), Il costo della vita (Einaudi, 2013, Premio “Lo Straniero”), Andare, camminare, lavorare (Feltrinelli, 2015), Addio (Chiarelettere, 2016). Scrive su “il manifesto”, “La Lettura” del “Corriere della Sera”, “Il Venerdì” di Repubblica, e collabora con Radio Tre.

</div>

<div class="item-footer">Published 05:00 • 2 days ago</div>

</div>

</article>

## Tuesday, 29\. September 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [Da Scurau](https://www.nazioneindiana.com/2020/09/29/giuseppenibali/)

<div class="item-body">

<div class="item-snippet">di Giuseppe Nibali Ultima voce chiama il sangue. Campo cruento gli uomini, altro sangue per le donne è il giorno. Tutti sono convocati, vecchi e nuovi viventi aspettano un gesto per sbranarsi. Il rivolo aspettano, verticale sullo sterno, il morituri stabilito dalla nascita, nella nascita futura rivelato.… Leggi il resto »</div>

<div class="item-content item-summary">

di Giuseppe Nibali

Ultima voce chiama il sangue.

Campo cruento gli uomini, altro sangue per le donne  
è il giorno. Tutti sono convocati, vecchi e nuovi  
viventi aspettano un gesto per sbranarsi. Il rivolo  
aspettano, verticale sullo sterno, il morituri stabilito  
dalla nascita, nella nascita futura rivelato. È tempo  
adesso per il sesso tra gli attori, gambe nude, lividi,  
dai piedi fino all’ano serpi, piaghe fili lo sfondo fuori  
anche case, molte, come in cerca vergognosa della luce.  
Altro mai, nemmeno nella voce, nella voce ultima

**

due mesi, niente. La città è andata avanti  
verso il mondo universo. Vi colpisce fino  
ai buchi dentro i nervi fino ai chiodi tra le  
costole. La città ha dato germe. Resistete  
non muovete il braccio: respirare restare  
come gli uomini che sono uomini due volte,  
una per l’origine l’altra per il tempo.

**

Corpi cavi enormi, gonne e questi figli come squarcio.  
Crolla la religione, Meroè, di chi conosce il tormento  
di giocare fino al buco dell’abisso; lo sgravo che ricordi  
gli spruzzi di merda sul lenzuolo e dentro l’amigdala  
appena lavati macelli, vene scure, osiamo dire:

Cattedrale vuota l’ulivo schiacciato contro il greto  
i rami le foglie lo schianto lo scantu della scorza  
materna sul petto. Matriarcato dei giochi l’ikea  
i segni, questi, del nuovo potere; parola della madre.

Sì, siamo la madre. La morte la morte. La morte.

**

Dal lato filtra l’acquenere nel cemento, passa rifugi antiaerei, tracce di ferrovia. Ai liquami arriva, alle ossa degli antichi. Di qua un nuovo cimitero: cavi molti, un prato. Per lo scopo i piedi premono sul vetro, le mani stanno in preghiera. Mio e comune il giorno in cui ho pisciato via dalla fica il flare, il colpo del sole sulle labbra. Un silenzio primitivo, il viso è morto, non vedi? Le strade, anche le strade, le gallerie come arterie di donna, le vedi? Le sorveglia un’altra volontà. Allora nulla si è sfatto da quanto siamo, non hai da cercare, né manca in TV di guardare i fiati sfiniti degli amanti, il collo che si curva di un airone. Così è fino alla matrice, allorché del maschio e della femmina farete un unico essere sicché non vi sia più né maschio né femmina. Così è fino alla matrice, alla prima carne strappata da uno stomaco.

**

Che bestia sei. Che bestia mentre aspetti col muso l’acqua  
battere sul dorso e le zampe arrivare alla nuca, mentre latri  
all’erba che spacca in giardino il pezzo vicino di cemento.  
Dalle foglie ritorna il grecale, la pioggia passa dalla feritoia  
nella casa, dai tendoni che coprono i raggi. Questo e di come  
ci siamo dimenticati, di come è successo in fretta. Tenendoti  
tu ai miei fianchi io alla maglia stesa accanto. Ora è la mossa.  
Fermo. La Bugonia. Solleva le mani dai fianchi, la mossa che  
faccio col culo. Svella piano la carcassa mia dalle labbra,  
la carcassa qui esplosa, il suo fegato emerso dalle piume.

**

Vi seguirà il male dietro l’edera, e di sopra,  
sul balcone in lamiera che avete per rifugio.  
Non è il tempo delle corse alla ringhiera  
mentre lo sfondo si disossa, e passa dall’arco delle vie  
per la montagna. È morto anche il vecchio prete  
di Ragalna, per la fine del suo giorno una domenica.

Chissà che luce vi assale lì dai tetti, dove il sole si  
inurba coi pastori fra i negozi e che fatica morire  
anche voi nella chiesa col barrito alto della fiera.  
Qui nel lontano la nebbia muove la pianura  
sopra i ponti, dalla miseria di colline, altre volte  
fuori alla finestra si alza lo scheletro di un albero.

**

Sgruma tutto l’osso, spolpa le parti del petto non ti fermare  
nei calli; tu scheggia gli incisivi sull’osso. Questo amore  
con la carne ci ha fatti bestiame umano di denti e radice  
di pianta tutta antica tutta crudo inverno. Sgruma l’osso  
anzi ascolta: un fischio convoca all’oggi i cadaveri nati  
sotto l’orrore delle dita.

Anche la donna, Vera, che pare tua figlia, la donna che guarda  
non un punto ma le case, è carne putrefatta già nell’atomo  
nel ribosio è carne di rovina, carne rischiarata dai fotoni  
al virginale. Non le mani guarda ma la felce cresciuta  
sul buio, che il buio tormenta col suo verde. Sui tasti supera  
comuni di uomini in trofallassi. Crea suono.

</div>

<div class="item-footer">Published 05:00 • 3 days ago</div>

</div>

</article>

## Monday, 28\. September 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [La moltiplicazione del Signor Distruggere](https://www.nazioneindiana.com/2020/09/28/la-moltiplicazione-del-signor-distruggere/)

<div class="item-body">

<div class="item-snippet">di Helena Janeczek Abbiamo un problema, un problema culturale e politico. L’onnipresenza del linguaggio misogino (misogino e non più solo maschilista) che nello spazio dei media (social e tradizionali) è diventato da tempo la norma della violenza verbale. Normale il tweet di Massimiliano Parente – l’ennesimo – che usa come sinonimo di “minchia” il cognome di Michela Murgia, mentre a lei</div>

<div class="item-content item-summary">

di Helena Janeczek  
♦Abbiamo un problema, un problema culturale e politico.  
L’onnipresenza del linguaggio misogino (misogino e non più solo maschilista) che nello spazio dei media (social e tradizionali) è diventato da tempo la norma della violenza verbale.  
Normale il tweet di Massimiliano Parente – l’ennesimo – che usa come sinonimo di “minchia” il cognome di Michela Murgia, mentre a lei pare cosa di un altro mondo che una rivista americana la difenda da un troll fascistoide. O Marco Gervasoni che, qualche settimana prima, commentava la copertina dedicata a Elly Schlein con “ma questa è n’omo?”  
Lo “scandalo” della modella Armine Harutyunyan scoppiato a un anno dalla sfilata di Gucci e divampato solo in Italia. Le valanghe di veleno su Greta Thunberg, ricorrenti come le anomalie meteorologiche causate dal cambiamento climatico.

Ogni volta che Vittorio Sgarbi urla “troia, puttana” a favore delle telecamere, con i video che poi diventano virali perché un sacco di ragazzi lo trova divertente, come se fosse un rapper blastatore o il “Signor Distruggere” con il suo milione di seguaci.  
Ecco, il problema è la moltiplicazione – metaforica e reale – dei “Signor Distruggere”.  
Il fatto che le sparate di uno scrittore o di un docente universitario mirino a una visibilità equiparabile a quella del personaggio social, per non dire a quella di Sgarbi, Feltri, Cruciani.  
E che quindi sarebbe doppiamente raccomandabile che la donna sotto attacco non rispondesse: per non concedere la visibilità cercata all’attaccabrighe e, oltretutto, perché le donne non dovrebbero mai abbassarsi a certi toni.  
Le donne dovrebbero stare salde come le querce nelle tempeste di shitstorm, ringraziare se gli tocca solo qualche “cessa”, rispondere semmai con pacatezza e pazienza dialogante.  
Cosa che, in realtà, non le mette al riparo. Si è visto recentemente con le polemiche intorno a Vera Gheno, la sociolinguistica da cui il presidente dell’Accademia della Crusca, addirittura, si è sentito in dovere di dissociarsi. Vera Gheno ha spiegato fino alla nausea che non pensa affatto di imporre dall’alto lo “shwa” inclusivo perché non si cambia così una lingua e la cultura che veicola. Eppure si è beccata una marea di insulti e dileggi sessisti, reazione incomparabile alle critiche riservate, per esempio, a Federico Faloppa che si occupa di simili tematiche.  
Per Vera Gheno, oltretutto, non vale neppure che a scatenare l’aggressività sia stata “l’iperesposizione” che rende tutti più appetibili per gli odiatori. Con le donne, appunto, scatta anche se non sei “mediatica”. In tutti casi, comunque, le donne non vengono prese di mira per ciò che sterotipicamente rappresentano in base alle proprie scelte (buonista, sinistrato, radical-chic ecc.) ma per ciò che non hanno scelto di essere, riducendole a corpi e emotività “uterina”.  
In un contesto del genere è fallace concentrare l’attenzione sul caso del giorno, far valere simpatie, antipatie, scusanti, distinguo. Questi attacchi possono toccare a qualunque donna dica o faccia – anche inconsapevolmente – qualcosa di “sbagliato” o, se siete uomini, alle vostre sorelle, madri, figlie, compagne, colleghe, amiche.  
Infine si tratta di dinamiche che investono la spazio pubblico, la polis virtuale che non è il bar sotto casa, anche se fa di tutto per somigliarvi. Una dimensione dove non c’entra nulla il giudizio sul valore letterario (o accademico) di ciò che hanno pubblicato i contendenti, né le relazioni private tra le persone.

ps. Non linkare tweet, video e pagine criticate è naturalmente una mia scelta.

</div>

<div class="item-footer">Published 12:39 • 4 days ago</div>

</div>

</article>

* * *

<article class="item">

### [Le forme dell’amore](https://www.nazioneindiana.com/2020/09/28/le-forme-dellamore/)

<div class="item-body">

<div class="item-snippet">di Matteo Quaglia   Mamma diceva sempre che non esiste solitudine, per chi non apprezza la compagnia. Secondo lei, per soffrire davvero, era necessario conoscere le alternative. Papà era un tipo più pragmatico. Un campione dell’evoluzionismo darwiniano. Non solo perché ha modificato la propria caratura umana in modo direttamente proporzionale al lievitare della sua influenza nell’Apparato, ma</div>

<div class="item-content item-summary">

di Matteo Quaglia

Mamma diceva sempre che non esiste solitudine, per chi non apprezza la compagnia. Secondo lei, per soffrire davvero, era necessario conoscere le alternative.

Papà era un tipo più pragmatico. Un campione dell’evoluzionismo darwiniano. Non solo perché ha modificato la propria caratura umana in modo direttamente proporzionale al lievitare della sua influenza nell’Apparato, ma anche perché è venuto a patti con lo stravolgimento di una vita, pianificata, fino a quel momento, al minimo dettaglio. Questo non significa che sia stato un cattivo padre. Non uno di quelli a cui si chiede di leggerti la storia della buonanotte. Non uno di quelli che ti accompagnano al corso di danza, o al cinema. Il suo pragmatismo lo ha guidato lungo le nostre vite con l’ostinazione del sordo. Ha chiuso gli occhi ed è andato avanti per la sua strada.

L’ultimo ricordo che ho, di lui in vita, è di questa mattina. Un uomo con la cornetta in mano e le gambe di burro. Quando hanno telefonato per dirci che mamma era morta, la faccia di papà ha assunto l’espressione di certi quadri di Courbet. Non ha versato una lacrima e non ha detto una parola. Si è chiuso nel suo studio e da quel momento, per lui, ho smesso di esistere.

Sebbene fossimo preparati alla notizia, la scomparsa di qualcuno che ami è sempre un pugno su per il culo.

Papà si è impiccato senza lasciare nemmeno una lettera, o un’altra forma di addio. L’ennesima dimostrazione dell’economicità di un certo funzionalismo.

Quando ho bussato e non ho ricevuto risposta, ho inspirato e ho chiuso gli occhi. Ho aperto la porta del suo studio ed era lì, appeso, con una specie di ghigno che ricordava un sorriso. L’ho tirato giù e l’ho adagiato sul tappeto. Mio padre era un uomo compatto, ma non avrei mai detto che pesasse così tanto. Era così compatto che, se mi ha voluto bene, è solo perché amava mamma. Il suo sentimento, nei miei confronti, una forma di rispetto per la donna che amava così tanto.

Così, in un colpo solo, ho perso mamma e papà. Ho sempre creduto che questo genere di coincidenze capitassero solo nei b movies, o in seguito a qualche cataclisma. La verità è che la vita sa essere più grottesca di ogni finzione o evento naturale. La verità è che papà non mi ha mai voluto e, dopo la pensione, mamma era il suo unico motivo di vita. Quando ti dicono che si può imparare a voler bene alle persone, be’, è una grande cazzata. Imparare non è da tutti. Voler bene non è da tutti. Dicono che si possa imparare dai propri sbagli, ma non è sempre vero. Ho capito che mio padre si era sempre adattato a tutto, perché non era in grado di adeguarsi a niente.

Dopo aver tirato giù papà e avergli dato quel minimo di compostezza cui aveva sempre aspirato in vita, ho chiamato il portavoce dell’Apparato per dargli la notizia. Gli ho detto che papà era morto d’infarto. Gli ho detto che avrei utilizzato la Macchina, quel giorno stesso, perché avevo una cosa da fare. Il portavoce dell’Apparato ha risposto indicandomi un indirizzo e un’ora precisa. Ha detto che mi avrebbero aspettato lì, con la Macchina e le condoglianze.

Mamma mi raccontava sempre che, quando ha scoperto di essere in attesa, ha trascorso un pomeriggio pensando al nome da darmi. Papà non le è stato molto d’aiuto. Più per questione di volontà, che per mancanza di immaginazione. Se avesse conosciuto le conseguenze che il parto avrebbe avuto sulla salute di mamma, papà l’avrebbe costretta ad abortire. Non voglio dire che, così facendo, avrebbe risolto due problemi in uno, ma insomma.

Il punto è che papà amava mamma e ha accettato la sua volontà di diventare madre e di consacrare quell’errore in una forma di amore. Ha accettato la mia nascita come un moscerino che ti si infila nell’occhio.

Mamma mi ha più volte detto che non sono stata proprio il frutto di uno sbaglio, perché non è mai proprio così. Ha ripetuto che, se anche sono stata uno sbaglio, sono lo sbaglio che l’ha resa più felice. Il nome che mamma ha deciso di darmi, quando sono nata, è Mia.

Ho raggiunto l’indirizzo all’ora indicata e ho trovato gli uomini dell’Apparato ad attendermi, i volti scuri di chi si sforza di dimostrare dolore e comprensione. Ci sono state strette di mano e parole di circostanza. Mi hanno chiesto se sapessi come utilizzare la Macchina. Ho detto di sì. Mi hanno detto che non ci sarebbe stato bisogno di riportare la Macchina indietro, perché la Macchina è sempre ovunque.

Così sono arrivata in quell’altro indirizzo preciso, a quell’ora precisa di diciannove anni fa. Mamma mi ha raccontato che lei e papà si sono conosciuti un pomeriggio piovoso, in un vecchio caffè. A mamma era caduto il libro che stava leggendo all’epoca, papà l’aveva raccolto e da lì era nato tutto quanto. Da lì ero nata io.

Vedo mamma. È giovane, una ragazza con la spensieratezza di chi ama le giornate di maggio. Il libro sotto il braccio. Mi hanno sempre detto che sono mia madre da giovane, ora ne ho la prova. Forse è questo il motivo per cui, papà, non ha mai utilizzato la Macchina per cambiare il corso degli eventi. Per cancellarmi dalla storia. Forse, questa è la stata la sua personalissima forma di amore.

Ora è il mio turno per fare ciò che va fatto.

Si dice che si impara dai propri sbagli, ma a volte anche gli sbagli imparano. Ed è per questo che siamo tutti e tre dentro il caffè e fuori pioviggina e siamo tre sconosciuti dal destino intrecciato. Da giovane, papà sembra meno duro, meno intagliato nel legno. Forse, è diventato ciò che è diventato in seguito alla mia nascita. A vederlo, ora, pare impossibile che tra diciannove anni si appenderà per il collo.

Mi avvicino a mamma. Lei non mi nota. È appoggiata al banco del bar. È bellissima. Papà è poco distante. Anche papà si avvicina, per ordinare da bere. Mamma si volta verso di me e il libro le scivola da sotto il braccio e cade al suolo, sollevando un piccolo sbuffo di polvere. Mi chino e raccolgo il libro. Lo porgo a mamma, che mi sorride. Papà nota la scena e mi sorride. Poi abbassa gli occhi sul giornale che sta leggendo.

Mi guardo le mani. Sto iniziando a scomparire, come Marty McFly in Ritorno al Futuro. Certe volte la vita è davvero come un film, a quanto pare.

Ho la testa ovattata, tanto che posso sentire i battiti del mio cuore farsi sempre più leggeri, mentre un raggio di sole bianchissimo attraversa i vetri impolverati del bar e  mi bagna di luce. Mentre scompaio per sempre, nel pomeriggio, ripenso a quella frase di mamma, secondo cui, per soffrire davvero, è necessario conoscere le alternative. Mi chiedo se papà soffrirà. O se, senza di me, la sua vita sarà più felice. Mamma non mi avrà, ma mi ha già avuta, in un’altra vita. E vissero tutti felici e contenti, in un modo o nell’altro, senza di me e con me.

I miei occhi incrociano di nuovo quelli di mamma e le labbra di mamma si aprono e mamma sembra sapere chi sono e mi dice qualcosa, che non riesco a capire, e in un attimo sono solo un ricordo, un’altra forma dell’amore, e scompaio con uno sbuffo negli angoli più reconditi della vita futura di mamma e papà.

</div>

<div class="item-footer">Published 05:00 • 4 days ago</div>

</div>

</article>

## Sunday, 27\. September 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [Livraisons](https://www.nazioneindiana.com/2020/09/27/livraisons/)

<div class="item-body">

<div class="item-snippet">Una libreria in inglese   Di Angelo Vannini     A Phyllis Cohen, e alla sua libreria di sogni   Non sono mai stato la persona adatta a questo, pensai una volta arrivato davanti alla porta, benché io a volte sia capace di fare quello che altri non possono fare in un lasso così breve di tempo, è evidente che non sono mai stato adatto, a questo come a ogni altra cosa del resto, per</div>

<div class="item-content item-summary">♦

photo by Robert Mack

Una libreria in inglese

Di Angelo Vannini

A Phyllis Cohen,

e alla sua libreria di sogni

Non sono mai stato la persona adatta a questo, pensai una volta arrivato davanti alla porta, benché io a volte sia capace di fare quello che altri non possono fare in un lasso così breve di tempo, è evidente che non sono mai stato adatto, a questo come a ogni altra cosa del resto, perché tutto quello che ho intrapreso in vita mia è sempre capitato mio malgrado, anche se per idea mia, anche se profondamente voluto da me, come questa idea assurda e terribile di lasciare Perpignano per andare a vivere ad Aix-en-Provence, una cosa che certamente ho voluto io ma che è andata, fin da subito, contro me stesso, perché era evidente, e lo era fin dall’inizio, che mai sarei stato all’altezza di fare quello che ad Aix-en-Provence mi era chiesto di fare, scrivere seicento pagine, solo a me poteva venire in mente di lanciarmi in un’impresa del genere, dato che non avevo tempo la mattina, né il pomeriggio e tantomeno la sera, e poi anche con tutto il tempo del mondo era perfettamente chiaro che non sarei potuto riuscire a scrivere seicento pagine tutte in francese, perché io non sono adatto a queste cose, anche se a volte preferisco mentirmi e non voglio riconoscerlo, anche se la gente non vede la mia inadeguatezza per quella che è, la mia insufficienza rispetto alle idee che si fanno di me, o delle mie capacità, che sono del tutto errate.

Sono idee del tutto errate, come errato è questo progetto, pensai davanti alla porta, un progetto per cui io divento un anello assolutamente indispensabile, e che farò certamente fallire, se soltanto questa cosa fosse vera, ma vera, voglio augurarmi, non è, e allora con l’aiuto della sorte, perché di sfighe ne ho avute tante, una dietro l’altra, sempre, da quindici anni a questa parte, ininterrottamente, tantoché mi dico che prima o poi questa lunga discesa, per quanto evidentemente senza fondo, come sono tutte le discese quando sono vere discese, e non finte, dovrà incontrare qualche soprassalto, uno o due, una cosa tra mille che va per il verso giusto, almeno una volta, cristo, una dico, forse con una manna dal cielo ce la farò, e ci sarà una cosa che riesce come dovrebbe riuscire, pensavo davanti alla porta, almeno una e per cui saranno contenti del mio operato, e penseranno che sia anche merito mio se sono riusciti a raggiungere un obiettivo, come appariva allora, tra i più difficili, anche se è e rimane del tutto improbabile, ma qui è tutto nero, pensavo, non c’è nemmeno una lucina eppure non possono essere se non qua dentro, almeno così mi hanno detto, pensai appena vidi tutto chiuso e sbarrato, l’interno preso dal buio, ma la serranda non era abbassata e in vetrina ancora si vedevano libri come se non fosse chiuso, libri ovunque, da ogni parte, libri come se piovessero, eppure la porta non si apriva, maledettamente, cominciai a pensare, era chiaro che non si sarebbe aperta con la scalogna maledetta che mi porto dietro, e come faccio ora che loro non mi vedono né possono sentire dato che io non posso urlare, non devo attirare l’attenzione proprio ora, pensavo, dato che qui sul marciapiede la mia situazione è irrimediabilmente illegale, di sicuro mi capiterà qualcosa se rimango ancora in questa strada, le volanti non passavano mai nella rue Delavigne, mi avevano detto loro, ma io di queste cose non mi sono mai fidato in vita mia perché non so quante volte sono stato controllato in situazioni completamente improbabili, come quella volta ad Ancona mentre passeggiavo con un amico, tranquillamente, nella maniera più tranquilla del mondo un piede dopo l’altro sul marciapiede della Via Nazionale, quando appena svoltati a sinistra davanti a un bar tre carabinieri col mitra, pareva che aspettassero proprio noi quei diavoletti, mezz’ora per controllare la carta d’identità via radio mentre ci tenevano sotto tiro col mitra come fossimo banditi usciti da una rapina, ed eravamo pure vestiti bene quella volta, camicette abbottonate e appena stirate, un primo pomeriggio d’estate, che cazzo ci facevano lì col mitra in un pomeriggio d’estate, pensai mentre ero davanti alla porta, con la sfiga che mi ritrovo passerà sicuramente una volante dei gendarmi stanotte, passerà proprio qui se non mi sbrigo ad entrare, ma loro non rispondono, anche quando comincio a bussare, non c’è nessuno dentro porco cane, m’hanno lasciato qui nella merda, era prevedibile, era assolutamente prevedibile che sarebbe stato un viaggio fatto completamente a vuoto e che mi sarebbe costato caro, avevo pensato mentre nessuno da dentro rispondeva, esposto nel mezzo della notte ad ogni tipo di ispezione, multa, prelevamento e incarceramento, tutto questo era chiaro che sarebbe successo, se all’improvviso, dopo non so quanto tempo, non mi avessero aperto. Io non ero adatto a quelle cose, a tutte quelle cose voglio dire, quello che facevo ad Aix-en-Provence come quello che avrei dovuto fare lì a Parigi, non ci sarei mai riuscito, per non parlare poi di quello che avevo fatto a Perpignano e per cui, quasi, ero morto di fatica e follia, con la schiena a pezzi e otto chili in più che non riuscivo a smaltire, per quanto corressi, per anni, tutta colpa del mio metabolismo, anche con quello sono stato sfigato, una che ne andasse dritta non c’era né ci sarà mai, la merda surgelata, parevo avanzando, se soltanto qualcuno mi avesse guardato accuratamente se ne sarebbe accorto, era evidente, ma nessuno mi guardava accuratamente e la cosa non mi sorprende, perché io stesso avrei fatto di tutto per tenermi lontano dalla mia vista, se solo avessi potuto, invece non potevo e mi toccava essere di nuovo lì con me stesso nel mezzo della notte aspettando che mi aprissero, e anche dopo che mi avevano aperto ero rimasto solo con me stesso, in mezzo a tutti gli altri che a poco a poco si erano alzati e visibilmente non erano per niente contenti del mio arrivo, e come biasimarli del resto, dato che neanche io ero contento, quella volta come ogni volta, dell’arrivo, che evidentemente non ero capace di fare e che fallivo miserabilmente in modi sempre più disastrosi e avvilenti.

A Perpignano, almeno, ero potuto sparire, anche se solo per un lasso limitato di tempo, a Perpignano avevo potuto far finta di non esistere e questo aveva potuto confortarmi, per un po’, e soltanto relativamente, ma poi da Perpignano decisi di andarmene per tentare questa follia di Aix-en-Provence che pagherò certamente caro, e presto, non appena la mia impossibilità di adempiere il contratto che ho firmato cinque mesi fa diverrà chiara a tutti, tempo un anno o due al massimo, pensai una volta entrato nella libreria, che non so perché ma non pareva una libreria anche se era piena di libri, e quella cosa non era certo di buon augurio, pensai, e subito pensai a non pensarlo, non più per tutto il resto del viaggio, tempo un anno o due e si accorgeranno dell’errore madornale che hanno fatto con me, sarò espulso dal centro di matematica applicata e mi toccherà fuggire da Aix-en-Provence e probabilmente tornare a Perpignano, anche se a Perpignano non ho più niente da fare, ma almeno da Perpignano potrei andare facilmente, si fa per dire, a lavorare in Catalogna, fare avanti e indietro tra la Spagna e la Francia per insegnare a scuola, finché, pensai, non mi sarebbe esplosa la testa.

A Perpignano sarebbe stato possibile, mentre ad Aix no, ma io in quel momento mi trovavo ad Aix, dove tutto sarebbe, un giorno all’altro, precipitato, anzi no, non ero più ad Aix, ero appena arrivato a Parigi senza sapere neanche il perché, imbarcato in un progetto completamente folle per cui non ero certamente all’altezza, mi trovo completamente allo sbando, pensai una volta entrato, è un miracolo che non mi abbiano già arrestato e chissà come farò, a festa finita, per ritornare a casa, a festa finita avevo pensato, anche se era chiaro che non era una festa la ragione per cui mi avevano voluto lì, una ragione di estrema ed impressionante urgenza, mi aveva detto Éléonore per telefono, fiondati ti prego, aveva detto e subito mi ero fiondato, come se avessero avuto davvero bisogno di me quando era del tutto evidente che io ero e sarei rimasto in ogni senso superfluo, e non si capiva perché continuassero a chiamare me, dato che non ero certamente il migliore in questo mestiere, che tra l’altro non era un mestiere perché non poteva darmi da vivere dal momento che facevo di tutto per restare nella legalità, e io non volevo essere uno illegale, assolutamente, mai avrei accettato di esserlo, e quindi mi toccava farmi assumere di anno in anno dai dipartimenti più svariati delle più svariate università per avere di cosa pagare l’affitto e comprare il pane, io non ero ricco e anche su questo ero stato sfortunato, perché c’è chi nasce senza problemi di soldi e senza problemi di soldi finirà per morire, ma io non facevo parte di questa categoria, i miei erano poveri cristi nati e cresciuti a Vaccarile dove pure io ero nato e cresciuto, prima di finire ad Urbino assieme ad altri poveri cristi, quanti crocifissi, mio dio, pensai una volta finito dentro alla libreria, quanti ne ho visti in tutto e quanti ne sarò ancora condannato a vedere. E questi qua pure erano poveri cristi, pensai, quello là da Buenos Aires è dovuto fuggire e da anni si nasconde a Parigi sotto falso nome, questa che da Chicago è venuta a ripercorrere le orme della Resistenza francese, quell’altro che si spaccia per greco ma in verità è apolide, dove cazzo sono finito, pensai, e soprattutto perché, per quale dannata ragione mi sono imbarcato in questa impresa chiaramente destinata a fallire, e per di più in un momento come questo, in cui sarei dovuto rimanere ad Aix-en-Provence a fare quello che stavo facendo, cioè niente, perché niente ero in grado di fare ad Aix-en-Provence dal momento che anche lì mi ero imbarcato in un’impresa impossibile, seicento pagine di formule, e per di più in francese, formule che mai sarei riuscito a scrivere, dovendo lavorare mattina pomeriggio e sera soltanto per tirare a campare, questo mondo è un mondo di santi, pensai una volta tolto il giaccone, tutto un sacrificio e nessuna redenzione, almeno non in questa parte della vita, e io non credevo all’altra parte, non sono uno che crede facilmente, pensai, non ho mai creduto alla befana per esempio, o a babbo natale, quando nonna e nonno ci venivano a trovare dicendo questo te lo manda babbo natale io sparavo già allora una pernacchia, e correvo via, perché capivo, se non vedo non credo, ma quello che vedo credo, e vedevo tutti i santi, i sacrificati, i matti, le bollette da pagare e le madonne da tirare, e chissenefrega, dicevo al prete ogni volta che mi pronosticava l’inferno, e avevo ragione io, perché l’inferno è in questa terra, non in quella. Togliti la merda dalle ossa, vedi se puzza ancora, dicevo ogni volta a mia sorella, se puzza ancora è perché tutto è dentro, rogna pure nel sangue, aveva fatto bene lei a lasciare l’Europa per rifugiarsi in Vietnam, e mi dicevo spesso che anche io avrei dovuto raggiungerla, mettere una croce sopra a tutto quanto e partire, ma poi quando andai in estate mi resi conto che non era meglio, la vita lì era uno scatafascio esattamente come qua, esattamente come qua si tribolava per le stesse ragioni per cui tribolavamo qua, cosa ho fatto di male io, pensavo allora da mia sorella, che neanche qui posso stare in pace due minuti, è proprio vero che è nel sangue, mi dicevo, e già ero tornato via, ero a Perpignano ancora prima di essere tornato a Perpignano, e una volta tornato davvero a Perpignano ci misi poco per andare ad Aix-en-Provence senza esserci ancora andato, pensai mentre guardavo i libri che erano ovunque, per terra e sugli scaffali, tutti in inglese cristo santo, neanche un testo in francese in una libreria del sesto arrondissement, manco fossimo davvero a Berkeley, mi dissi, perché noi viaggiamo con la mente prima del corpo, e solo dopo il corpo segue, ma a volte è il corpo che va e la mente che tiene, non si muove, e allora chissà se mai mi sono mosso da Vaccarile, pensai davanti agli scaffali tutti in inglese, a Perpignano forse non ci ero mai arrivato e me ne rendevo conto in quel momento stesso, mentre mi preparavo a essere nuovamente inutile per me e per tutti in una faccenda, come disse Éléonore, della massima urgenza, che certamente non avrei saputo affrontare nella maniera adeguata, ammesso che una maniera adeguata potesse mai esistere in un mondo come quello in cui siamo stati condannati a vivere.

Ma porca, vociferavo, porca, sempre dentro di me, mentre quelli si muovevano tutti in coro per sistemarmi, era quasi commovente tutto quel giostrare all’unisono attorno al mio materiale, devono tenerci davvero, pensai, se in piena notte ancora non mi hanno mandato a cagare, io mi sarei mandato a cagare molto spesso, se avessi potuto, pensai, e soprattutto per esser piombato dal nulla con così tanto ritardo, ma cosa mi è saltato in mente, partire da Aix-en-Provence in una situazione di emergenza sanitaria assoluta per andare illegalmente a Parigi, al fine di compiere un’operazione che mai sarei riuscito a compiere, in pieno lockdown, e questo avrebbe dovuto essere sotto gli occhi di tutti, ma loro probabilmente fingevano di non vedere, era davvero improbabile che pensassero si potesse realizzare grazie a me quello che volevano realizzare, anche per una combriccola di svitati come erano loro, non era verisimile che ci credessero davvero, pensavo mentre mi sistemavo, in ogni caso non sarei mai riuscito a dormire quella notte, questo era evidente, e anche le seguenti, sarebbe stato impossibile dormire in una situazione come quella, in un posto come quello e con tutto quello che stava succedendo fuori, una città fantasma, mi era sembrata, mi accorsi in quel momento, Parigi al mio arrivo, e probabilmente non sarei mai riuscito ad arrivare in quella libreria senza risvegliare almeno un fantasma. L’unica cosa bella, pensai mentre attaccavo i computer alla corrente, è che a quella gente importa di me, almeno apparentemente, mi hanno sempre detto le cose come stanno, che fanno quello che fanno non per soldi, perché non li hanno e mai li avranno, ma per giustizia, per giustizia fanno quello che fanno perché quando il mondo è rotto c’è chi pensa ancora che bisogna aggiustarlo, e questo era per me l’unica cosa bella che però non poteva darmi sollievo perché io non ero all’altezza, non avrei mai potuto far parte del loro gruppo in pianta stabile senza morire, un giorno o l’altro, di fame, e non capivo come facevano loro, a non morire di fame, uno con meno lavoro dell’altro, chissà quale era il segreto, chi gli spesava l’affitto, dato che, ne ero sicuro, non era il commercio perché non vendevano, non era il Centro di Mediazione Anticoloniale perché ancora non esisteva, ancora quel centro non era un centro, dato che sarebbero divenuti loro il centro, ognuno di loro e tutti assieme, un passeraio, non avevano neanche un ufficio se non quel buco di libreria in cui si rifugiavano di tanto in tanto da qualche mese, a quanto Éléonore al telefono mi aveva detto, ma come si fa, pensai, a essere in una situazione come questa, in un posto come questo, cacciato qui senza nessuna possibilità di successo, niente, a Parigi non riesco, ad Aix sarà un fallimento, Perpignano ormai per me non esiste più, come mi sono ridotto, tra l’altro non si sa nemmeno se ad Aix riuscirò a tornare senza farmi arrestare, solo a me poteva capitare una situazione così, per cui l’unico posto sicuro è questo qui, una libreria in inglese, chissà come ci sono finiti gli altri, pensai, il quartiere dell’Odéon, dico, roba da matti, voler guarire il mondo a partire da qui, come fosse un sogno o un bisogno, una fisima da bel lunedì.

</div>

<div class="item-footer">Published 05:00 • 5 days ago</div>

</div>

</article>

## Saturday, 26\. September 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [Antropocene fantastico](https://www.nazioneindiana.com/2020/09/26/antropocene-fantastico/)

<div class="item-body">

<div class="item-snippet">      In questi giorni è uscito nelle librerie Antropocene fantastico. Scrivere un altro mondo, il nuovo pamphlet di Matteo Meschiari, pubblicato da Armillaria. Ne ospito qui un estratto in anteprima, tratto dal capitolo Kairocene.   KAIROCENE – RIFONDARE IL TEMPO   Quale passato si annida nel futuro?… Leggi il resto »</div>

<div class="item-content item-summary">

♦

In questi giorni è uscito nelle librerie Antropocene fantastico. Scrivere un altro mondo, il nuovo pamphlet di Matteo Meschiari, pubblicato da Armillaria.

Ne ospito qui un estratto in anteprima, tratto dal capitolo Kairocene.

KAIROCENE – RIFONDARE IL TEMPO

Quale passato si annida nel futuro? In che cosa Paleolitico e Antropocene si somigliano? La parola Antropocene è irritante, un’irritazione che viene essenzialmente dalla sua proteiforme adattabilità ai contesti, dalla sua eccessiva carica di seduzione e facilità d’uso. Ma, concettualmente, quello che non convince è la sua perenne atmosfera alla Blade Runner, il suo sapore di futuro a tinte fosche, reale ma banale, come una quinta teatrale fissa, scontata. L’esperienza di Covid-19 ha smentito ogni visione distopica: l’Antropocene è qui senza mutare la percezione del presente. Anzi. Nei comportamenti e nelle atmosfere il presente è venato più di preistoria che di fantascienza. Filosofi oscurantisti e virologi impotenti ci fanno sentire più in un passato immaginato che in futuro promesso. È inquietante, certo, ma si apre una possibilità inedita all’immaginario del dopo: un Antropocene dagli attributi diversi, più debitore a J.R.R. Tolkien che a Philip K. Dick. E Tolkien per me è il vero scrittore-guida in questo momento storico, perché se un Antropocene Fantastico è possibile è solo tornando alla radice di chi ha riflettuto sul fantastico in modo ineguagliato. Come dicevo in precedenza, Tolkien non è il Fantasy, perché lo scarto è tutto tra i due mondi è tutto nell’idea di studio, nella filologia della parola e dello sguardo, e soprattutto nella credenza: Tolkien ci ha lasciato delle istruzioni per l’uso, a una guida mitopoietica del presente e del dopo che ci attende. Fiaba, subcreazione, storytelling fantastico non sono cose da conoscere sulla carta, non sono il destino di un singolo autore, ma sono pratiche sociali, collettive, performative, che hanno il potere di aiutarci a reimmaginare la realtà: «Tutte le narrazioni si possono avverare; pure alla fine, redente, possono risultare non meno simili e insieme dissimili dalle forme da noi date loro, di quanto l’Uomo, finalmente redento, sarà simile e dissimile, insieme, all’uomo caduto a noi noto».

La citazione chiude un testo in cui all’inizio si pongono le basi: Feeria «è un reame che contiene molte altre cose accanto a elfi e fate, oltre a gnomi, streghe, trolls, giganti e draghi: racchiude i mari, il sole, la luna, il cielo, e la terra e tutte le cose che sono in essa, alberi e uccelli, acque e sassi, pane e vino, e noi stessi, uomini mortali, quando siamo vittime di un incantesimo». Il tono apparentemente discorsivo, a tratti bonario, del saggio Sulle fiabe, non deve distrarci con la sua apparente semplicità. Tolkien sta leggendo una conferenza (una Andrew Lang Lecture tenutasi all’università di St Andrews l’8 marzo 1939) in bilico tra filologia e autopoetica. Proprio la sua natura ambigua, duplice, rende difficile estrapolare delle coordinate “utili” a ottant’anni di distanza, ma quello che si dice qui è soprattutto un invito ad aggiustare lo sguardo, una cosa difficile da proporre e da apprendere. Tolkien ci avverte: Feeria non è solo storie di fate o storie di umani tra le fate, Feeria è un luogo, e non dobbiamo smettere di pensare che in quanto luogo è fatta anche di cose “comuni”, “normali”, che in realtà comuni e normali non sono. Su posizioni non troppo lontane da quelle di Viktor Šklovskij sullo straniamento, Tolkien sta dicendo che abbiamo perso la vocazione a guardare il mondo “primario” con attitudine meravigliata. Come recuperare allora lo stupore verso un sasso o una foglia uscendo «dalla tediosa opacità del banale o del familiare»?

La strada non è semplice perché bisognerebbe comprendere e accettare una frase densissima che il filologo e il linguista storico cala nel suo saggio come un fendente: «le lingue, soprattutto le europee moderne, sono una malattia della mitologia». Tolkien, contrariamente a chi dice di eliminarli, elogia la funzione poietica degli aggettivi: «La mente che pensò leggero, pesante, grigio, giallo, immobile, veloce, concepì anche la magia atta a rendere cose pesanti, leggere e atte a volare, a trasformare il grigio piombo in giallo oro, l’immobile roccia in acqua veloce». Questo atto di subcreazione è lo stesso che ritroviamo negli inventori del mito: la mitopoiesi è un atto linguistico primario molto più articolato di una mera architettura allegorica. Il mito non è il tuono che diventa un dio o un irascibile contadino dalla barba rossa elevato a rango divino, il mito è la zona di coesistenza di tuono, Thor e contadino, un luogo di simultaneità narrativa e ontologica che Tolkien chiama appunto Feeria. Feeria è allora la co-possibilità. E dove la co-possibilità dei piani si interrompe, per stanchezza creativa, per cinismo, per disordine cognitivo, per usura, allora ci troviamo di fronte a una specie di “malattia del mito”, una sfiducia della lingua per cui subcreazione e sospensione dell’incredulità sono solo giochi temporanei, fittizi, senza la “credenza” profonda di poter “fare mito” anche nel quotidiano. Il problema, ovviamente, non è solo un nodo epistemologico del mondo contemporaneo. Sono e saranno sempre molto pochi i portatori di parola disposti a credere in un commercio diretto tra mito e tempo presente, in un reale scambio di fluidi tra Feeria e il mondo primario.

♦

Ora, che ci piaccia o meno la parola, siamo entrati nell’Antropocene. Possiamo vedere quest’epoca di transizione e la futura prossima come un’ennesima declinazione distopica, come una serie Netflix da guardare a distanza stando seduti sul divano, oppure possiamo intercettare nell’Antropocene i grandi flussi mitici che, come accade a ogni epoca, lo attraversano e lo alimentano. Tolkien lo dice così: «Costruire un Mondo secondario dentro il quale il sole verde risulti credibile, imponendo Credenza Secondaria, richiederà probabilmente fatica e riflessione, e certamente esigerà una particolare abilità, una sorta di facoltà magica. Pochi si cimentano in compiti così ardui; ma quando li si affronta e li si attua in misura maggiore o minore, si ottiene un risultato artistico senza pari: arte narrativa, insomma, elaborazione di racconti nella forma primaria e più pregnante». È chiaro che chiedersi come sarà la “letteratura del dopo” ha più a che fare con questo, con un sole verde, che non con potenziali e anodini romanzi su Covid-19, distanziamento sociale, contenzione domestica e mascherine a passeggio. Nel collasso e nella Pandemia, e forse proprio per questo, dovremmo ricordarci di quelle che Tolkien chiamava «le cose più permanenti e fondamentali».

Tolkien concepisce il Silmarillon nel 1917\. Suo figlio Christopher lo pubblica postumo nel 1977\. Christopher aveva 53 anni e Guy Gavriel Kay, tra il 1974-75, ne aveva appena 20\. Kay, canadese a Oxford, aiutò Christopher nella riscrittura delle parti più tardive. Il libro, che Tolkien voleva pubblicare assieme al Signore degli anelli, ha avuto una genesi di 60 anni. E nonostante la riscrittura postuma resta un incompiuto. Al suo interno ci sono tempi narrativi e tempi redazionali che formano un intrico così complesso da aver immobilizzato il loro stesso autore. Per noi invece sono un invito a riflettere non sul worldbuilding ma sull’etica della parola: dalla cronaca alla cronologia, dalle agenzie stampa agli annali. Un cambio di prospettiva che potrebbe aiutare a stendere un balsamo calmante sulla fretta di correre a registrare tendenze intellettuali e mode sulle testate on line. Il futuro è crollato. Abbiamo tutto il tempo adesso. Tolkien era sintonizzato su Kairos non su Kronos. Noi certamente non siamo Tolkien, ma siamo lettori e scrittori davanti a una scelta. E questa scelta è di vita o di morte.

</div>

<div class="item-footer">Published 05:00 • 6 days ago</div>

</div>

</article>

## Friday, 25\. September 2020

<article class="item">

#### [Nazione Indiana](https://www.nazioneindiana.com)

### [Storia di farfalle e altre metamorfosi di Chiara Pellegrini – recensione](https://www.nazioneindiana.com/2020/09/25/recensione-silvia-morotti/)

<div class="item-body">

<div class="item-snippet">di Silvia Morotti    Storia di farfalle e altre metamorfosi di Chiara Pellegrini (Robin, 2020)   “A Vincenzo Consolo, maestro di voce, maestro di memoria”: Chiara Pellegrini esordisce con un romanzo, Storia di farfalle e altre metamorfosi, che si apre nel segno di un’educazione letteraria e morale.… Leggi il resto »</div>

<div class="item-content item-summary">

di Silvia Morotti 

Storia di farfalle e altre metamorfosi di Chiara Pellegrini (Robin, 2020)

♦

“A Vincenzo Consolo, maestro di voce, maestro di memoria”: Chiara Pellegrini esordisce con un romanzo, Storia di farfalle e altre metamorfosi, che si apre nel segno di un’educazione letteraria e morale. Un libro polifonico e stilisticamente curato, con una lingua limpida, incisiva e al tempo stesso capace di abbandonarsi al “messaggio celeste” (p.9) della natura, di scendere in profondità, anzi, come scrive lo stesso Consolo, di “verticalizzare il linguaggio, spostarlo verso la zona della poesia”. Il romanzo inizia con una data fortemente simbolica: 8 marzo mattino. Si tratta di una lettera, la prima di un lungo carteggio: l’autrice è una delicatissima adolescente che ricorda Katherine Mansfield nel nome e, soprattutto, nel sentire, nel suo trovare da subito, più o meno consapevolmente, la propria religione e il proprio mondo nella scrittura. 8 marzo mattino: di quale anno? Non importa. Il tempo del romanzo si dilata: l’adolescente scrive alla se stessa che sarà, domanda alla donna se potrà finalmente, un giorno, “riempire fino in fondo ogni spazio” o se resterà per sempre “un angolo di vuoto” (p.7). Ed ecco che la donna risponde: non vuole illudere, non vuole nascondere alla se stessa del passato le ferite “che gocciano per molto tempo” (p.17), vuole che la ragazza impari ad appartenere, a “rimanere diversa” (p. 23).

La ragazza di ieri e la donna di oggi appaiono al lettore racchiuse in una stanza ideale, riunite in un miracoloso dialogo, ma il romanzo non è privo di un aggancio con l’esterno: possiamo immaginare che nella stanza ci sia una grande finestra, una di quelle finestre che tanto amava anche un’altra adolescente, Emma Bovary; lo sguardo delle due donne si posa quindi fuori: non è solo uno sguardo sognante, è anche lo sguardo di chi contempla il mondo, un mondo che si lascia cogliere nel momento in cui la primavera si schiude, fino a quando matura, alle soglie dell’estate. Una primavera e un’estate di qualsiasi anno, una primavera e un’estate di una vita che fiorisce e si trasforma, come ogni vita in ogni tempo.

La metamorfosi è talvolta espiazione e percorso di salvezza: “si resta diversi”, si deve attraversare il dolore, perdere il sé per poi riconoscersi (o almeno ricomporre qualche frammento). Tra i tanti riferimenti letterari possibili, l’immagine della farfalla non può che ricordare Guido Gozzano, l’entomologo, chiuso nel suo eremo, dove silenziose e in attesa dormono le crisalidi. Nel romanzo di Chiara Pellegrini, l’attesa è sicuramente un tema chiave, come è naturale in pagine scritte in gran parte da un’adolescente; l’adolescenza è l’età dell’attesa ed anche l’età in cui la vita ti si offre come un ventaglio di infinite possibilità: attesa, quindi, ma anche scelta. Storia di farfalle e altre metamorfosi non è un romanzo crepuscolare: è più forte, alla fine, la voglia di bruciare nella luce, dopo aver passato la vita a evitare di scegliere. Quando la metamorfosi avviene, quando Caterina si scopre farfalla, porta impresso, come l’Acherontia di Gozzano, un segno spaventoso, qualcosa a cui non è riuscita a dar nome per molto tempo, un trauma che ha condizionato, sotterraneo e prepotente, tutta la sua esistenza. Se le voci maschili sono evanescenti – l’amore non goduto della giovinezza o l’amore della maturità- c’è invece personaggio maschile che, pur restando sullo sfondo, domina l’intera esistenza di Caterina: è la vera ferita, il dolore rimosso, l’incarnazione del male che non ha voce ma ha “mani”, “mani calde”, odiose e brutali, il cui ricordo ossessiona Caterina. Il trauma avviene quando Caterina sta per sbocciare. La farà sentire “fuori posto” (p.7) nella sua primavera e nella sua estate. Le renderà indispensabile trovare una strada per “restare diversa”, per fiorire, nonostante tutto. Un varco per Caterina sono le piante e i fiori che lei ama. Le piante non possono muoversi, non possono parlare. Le piante le somigliano ancora di più dopo quel trauma che l’ha inchiodata e le ha tolto la voce. Eppure, lei continua a fiorire, consapevole di quanto dolore richieda il mutare forma. “Fiorire non è uno scherzo”, scrive Caterina adulta (cfr. lettera del 23 marzo, notte di stelle):

Fiorire non è uno scherzo. È necessario spaccarsi ed è doloroso. La gemma riposa nella fibra del ramo tutto l’inverno. Quando primavera entra e, come sappiamo, non bussa e ha passo sicuro, la gemma erompe dalla scorza ed è una spaccatura. Le fibre si sono tese allo spasimo dentro il ramo per far posto all’ingrossarsi di quel grumo composito e duro di vita e quando questo è gonfio abbastanza, ecco che la sua eruzione lacera e apre il varco. Primavera entra e non bussa e ha passo sicuro. Ieri il verde non c’era, oggi vibra a ogni soffio sulle punte dei rami. Ma questa esplosione, che sembra avvenuta stanotte, chiamata dal silenzio delle stelle, ha impiegato mesi per aggregarsi, comporsi, strutturarsi, e lo ha fatto a spese delle fibre dell’albero, piegate, ritorte, compresse e infine strappate, lo ha fatto succhiando, mungendo, spremendo linfe e umori vitali alla pianta tutta. Tutto quel che cresce fa male a tutto ciò che racchiude. Tutto ciò che cresce lacera tutto ciò che lo vorrebbe avvolgere e contenere. Crescere e racchiudere, coraggio e paura. Il movimento della vita. La vita e la morte. Coraggio e paura.

Anche la letteratura, come la natura, è cosa viva. In un universo frammentato e sfuggente, l’io si perde in un gioco di specchi: Caterina si ritrova nel mondo vegetale e nei libri, in altre voci di donne, poetesse, scrittrici o protagoniste di pagine narrative. Tra le molte storie citate, nel romanzo si ricorda il racconto Rose rosse, della siciliana Maria Messina, una storia dura e violenta: “sostieni anche tu, se ne hai coraggio, che è la solita scrittura femminea”, afferma la voce narrante, parlando a se stessa, ma rivolgendosi in realtà a un uditorio più vasto, al pubblico che ancora dibatte sull’annosa questione se esista o meno una scrittura femminile. A tale riguardo, l’autrice di Farfalle e altre metamorfosi rivendica l’esistenza di quello che Sandra Petrignani (Laterza, 2019) chiama “lessico femminile”: una lingua diversa, espressione di un “pensiero naturalmente autocritico” e spesso “inascoltato” (ibidem, p.7), una lingua che sa trattare con leggerezza temi pesanti, proprio come avviene per Maria Messina e per la stessa Pellegrini.

Caterina diviene farfalla e, in parte, si libera e si riconosce; non smette di confrontarsi con il dolore che l’ha resa quello che è, ma trova una strategia per “restare diversa”. Come Marcel, alla fine della Recherche, si scopre scrittore, così Caterina comprende che ciò che l’aspetta, da sempre, è “un volo di parole” (p. 217). Le due donne, la ragazzina e la donna matura, trovano un varco e balzano fuori, fuggono, in un luogo dove non è necessario scegliere. E passare quel varco “è rimanere diversi”, “trasfigurare” (cfr. p. 21):

No, non è una contraddizione: rimanere diversi è un trasfigurare. Sei ancora tu, ma indossi una veste nuova, come dopo una risurrezione, una volta che la pietra del sepolcro è rotolata di lato e si esce dalla tomba come dal grembo materno, scintillanti di luce e rinati.

Scrivere non imprigiona, scrivere è “restare diversi”: Caterina, come Katherine, trova nella scrittura la sua religione, il suo mondo, la sua vita.

Silvia Morotti

</div>

<div class="item-footer">Published 10:10 • 7 days ago</div>

</div>

</article>

## Monday, 06\. April 2020

<article class="item">

#### [schema.org News](http://blog.schema.org)

### [COVID-19 schema for CDC hospital reporting](http://blog.schema.org/2020/04/covid-19-schema-for-cdc-hospital.html)

<div class="item-body">

<div class="item-snippet">The COVID-19 pandemic requires various medical and government authorities to aggregate data about available resources from a wide range of medical facilities. Clearly standard schemas for this structured data can be very useful.The Centers for Disease Control (CDC) in the U.S. defined a set of data fields to facilitate exchange of this data. We are introducing a Schema.org representa</div>

<div class="item-content item-summary">The COVID-19 pandemic requires various medical and government authorities to aggregate data about available resources from a wide range of medical facilities. Clearly standard schemas for this structured data can be very useful.  

The Centers for Disease Control (CDC) in the U.S. defined a set of data fields to facilitate exchange of this data. We are introducing a Schema.org representation of these data fields.   

The purpose of this schema definition is to provide a standards-based representation that can be used to encode and exchange records that correspond to the CDC format, with usage within the U.S. primarily in mind. While the existence of this schema may provide additional implementation options for those working with US hospital reporting data about COVID-19, please refer to the CDC and other appropriate bodies for authoritative guidance on the latest reporting workflows and data formats.  

Depending upon context, any of the formats and standards that work with Schema.org may be applicable for encoding this data, including the Microdata, RDFa and JSON-LD data formats, as well as related technologies such as W3C SPARQL for data query. JSON-LD is in most cases likely to be the most appropriate format. There is no assumption that data encoded using this schema should necessarily be published on the public Web, nor that it would be used by search engines.  

We will continue to improve this vocabulary in the light of feedback, and welcome suggestions for improvements and additions particularly from US healthcare organizations who are using it. This CDC-based vocabulary follows other recent changes we have made to Schema.org. For details of recent changes see our release notes and our previous post announcing the SpecialAnnouncement markup, which is now supported at both Bing (blog, docs) and Google (blog, docs). As the global response to COVID-19 evolves we will do our best to improve schema.org's vocabularies to represent the changes that Coronavirus is bringing to society, and to assist those using structured data to help with the response.  

</div>

<div class="item-footer">Published 16:42 • 6 months ago | Updated Monday, 06\. April 2020 16:42 • 6 months ago</div>

</div>

</article>

## Tuesday, 17\. March 2020

<article class="item">

#### [schema.org News](http://blog.schema.org)

### [Schema for Coronavirus special announcements, Covid-19 Testing Facilities and more](http://blog.schema.org/2020/03/schema-for-coronavirus-special.html)

<div class="item-body">

<div class="item-snippet">The COVID-19 pandemic is causing a large number of “Special Announcements” pertaining to changes in schedules and other aspects of everyday life. This includes not just closure of facilities and rescheduling of events but also new availability of medical facilities such as testing centers.We have today published Schema.org 7.0, which includes fast-tracked new vocabulary to assist the global respons</div>

<div class="item-content item-summary">The COVID-19 pandemic is causing a large number of “Special Announcements” pertaining to changes in schedules and other aspects of everyday life. This includes not just closure of facilities and rescheduling of events but also new availability of medical facilities such as testing centers.  

We have today published Schema.org 7.0, which includes fast-tracked new vocabulary to assist the global response to the Coronavirus outbreak.  

It includes a "SpecialAnnouncement" type that provides for simple date-stamped textual updates, as well as markup to associate the announcement with a situation (such as the Coronavirus pandemic), and to indicate URLs for various kinds of update such a school closures, public transport closures, quarantine guidelines, travel bans, and information about getting tested.    

Many new testing facilities are being rapidly established worldwide, to test for COVID-19\. Schema.org now has a CovidTestingFacility type to represent these, regardless of whether they are part of long-established medical facilities or temporary adaptations to the emergency.  

We are also making improvements to other areas of Schema.org to help with the worldwide migration to working online and working from home, for example by helping event organizers indicate when an event has moved from having a physical location to being conducted online, and whether the event's "eventAttendanceMode" is online, offlline or mixed.   

We will continue to improve this vocabulary in the light of feedback (github; doc), and welcome suggestions for improvements and additions particularly from organizations who are publishing such updates.   

Dan Brickley, R.V.Guha, Google.  
Tom Marsh, Microsoft.</div>

<div class="item-footer">Published 03:16 • 7 months ago | Updated Tuesday, 17\. March 2020 03:23 • 7 months ago</div>

</div>

</article>

## Monday, 24\. February 2020

<article class="item">

#### [schema.org News](http://blog.schema.org)

### [Schema.org 6.0](http://blog.schema.org/2020/01/schemaorg-60.html)

<div class="item-body">

<div class="item-snippet">Schema.org version 6.0 has been released. See the release notes for full details.  As always, the release notes have full details and links (including previous releases e.g. 5.0 and 4.0).We are now aiming to release updated schemas on an approximately monthly basis (with longer gaps around vacation periods). Typically, new terms are first added to our "Pending" area to give time</div>

<div class="item-content item-summary">Schema.org version 6.0 has been released. See the release notes for full details.  As always, the release notes have full details and links (including previous releases e.g. 5.0 and 4.0).  

We are now aiming to release updated schemas on an approximately monthly basis (with longer gaps around vacation periods). Typically, new terms are first added to our "Pending" area to give time for the definitions to benefit from implementation experience before they are added to the "core" of Schema.org. As always, many thanks to everyone who has contributed to this release of Schema.org.  

--  
Dan Brickley, for Schema.org.  

</div>

<div class="item-footer">Published 16:13 • 8 months ago | Updated Monday, 24\. February 2020 19:54 • 7 months ago</div>

</div>

</article>

## Thursday, 01\. August 2019

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Introduction to Statistics With Data Packages and Gonum](http://okfnlabs.org/blog/2019/08/01/intro-statistics-datapackage-gonum.html)

<div class="item-body">

<div class="item-snippet">After 6 years at Google, Daniel Fireman is currently a Ph.D. student, professor and activist for government transparency and accountability in the Northeast of Brazil. He was one of the 2017’s Frictionless Data Tool Fund grantees and implemented the core Frictionless Data specification in the Go programming language: datapackage and tableschema, which he still maintains. You can read more about thi</div>

<div class="item-content item-summary">

After 6 years at Google, Daniel Fireman is currently a Ph.D. student, professor and activist for government transparency and accountability in the Northeast of Brazil. He was one of the 2017’s Frictionless Data Tool Fund grantees and implemented the core Frictionless Data specification in the Go programming language: datapackage and tableschema, which he still maintains. You can read more about this in his grantee profile.

Since its first release in 2017, we’ve been improving datapackage and tableschema packages. Besides fixing bugs, we tried to make it easier to use data packages together with statistical/plotting libraries like Gonum. This post shows an example of such usage and was inspired in this post, from Sebastian Binet.

Our goal in this tutorial is to load a data package from the web and use Gonum to calculate some basic statistics.

Go, Data Packages & Gonum

datapackage is “a package for working with Data Packages“. A Data Package consists of:

*   Metadata that describes the structure and contents of the package
*   Resources such as data files that form the contents of the package

Gonum is “a set of packages designed to make writing numeric and scientific algorithms productive, performant and scalable.”

Before being able to use `datapackage` and `Gonum`, we need to install Go. We can download and install the `Go` toolchain for a variety of platforms and operating systems from golang.org/dl. This post assumes the installation of version 11 or newer.

After installing Go, the runtime will download `Gonum`, `datapackage` and all its dependencies as part of running the go script.

Reading Datapackage

In this post, we are using a Tabular Data Package containing the periodic table. The package descriptor (datapackage.json) and contents (data.csv) are stored on GitHub. This dataset includes the atomic number, symbol, element name, atomic mass, and the metallicity of the element. Let’s start by taking a quick look at the header and the first rows.

    // file: stats.go

    package main

    import (
        "fmt"

        "github.com/frictionlessdata/datapackage-go/datapackage"
    )

    func main() {
        pkg, err := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
        if err != nil {
            panic(err)
        }
        res := pkg.GetResource("data")
        table, err := res.ReadAll()
        if err != nil {
            panic(err)
        }
        for i := 0; i < 4; i++ {
            fmt.Println(table[i])
        }
    }

Gonum and statistics

Gonum provides many statistical functions. Let’s use it to calculate the mean, median, standard deviation and variance of the atomic masses.

    // file: stats.go

    package main

    import (
            "fmt"
            "math"
            "sort"

            "github.com/frictionlessdata/datapackage-go/datapackage"
            "github.com/frictionlessdata/tableschema-go/csv"
            "gonum.org/v1/gonum/stat"
    )

    func main() {
            pkg, err := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
            if err != nil {
                    panic(err)
            }
            var masses []float64
            res := pkg.GetResource("data")
            if err := res.CastColumn("atomic mass", &masses, csv.LoadHeaders()); err != nil {
                    panic(err)
            }
            fmt.Printf("data: %v\n", masses)

            sort.Float64s(masses)
            fmt.Printf("data: %v (sorted)\n", masses)

            // computes the weighted mean of the dataset.
            // we don't have any weights (ie, all weights are 1)
            // so we just pass a nil slice.
            mean := stat.Mean(masses, nil)

            // computes the median of the dataset.
            // here as well, we pass a nil slice as weights.
            median := stat.Quantile(0.5, stat.Empirical, masses, nil)

            variance := stat.Variance(masses, nil)
            stddev := math.Sqrt(variance)

            fmt.Printf("mean=     %v\n", mean)
            fmt.Printf("median=   %v\n", median)
            fmt.Printf("variance= %v\n", variance)
            fmt.Printf("std-dev=  %v\n", stddev)
    }

The program above performs some basic statistical operations on our dataset:

    $> go run stats.go
    ... dependency download logs ...
    data: [1.00794 4.002602 6.941 9.012182 10.811 12.0107 14.0067 15.9994 18.9984032 20.1797 22.98976928 24.305 26.9815386 28.0855 30.973762 32.065 35.453 39.948 39.0983 40.078 44.955912 47.867 50.9415 51.9961 54.938045 55.845 58.933195 58.6934 63.546 65.38 69.723 72.64 74.9216 78.96 79.904 83.798 85.4678 87.62 88.90585 91.224 92.90638 95.96 98 101.07 102.9055 106.42 107.8682 112.411 114.818 118.71 121.76 127.6 126.90447 131.293 132.9054519 137.327 138.90547 140.116 140.90765 144.242 145 150.36 151.964 157.25 158.92535 162.5 164.93032 167.259 168.93421 173.054 174.9668 178.49 180.94788 183.84 186.207 190.23 192.217 195.084 196.966569 200.59 204.3833 207.2 208.9804 209 210 222 223 226 227 232.03806 231.03588 238.02891 237 244 243 247 247 251 252 257 258 259 262 267 268 271 272 270 276 281 280 285 284 289 288 293 294 294]
    data: [1.00794 4.002602 6.941 9.012182 10.811 12.0107 14.0067 15.9994 18.9984032 20.1797 22.98976928 24.305 26.9815386 28.0855 30.973762 32.065 35.453 39.0983 39.948 40.078 44.955912 47.867 50.9415 51.9961 54.938045 55.845 58.6934 58.933195 63.546 65.38 69.723 72.64 74.9216 78.96 79.904 83.798 85.4678 87.62 88.90585 91.224 92.90638 95.96 98 101.07 102.9055 106.42 107.8682 112.411 114.818 118.71 121.76 126.90447 127.6 131.293 132.9054519 137.327 138.90547 140.116 140.90765 144.242 145 150.36 151.964 157.25 158.92535 162.5 164.93032 167.259 168.93421 173.054 174.9668 178.49 180.94788 183.84 186.207 190.23 192.217 195.084 196.966569 200.59 204.3833 207.2 208.9804 209 210 222 223 226 227 231.03588 232.03806 237 238.02891 243 244 247 247 251 252 257 258 259 262 267 268 270 271 272 276 280 281 284 285 288 289 293 294 294] (sorted)
    mean=     146.43746355915252
    median=   140.90765
    variance= 8026.634755570227
    std-dev=  89.59148818704948

Thanks for reading!

We welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-go repository.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Tuesday, 02\. April 2019

<article class="item">

#### [schema.org News](http://blog.schema.org)

### [Schema.org 3.5: Simpler extension model, projects, grants and funding schemas, and new terms for describing educational and occupational credentials](http://blog.schema.org/2019/04/schemaorg-35-simpler-extension-model.html)

<div class="item-body">

<div class="item-snippet">Schema.org version 3.5 has been released. This release moves a number of terms from the experimental "Pending" area into the Schema.org core. It also simplifies and clarifies the Schema.org extension model, reducing our emphasis on using named subdomains for topical groups of schemas. New terms introduced in Pending area include improvements for describing projects, grants and funding agencies; fo</div>

<div class="item-content item-summary">Schema.org version 3.5 has been released. This release moves a number of terms from the experimental "Pending" area into the Schema.org core. It also simplifies and clarifies the Schema.org extension model, reducing our emphasis on using named subdomains for topical groups of schemas. New terms introduced in Pending area include improvements for describing projects, grants and funding agencies; for describing open-ended date ranges (e.g. datasets); and a substantial vocabulary for Educational and Occupational Credentials. Many thanks to all who contributed!</div>

<div class="item-footer">Published 16:19 • over a year ago | Updated Tuesday, 02\. April 2019 16:19 • over a year ago</div>

</div>

</article>

## Thursday, 18\. October 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Announcing datapackage-pipelines version 2.0](http://okfnlabs.org/blog/2018/10/18/announcing-datapackage-pipelines-v2.html)

<div class="item-body">

<div class="item-snippet">Today we’re releasing a major version for datapackage-pipelines, version 2.0.0. This new version marks a big step forward in realizing the Data Factory concept and framework. We integrated datapackage-pipelines with its younger sister dataflows, and created a set of common building blocks you can now use interchangeably between the two frameworks. ♦ figure 1: diagram showing the relationship bet</div>

<div class="item-content item-summary">

Today we’re releasing a major version for datapackage-pipelines, version 2.0.0.

This new version marks a big step forward in realizing the Data Factory concept and framework. We integrated datapackage-pipelines with its younger sister dataflows, and created a set of common building blocks you can now use interchangeably between the two frameworks.

♦  
figure 1: diagram showing the relationship between dataflows and datapackage-pipelines

It’s now possible to bootstrap and develop flows using dataflows, and then run these flows as-is on a datapackage-pipelines server - or effortlessly convert them to the declarative yaml syntax.

Install datapackage-pipelines using `pip`:

    pip install datapackage-pipelines

What Changed? New Low-level API and stdout Redirect

One big change (and a long time request) is that processors are now allowed to print from inside their processing code, without interfering with the correct operation of the pipeline. All prints are automatically converted to logging.info(…) calls.This behaviour is enabled when using the new low-level API. The main change we’ve introduced is that ingest() is now a context manager. This means that you now should run:

    # New style for ingest and spew
    with ingest() as ctx:
     # Do stuff with datapackage and resource_iterator
     spew(ctx.datapackage,
     ctx.resource_iterator,
     ctx.stats)

Backward compatibility is maintained for the old way of using ingest(), so you don’t have to update all your code immediately.

    # This still works, but won’t handle print()s
    parameters, datapackage, resource_iterator = ingest()
    spew(datapackage, resource_iterator)

Dataflows integration

There’s a new integration with dataflows which allows running Flows directly from the `pipeline-spec.yaml` file. You can integrate dataflows within pipeline specs using the `flow` attribute instead of `run`. For example, given the following flow file, saved under `my-flow.py`:

    from dataflows import Flow, dump_to_path, load, update_package
    ​
    def flow(parameters, datapackage, resources, stats):
      stats[‘multiplied_fields’] = 0
     ​
      def multiply(field, n):
        def step(row):
          row[field] = row[field] * n
          stats[‘multiplied_fields’] += 1
          return step
    ​
        return Flow(update_package(name=’my-datapackage’),
                    load((datapackage, resources),
                    multiply(‘my-field’, 2))

And a `pipeline-spec.yaml` in the same directory:

    my-flow:
     pipeline:
       — run: load_resource
     parameters:
       url: example.com/my-datapackage/datapackage.json
       resource: my-resource
         — flow: my-flow
         — run: dump.to_path

You can run the pipeline using `dpp run my-flow`.

If you want to wrap a flow inside a processor, you can use the `spew_flow` helper function:

    from dataflows import Flow
    from datapackage_pipelines.wrapper import ingest
    from datapackage_pipelines.utilities.flow_utils import spew_flow
    ​
    def flow(parameters):
     return Flow(
     # Flow processing comes here
     )
    ​
    ​
    if __name__ == ‘__main__’:
     with ingest() as ctx:
     spew_flow(flow(ctx.parameters), ctx)

Standard Processor Refactoring

We refactored all standard processors to use their counterparts from dataflows, thus removing code duplication and allowing us to move forward quicker. As a result, we’re also introducing a couple of new processors:

*   `load` - Loads and streams a new resource (or resources) into the data package. It’s based on the dataflows processor with the same name, so it supports loading from local files, remote URL, data packages, locations in environment variables etc. For more information, consult the dataflows documentation.

*   `printer` - Smart printing processor for displaying the contents of the stream - comes in handy for development or monitoring a pipeline.It will not print all rows, but an logarithmically sparse sample - in other words, it will print rows 1-20, 100-110, 1000-1010 etc. It also prints the last 10 rows of the dataset.

Deprecations

We are deprecating a few processors — you can still use them as usual but they will be removed in the next major version (3.0):

*   `add_metadata` - was renamed to `update_package` for consistency
*   `add_resource` and `stream_remote_resources` - are being replaced by the `load`
*   `dump.to_path`, `dump.to_zip`, `dump.to_sql` - are being deprecated - you should use `dump_to_path`, `dump_to_zip` and `dump_to_sql` instead. Note that `dump_to_path` and `dump_to_zip` lack some features that exist in the current processors — for example, custom file formatters and non-tabular file support. We might introduce some of that functionality into the new processors as well in the next versions - in the meantime, please let us know what you think about these features and how badly you need them.

The Road Ahead

In the next versions we’re planning to further the integration of dataflows and datapackage-pipelines. We’re going to work on streamlining development and deployment as well as taking care of naming and documentation to harmonize all aspects of the dataflows ecosystem. We’re also working on de-composing datapackage-pipelines into smaller, self contained components. In this version we took apart the standard processor code and some supporting libraries (e.g. `kvstore`) and delegated it to external libraries.

Links and References

*   Read more on datapackage-pipelines here: github.com/frictionlessdata/datapackage-pipelines
*   Read more on dataflows here: github.com/datahq/dataflows
*   Read more on Data Factory here: okfnlabs.org/blog/2018/08/29/data-factory-data-flows-introduction.html

Contributors

Thanks to Ori Hoch for contributing code and other invaluable assistance with this release.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Thursday, 30\. August 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Data Factory & DataFlows - Tutorial](http://okfnlabs.org/blog/2018/08/30/data-factory-data-flows-tutorial.html)

<div class="item-body">

<div class="item-snippet">Data Factory is an open framework for building and running lightweight data processing workflows quickly and easily. We recommend reading this introductory blogpost to gain a better understanding of underlying Data Factory concepts before diving into the tutorial below. Learn how to write your own processing flows Let’s start with the traditional ‘hello, world’ example: from dataflows import F</div>

<div class="item-content item-summary">

Data Factory is an open framework for building and running lightweight data processing workflows quickly and easily. We recommend reading this introductory blogpost to gain a better understanding of underlying Data Factory concepts before diving into the tutorial below.

Learn how to write your own processing flows

Let’s start with the traditional ‘hello, world’ example:

    from dataflows import Flow

    data = [
      {'data': 'Hello'},
      {'data': 'World'}
    ]

    def lowerData(row):
    	row['data'] = row['data'].lower()

    f = Flow(
          data,
          lowerData
    )
    data, *_ = f.results()

    print(data)

    # -->
    # [
    #   [
    #     {'data': 'hello'},
    #     {'data': 'world'}
    #   ]
    # ]

This very simple flow takes a list of `dict`s and applies a row processing function on each one of them.

We can load data from a file instead:

    from dataflows import Flow, load

    # beatles.csv:
    # name,instrument
    # john,guitar
    # paul,bass
    # george,guitar
    # ringo,drums

    def titleName(row):
        row['name'] = row['name'].title()

    f = Flow(
          load('beatles.csv'),
          titleName
    )
    data, *_ = f.results()

    print(data)

    # -->
    # [
    #   [
    #     {'name': 'John', 'instrument': 'guitar'},
    #     {'name': 'Paul', 'instrument': 'bass'},
    #     {'name': 'George', 'instrument': 'guitar'},
    #     {'name': 'Ringo', 'instrument': 'drums'}
    #   ]
    # ]

The source file can be a CSV file, an Excel file or a JSON file. You can use a local file name or a URL for a file hosted somewhere on the web.

Data sources can be generators and not just lists or files. Let’s take as an example a very simple scraper:

    from dataflows import Flow

    from xml.etree import ElementTree
    from urllib.request import urlopen

    # Get from Wikipedia the population count for each country
    def country_population():
        # Read the Wikipedia page and parse it using etree
        page = urlopen('en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population').read()
        tree = ElementTree.fromstring(page)
        # Iterate on all tables, rows and cells
        for table in tree.findall('.//table'):
            if 'wikitable' in table.attrib.get('class', ''):
                for row in table.findall('tr'):
                    cells = row.findall('td')
                    if len(cells) > 3:
                        # If a matching row is found...
                        name = cells[1].find('.//a').attrib.get('title')
                        population = cells[2].text
                        # ... yield a row with the information
                        yield dict(
                            name=name,
                            population=population
                        )

    f = Flow(
          country_population(),
    )
    data, *_ = f.results()

    print(data)
    # --->
    # [
    #   [
    #     {'name': 'China', 'population': '1,391,090,000'},
    #     {'name': 'India', 'population': '1,332,140,000'},
    #     {'name': 'United States', 'population': '327,187,000'},
    #     {'name': 'Indonesia', 'population': '261,890,900'},
    #     ...
    #   ]
    # ]

This is nice, but we do prefer the numbers to be actual numbers and not strings.

In order to do that, let’s simply define their type to be numeric:

    from dataflows import Flow, set_type

    def country_population():
        # same as before
    	...

    f = Flow(
    	country_population(),
        set_type('population', type='number', groupChar=',')
    )
    data, *_ = f.results()

    print(data)
    # -->
    # [
    #   [
    #     {'name': 'China', 'population': Decimal('1391090000')},
    #     {'name': 'India', 'population': Decimal('1332140000')},
    #     {'name': 'United States', 'population': Decimal('327187000')},
    #     {'name': 'Indonesia', 'population': Decimal('261890900')},
    #     ...
    #   ]
    # ]

Data is automatically converted to the correct native Python type.

Apart from data-types, it’s also possible to set other constraints to the data. If the data fails validation (or does not fit the assigned data-type) an exception will be thrown - making this method highly effective for validating data and ensuring data quality.

What about large data files? In the above examples, the results are loaded into memory, which is not always preferable or acceptable. In many cases, we’d like to store the results directly onto a hard drive - without having the machine’s RAM limit in any way the amount of data we can process.

We do it by using dump processors:

    from dataflows import Flow, set_type, dump_to_path

    def country_population():
        # same as before
    	...

    f = Flow(
    	country_population(),
        set_type('population', type='number', groupChar=','),
        dump_to_path('country_population')
    )
    *_ = f.process()

Running this code will create a local directory called `county_population`, containing two files:

    ├── country_population
    │   ├── datapackage.json
    │   └── res_1.csv

The CSV file - `res_1.csv` - is where the data is stored. The `datapackage.json` file is a metadata file, holding information about the data, including its schema.

We can now open the CSV file with any spreadsheet program or code library supporting the CSV format - or using one of the data package libraries out there, like so:

    from datapackage import Package
    pkg = Package('country_population/res_1.csv')
    it = pkg.resources[0].iter(keyed=True)
    print(next(it))
    # prints:
    # {'name': 'China', 'population': Decimal('1391110000')}

Note how using the data package meta-data, data-types are restored and there’s no need to ‘re-parse’ the data. This also works with other types too, such as dates, booleans and even `list`s and `dict`s.

So far we’ve seen how to load data, process it row by row, and then inspect the results or store them in a data package.

Let’s see how we can do more complex processing by manipulating the entire data stream:

    from dataflows import Flow, set_type, dump_to_path

    # Generate all triplets (a,b,c) so that 1 <= a <= b < c <= 20
    def all_triplets():
        for a in range(1, 20):
            for b in range(a, 20):
                for c in range(b+1, 21):
                    yield dict(a=a, b=b, c=c)

    # Yield row only if a^2 + b^2 == c^1
    def filter_pythagorean_triplets(rows):
        for row in rows:
            if row['a']**2 + row['b']**2 == row['c']**2:
                yield row

    f = Flow(
        all_triplets(),
        set_type('a', type='integer'),
        set_type('b', type='integer'),
        set_type('c', type='integer'),
        filter_pythagorean_triplets,
        dump_to_path('pythagorean_triplets')
    )
    _ = f.process()

    # -->
    # pythagorean_triplets/res_1.csv contains:
    # a,b,c
    # 3,4,5
    # 5,12,13
    # 6,8,10
    # 8,15,17
    # 9,12,15
    # 12,16,20

The `filter_pythagorean_triplets` function takes an iterator of rows, and yields only the ones that pass its condition.

The flow framework knows whether a function is meant to handle a single row or a row iterator based on its parameters:

*   if it accepts a single `row` parameter, then it’s a row processor.
*   if it accepts a single `rows` parameter, then it’s a rows processor.
*   if it accepts a single `package` parameter, then it’s a package processor.

Let’s see a few examples of what we can do with a package processors.

First, let’s add a field to the data:

    from dataflows import Flow, load, dump_to_path

    def add_is_guitarist_column_to_schema(package):
    	# Add a new field to the first resource
        package.pkg.resources[0]
                   .descriptor['schema']['fields']
                   .append(dict(
                name='is_guitarist',
                type='boolean'
        ))
        # Must yield the modified datapackage
        yield package.pkg
        # And its resources
        yield from package

    def add_is_guitarist_column(row):
    	row['is_guitarist'] = row['instrument'] == 'guitar'
        return row

    f = Flow(
        # Same one as above
        load('beatles.csv'),
        add_is_guitarist_column_to_schema,
        add_is_guitarist_column,
        dump_to_path('beatles_guitarists')
    )
    _ = f.process()

In this example we create two steps - one for adding the new field (`is_guitarist`) to the schema and another step to modify the actual data.

We can combine the two into one step:

    from dataflows import Flow, load, dump_to_path

    def add_is_guitarist_column(package):

        # Add a new field to the first resource
        package.pkg.resources[0].descriptor['schema']['fields'].append(dict(
            name='is_guitarist',
            type='boolean'
        ))
        # Must yield the modified datapackage
        yield package.pkg

        # Now iterate on all resources
        resources = iter(package)
        # Take the first resource
        beatles = next(resources)

        # And yield it with with the modification
        def f(row):
            row['is_guitarist'] = row['instrument'] == 'guitar'
            return row

        yield map(f, beatles)

    f = Flow(
        # Same one as above
        load('beatles.csv'),
        add_is_guitarist_column,
        dump_to_path('beatles_guitarists')
    )
    _ = f.process()

The contract for the `package` processing function is simple:

First modify `package.pkg` (which is a `Package` instance) and yield it.

Then, yield any resources that should exist on the output, with or without modifications.

In the next example we’re removing an entire resource in a package processor - this next one filters the list of Academy Award nominees to those who won both the Oscar and an Emmy award:

        from dataflows import Flow, load, dump_to_path

        def find_double_winners(package):

            # Remove the emmies resource -
            #    we're going to consume it now
            package.pkg.remove_resource('emmies')
            # Must yield the modified datapackage
            yield package.pkg

            # Now iterate on all resources
            resources = iter(package)

            # Emmies is the first -
            # read all its data and create a set of winner names
            emmy = next(resources)
            emmy_winners = set(
                map(lambda x: x['nominee'],
                    filter(lambda x: x['winner'],
                           emmy))
            )

            # Oscars are next -
            # filter rows based on the emmy winner set
            academy = next(resources)
            yield filter(lambda row: (row['Winner'] and
                                      row['Name'] in emmy_winners),
                         academy)

        f = Flow(
            # Emmy award nominees and winners
            load('emmy.csv', name='emmies'),
            # Academy award nominees and winners
            load('academy.csv', encoding='utf8', name='oscars'),
            find_double_winners,
            dump_to_path('double_winners')
        )
        _ = f.process()

    # -->
    # double_winners/academy.csv contains:
    # 1931/1932,5,Actress,1,Helen Hayes,The Sin of Madelon Claudet
    # 1932/1933,6,Actress,1,Katharine Hepburn,Morning Glory
    # 1935,8,Actress,1,Bette Davis,Dangerous
    # 1938,11,Actress,1,Bette Davis,Jezebel
    # ...

Builtin Processors

DataFlows comes with a few built-in processors which do most of the heavy lifting in many common scenarios, leaving you to implement only the minimum code that is specific to your specific problem.

A complete list, which also includes an API reference for each one of them, can be found in the DataFlows Built-in Processors page.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Wednesday, 29\. August 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Data Factory & DataFlows - An Introduction](http://okfnlabs.org/blog/2018/08/29/data-factory-data-flows-introduction.html)

<div class="item-body">

<div class="item-snippet">Today I’d like to introduce a new library we’ve been working on - dataflows. DataFlows is a part of a larger conceptual framework for data processing. We call it ‘Data Factory’ - an open framework for building and running lightweight data processing workflows quickly and easily. LAMP for data wrangling! Most of you already know what Data Packages are. In short, it is a portable format for packagi</div>

<div class="item-content item-summary">

Today I’d like to introduce a new library we’ve been working on - `dataflows`. DataFlows is a part of a larger conceptual framework for data processing.

We call it ‘Data Factory’ - an open framework for building and running lightweight data processing workflows quickly and easily. LAMP for data wrangling!

Most of you already know what Data Packages are. In short, it is a portable format for packaging different resources (tabular or otherwise) in a standard way that takes care of most interoperability problems (e.g. “what’s the character encoding of the file?” or “what is the data type for this column?” or “which date format are they using?”). It also provides rich and flexible metadata, which users can then use to understand what the data is about (take a look at frictionlessdata.io to learn more!).

Data Factory complements the Data Package concepts by adding dynamics to the mix.

While Data Packages are a great solution for describing data sets, these data sets are always static - located in one place. Data Factory is all about transforming Data Packages - modifying their data or meta-data and transmitting them from one location to another.

Data Factory defines standard interfaces for building processors - software modules for mutating a Data Package - and protocols for streaming the contents of a Data Package for efficient processing.

Philosophy and Goals

Data Factory is more pattern/convention than library.

An analogy is with web frameworks. Web frameworks were more about a core pattern plus a set of ready to use components than a library themselves. For example, python frameworks were built around WSGI e.g. Pylons, Flask etc. Or consider ExpressJS for Node.

In this sense these frameworks were about convention over configuration. They attempted to decrease the number of decisions that a developer using the framework is required to make without necessarily losing flexibility.

Like web frameworks, Data Factory uses convention over configuration with the aim of decreasing the number of decisions that a data developer is required to make without necessarily losing flexibility.

By following a standard scheme, developers are able to use a large and growing library of existing, reusable processors. This also increases readability and maintainability of data processing code.

Our focus is on:

*   Small to medium sized data (KBs to GBs)
*   Desktop wrangling - people who start on their desktop
*   Easy transition from desktop to “cloud”
*   Heterogeneous data sources
*   Process using basic building blocks that are extensible
*   Less technical audience
*   Limited resources - limit on memory, CPU, etc.

What are we not?

*   Big data processing and machine learning. e.g. if you want to wrangle TBs of data in a distributed setup or want to train a machine learning model with GBs of data, you probably don’t want this.
*   Processing real-time event data.
*   Technical know-how is needed: we aren’t a fancy ETL UI – you probably need a bit of technical sophistication

Architecture

This new framework is built on the foundations of the Frictionless Data project - both conceptually as well as technically. This project provided us the definition of Data Packages and the software to read and write these packages.

> On top of this Frictionless Data basis, we’re introducing a few new concepts:
> 
> *   the Data Stream - essentially a Data Package in transit;
> *   the Data Processor, which manipulates a Data Package, receiving one Data Stream as its input and producing a new Data Stream as its output.
> *   A chain of Data Processors is what we call a Data Flow.

We will be providing a library of such processors: some for loading data from various sources, some for storing data in different locations, services or databases, and some for doing common manipulation and transformation on the processed data.

On top of all that we’re building a few integrated services:

*   `dataflows-server` (formerly known as `datapackage-pipelines`) - a server side multi-processor runner for Data Flows.
*   `dataflows-cli` - a client library for building and running Data Flows locally
*   `dataflows-blueprints` - ready made flow generators for common scenarios (e.g. ‘I want to regularly pull all my analytics from these X services and dump them in a database’)
*   and more to come.

♦

On Data Wrangling

In our experience, data processing starts simple - downloading and inspecting a CSV, deleting a column or a row. We wanted something that was as fast as the command line to get started but would also provide a solid basis as your pipeline grows. We also wanted something that provided some standardization and conventions over completely bespoke code.

With integration in mind, DataFlows comes with very little environmental requirements, and can be embedded in your existing data processing setup.

In short, DataFlows provides a simple, quick and easy-to-setup, and extensible way to build lightweight data processing pipelines.

Introducing dataflows

The first piece of software we’re introducing today is `dataflows` and its standard library of processors.

`dataflows` introduces the concept of a `Flow` - a chain of data processors, reading, transforming and modifying a stream of data and writing it to any location (or loading it to memory for further analysis).

`dataflows` also comes with a rich set of built-in data processors, ready to do most of the heavy-lifting you’ll need to reduce boilerplate code and increase your productivity.

A demo is worth a thousand words

Most data processing starts simple: getting a file and having a look.

With `dataflows` you can do this in a few seconds and you’ll have a solid basis for whatever you want to do next.

Bootstrapping a data processing script

    $ pip install dataflows
    $ dataflows init rawgit.com/datahq/demo/_/first.csv
    Writing processing code into first_csv.py
    Running first_csv.py
    first:
      #  Name        Composed    DOB
         (string)    (string)    (date)
    ---  ----------  ----------  ----------
      1  George      22          1943-02-25
      2  John        90          1940-10-09
      3  Richard     2           1940-07-07
      4  Paul        88          1942-06-18
      5  Brian       n/a         1934-09-19

    Done!

`dataflows init` actually does 3 things:

*   Analyzes the source file
*   Creates a processing script for reading it
*   Runs that script for you

In our case, a script named `first_csv.py` was created - here’s what it contains:

    # ...

    def first_csv():
        flow = Flow(
            # Load inputs
            load('rawgit.com/datahq/demo/_/first.csv',
                 format='csv', ),
            # Process them (if necessary)
            # Save the results
            add_metadata(name='first_csv', title='first.csv'),
            printer(),
        )
        flow.process()

    if __name__ == '__main__':
        first_csv()

The `flow` variable contains the chain of processing steps (i.e. the processors). In this simple flow, `load` loads the source data, `add_metadata` modifies the file’s metadata and `printer` outputs the contents to the standard output.

You can run this script again at any time, and it will re-run the processing flow:

    $ python first_csv.py
    first:
      #  Name        Composed    DOB
         (string)    (string)    (date)
    ---  ----------  ----------  ----------
      1  George      22          1943-02-25
    ...

This is all very nice, but now it’s time for some real data wrangling. By editing the processing script it’s possible to add more functionality to the flow - `dataflows` provides a simple, solid basis for building up your pipeline quickly, reliably and repeatedly.

Fixing some bad data

Let’s start by getting rid of that annoying `n/a` in the last line of the data.

We edit `first_csv.py` and add to the flow two more steps:

    def removeNA(row):
        row['Composed'] = row['Composed'].replace('n/a', '')

    f = Flow(
            load('rawgit.com/datahq/demo/_/first.csv'),
    		# added here custom processing:
    	    removeNa,
    	    # now parse column as Integer:
            set_type('Composed', type='integer'),
            printer()
        )

`removeNa` is a simple function which modifies each row it sees, replacing `n/a`s with the empty string. After it we call `set_type`, which declares the `Composed`column should be an integer - and verifies it’s indeed an integer while processing data.

Writing the cleaned data

Finally, let’s write the output to a file using the `dump_to_path` processor:

    def removeNA(row):
        row['Composed'] = row['Composed'].replace('n/a', '')

    f = Flow(
    		load('rawgit.com/datahq/demo/_/first.csv'),                	
           add_metadata(
                name='beatles_infoz',
                title='Beatle Member Information',
            ),
     	    removeNa,
            set_type('Composed', type='integer'),
            dump_to_path('first_csv/')
        )

Now, we re-run our modified processing script…

    $ python first_csv.py
    ...

we get a valid Data Package which we can use…

    $ tree
    ├── first_csv
    │   ├── datapackage.json
    │   └── first.csv

which contains a normalized and cleaned-up CSV file…

    $ head out/out.csv
    Name,Composed,DOB
    George,22,1943-02-25
    John,90,1940-10-09
    Richard,2,1940-07-07
    Paul,88,1942-06-18
    Brian,,1934-09-19

`datapackage.json`, a JSON file containing the package’s metadata…

    $ cat first_csv/datapackage.json    # Edited for brevity
    {
      "count_of_rows": 5,
      "name": "beatles_infoz",
      "title": "Beatle Member Information",
      "resources": [
        {
          "name": "first",
          "path": "first.csv",
          "schema": {
            "fields": [
              {"name": "Name",     "type": "string"},
              {"name": "Composed", "type": "integer"},
              {"name": "DOB",      "type": "date"}
            ]
          }
        }
      ]
    }

and is very simple to use in Python (or JS, Ruby, PHP and many other programming languages) -

    $ python
    >>> from datapackage import Package
    >>> p = Package('first_csv/datapackage.json')
    >>> list(p.resources[0].iter())
    [['George', 22, datetime.date(1943, 2, 25)],
     ['John', 90, datetime.date(1940, 10, 9)],
     ['Richard', 2, datetime.date(1940, 7, 7)],
     ['Paul', 88, datetime.date(1942, 6, 18)],
     ['Brian', None, datetime.date(1934, 9, 19)]]
    >>>

More ….

Lots, lots more - there is a whole suite of processors built in plus you can quickly add your own with a few lines of python code.

Dig in at the project’s GitHub Page or continue reading the in-depth tutorial here.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Monday, 07\. May 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Processing Tabular Data Packages in Clojure](http://okfnlabs.org/blog/2018/05/07/datapackages-in-clojure.html)

<div class="item-body">

<div class="item-snippet">Matt Thompson was one of 2017’s Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data data package and table schema libraries in Clojure programming language. You can read more about this in his grantee profile. In this post, Thompson will show you how to set up and use the Clojure libraries for working with Tabular Data Packages. This tutorial uses</div>

<div class="item-content item-summary">

Matt Thompson was one of 2017’s Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data data package and table schema libraries in Clojure programming language. You can read more about this in his grantee profile. In this post, Thompson will show you how to set up and use the Clojure libraries for working with Tabular Data Packages.

This tutorial uses a worked example of downloading a data package from a remote location on the web, and using the Frictionless Data tools to read its contents and metadata into Clojure data structures.

Setup

First, we need to set up the project structure using the Leiningen tool. If you don’t have Leiningen set up on your system, follow the link to download and install it. Once it is set up, run the following command from the command line to create the folders and files for a basic Clojure project:

    $ lein new periodic-table

This will create the periodic-table folder. Inside the periodic-table/src/periodic-table folder should be a file named core.clj. This is the file you need to edit during this tutorial.

The Data

For this tutorial, we will use a pre-created data package, the Periodic Table Data Package hosted by the Frictionless Data project. A Data Package is a simple container format used to describe and package a collection of data. It consists of two parts:

*   Metadata that describes the structure and contents of the package
*   Resources such as data files that form the contents of the package

Our Clojure code will download the data package and process it using the metadata information contained in the package. The data package can be found here on GitHub.

The data package contains data about elements in the periodic table, including each element’s name, atomic number, symbol and atomic weight. The table below shows a sample taken from the first three rows of the CSV file:

atomic number symbol name atomic mass metal or nonmetal? 1 H Hydrogen 1.00794 nonmetal 2 He Helium 4.002602 noble gas 3 Li Lithium 6.941 alkali metal Loading the Data Package

The first step is to load the data package into a Clojure data structure (a map). The initial step is to require the data package library in our code (which we will give the alias dp). Then we can use the load function to load our data package into our project. Enter the following code into the core.clj file:

    (ns periodic-table.core
      (:require [frictionlessdata.datapackage :as dp]
                [frictionlessdata.tableschema :as ts]
                [clojure.spec.alpha :as s]))

    (def pkg
      (dp/load "raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json"))

This pulls the data in from the remote GitHub location and converts the metadata into a Clojure map. We can access this metadata by using the `descriptor` function along with keys such as `:name` and `:title` to get the relevant information:

    (println (str "Package name:" (dp/descriptor pkg :name)))
    (println (str "Package title:" (dp/descriptor pkg :title)))

The package descriptor contains metadata that describes the contents of the data package. What about accessing the data itself? We can get to it using the `get-resources` function:

    (def table (dp/get-resources pkg :data))

    (doseq [row table]
      (println row))

The above code locates the data in the data package, then goes through it line by line and prints the contents.

Casting Types with core.spec

We can use Clojure’s spec library to define a schema for our data, which can then be used to cast the types of the data in the CSV file.

Below is a spec description of a periodic element type, consisting of an atomic number, atomic symbol, the element’s name, its mass, and whether or not the element is a metal or non-metal:

    (s/def ::number int?)
    (s/def ::symbol string?)
    (s/def ::name string?)
    (s/def ::mass float?)
    (s/def ::metal string?)

    (s/def ::element (s/keys :req [::number ::symbol ::name ::mass ::metal]))

The above spec can be used to cast values in our tabular data so that they match the specified schema. The example below shows our tabular data values being cast to fit the spec description. Then the `-main` function loops through the elements, printing only those with an atomic mass of over 10.

    (ns periodic-table.core
      (:require [frictionlessdata.datapackage :as dp]
                [frictionlessdata.tableschema :as ts]
                [clojure.spec.alpha :as s]))

    (s/def ::number int?)
    (s/def ::symbol string?)
    (s/def ::name string?)
    (s/def ::mass float?)
    (s/def ::metal string?)

    (s/def ::element (s/keys :req [::number ::symbol ::name ::mass ::metal]))

    (def pkg
      (dp/load "raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json"))

    (def resources (dp/get-resources pkg :data))

    (def elements (dp/cast resources element))

    (defn -main []
      (doseq [e elements]
        (if (< (:mass e) 10)
          (println e))))

When run, the program produces the following output:

    $ lein run
    {::number 1 ::symbol "H" ::name "Hydrogen" ::mass 1.00794 ::metal "nonmetal"}
    {::number 2 ::symbol "He" ::name "Helium" ::mass 4.002602 ::metal "noble gas"}
    {::number 3 ::symbol "Li" ::name "Lithium" ::mass 6.941 ::metal "alkali gas"}
    {::number 4 ::symbol "Be" ::name "Beryllium" ::mass 9.012182 ::metal "alkaline earth metal"}

This concludes our simple tutorial for using the Clojure libraries for Frictionless Data.

We welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-clj repository.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Wednesday, 02\. May 2018

<article class="item">

#### [schema.org News](http://blog.schema.org)

### [Datacommons.org and Schema.org](http://blog.schema.org/2018/05/datacommonsorg-and-schemaorg.html)

<div class="item-body">

<div class="item-snippet">Over the past few years we have seen a number of application areas benefit from Schema.org markup. Schema.org discussions have often centered around the importance of ease of use, simplicity and adoption for publishers and webmasters. While those principles will continue to guide our work, it is also important to work to make it easier to consume structured data, by building applications and making</div>

<div class="item-content item-summary">Over the past few years we have seen a number of application areas benefit from Schema.org markup. Schema.org discussions have often centered around the importance of ease of use, simplicity and adoption for publishers and webmasters. While those principles will continue to guide our work, it is also important to work to make it easier to consume structured data, by building applications and making more use of the information it carries. We are therefore happy to welcome the new Data Commons initiative, which is devoted to sharing such datasets, beginning with a corpus of fact check data based on the schema.org ClaimReview markup as adopted by many fact checkers around the world. We expect that this work will benefit the wider ecosystem around structured data by encouraging use and re-use of schema.org related datasets.</div>

<div class="item-footer">Published 20:41 • over a year ago | Updated Wednesday, 02\. May 2018 20:46 • over a year ago</div>

</div>

</article>

## Saturday, 28\. April 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Processing Tabular Data Packages in Java](http://okfnlabs.org/blog/2018/04/28/datapackages-in-java.html)

<div class="item-body">

<div class="item-snippet">Georges Labrèche was one of 2017’s Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Java programming language. You can read more about this in his grantee profile. In this post, Labrèche will show you how to install and use the Java libraries for working with Tabular Data Packages. Our goal in this tutorial is to load tabular data</div>

<div class="item-content item-summary">

Georges Labrèche was one of 2017’s Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Java programming language. You can read more about this in his grantee profile.

In this post, Labrèche will show you how to install and use the Java libraries for working with Tabular Data Packages.

Our goal in this tutorial is to load tabular data from a CSV file, infer data types and the table’s schema.

Setup

First things first, you’ll want to grab datapackage-java and the tableschema-java libraries.

The Data

For our example, we will use a Tabular Data Package containing the periodic table. You can find the data package descriptor and the data on GitHub.

Packaging

Let’s start by fetching and packaging the data:

    // fetch the data
    URL url = new URL("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json");

    // package the data
    Package dp = new Package(url);

That’s it, you’re all set to start playing with the packaged data. There are parameters you can set such as loading a schema or imposing strict validation so be sure to go through the project’s README for more detail.

Iterating

Now that you have a Data Package instance, let’s see what the data looks like. A data package can contain more than one resource so you have to use the `Package.getResource()` method to specify which resource you’d like to access.

Let’s iterate over the data:

    // Get a resource named data from the data package
    Resource resource = pkg.getResource("data");

    // Get the Iterator
    Iterator<String[]> iter = resource.iter();

    // Iterate
    while(iter.hasNext()){
    	String[] row = iter.next();
       	String atomicNumber = row[0];
       	String symbol = row[1];
       	String name = row[2];
      	String atomicMass = row[3];
       	String metalOrNonMetal = row[4];
    }

Notice how we’re fetching all values as `String`. This may not be what you want, particularly for the atomic number and mass. Alternatively, you can trigger data type inference and casting like this:

    // Get Iterator.
    // Third boolean is the cast flag.
    Iterator<Object[]> iter = resource.iter(false, false, true));

    // Iterator
    while(iter.hasNext()){
    	String[] row = iter.next();
       	int atomicNumber = row[0];
       	String symbol = row[1];
       	String name = row[2];
      	float atomicMass = row[3];
       	String metalOrNonMetal = row[4];
    }

And that’s it, your data is now associated with the appropriate data types!

Inferring the Schema

We wouldn’t have had to infer the data types if we had included a Table Schema when creating an instance of our Data Package. If a Table Schema is not available, then it’s something that can also be inferred and created with `tableschema-java`:

    URL url = new URL("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/data.csv");
    Table table = new Table(url);
    Schema schema = table.inferSchema();
    schema.write("/path/to/write/schema.json");

The type inference algorithm tries to cast to available types and each successful type casting increments a popularity score for the successful type cast in question. At the end, the best score so far is returned.

The inference algorithm traverses all of the table’s rows and attempts to cast every single value of the table. When dealing with large tables, you might want to limit the number of rows that the inference algorithm processes:

    // Only process the first 25 rows for type inference.
    Schema schema = table.inferSchema(25);

Be sure to go through `tableschema-java`’s README as well to learn more about how to operate with Table Schema.

Contributing

In case you discovered an issue that you’d like to contribute a fix for, or if you would like to extend functionality:

    # install jabba and maven2
    $ cd tableschema-java
    $ jabba install 1.8
    $ jabba use 1.8
    $ mvn install -DskipTests=true -Dmaven.javadoc.skip=true -B -V
    $ mvn test -B

Make sure that all tests pass, and submit a PR with your contributions once you’re ready.

We also welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-java repository.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Thursday, 08\. March 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Collecting, Analysing and Sharing Twitter Data](http://okfnlabs.org/blog/2018/03/08/open-data-day-tweets.html)

<div class="item-body">

<div class="item-snippet">On March 3, communities around the world marked Open Data Day in over 400 events. Here’s the dataset for all Open Data Day 2018 events. In this post, we will harvest Open Data Day affiliated content from Twitter and analyze it using R before packaging and publishing the data and associated resources publicly on GitHub. Collecting the Data With over 300million monthly users [source, January 2018]</div>

<div class="item-content item-summary">

On March 3, communities around the world marked Open Data Day in over 400 events. Here’s the dataset for all Open Data Day 2018 events.

In this post, we will harvest Open Data Day affiliated content from Twitter and analyze it using R before packaging and publishing the data and associated resources publicly on GitHub.

Collecting the Data

With over 300million monthly users [source, January 2018], Twitter is a popular social network that I particularly like for its abbreviated messages, known as Tweets. Twitter’s Standard Search API allows users to mine tweets from as far back as a week for free.

R is a popular programming language for data analysis and has an active community of contributors that add to its capabilities by writing custom packages for interacting with different tools and platforms and achieving different tasks. In this post, we will employ two such packages:

*   twitteR allows us to interact with the Twitter API. We will install this from CRAN, the official packages repository for R.
*   Frictionless Data’s datapackage.r library will allow us to collate our open data day data and associated resources, such as the R script in one place before we publish it. We will install this from GitHub.

To get started, create a new application on apps.twitter.com and take note of the API and access tokens. We will need to specify these in our R script.

    # install and load the twitteR library

    install.packages("twitteR")
    library(twitteR)

    # specify Twitter API and Access Tokens

    api_key <- "YOUR_API_KEY"
    api_secret <- "YOUR_API_SECRET"
    access_token <- "YOUR_ACCESS_TOKEN"
    access_secret <- "YOUR_ACCESS_SECRET"

    setup_twitter_oauth(api_key, api_secret, access_token, access_secret)

We are now ready to read tweets from the two official Open Data Day hashtags: #opendataday and #odd18\. With a maximum number of 100 tweets per request, Twitter’s Search API allows for 180 Requests every 15 minutes. Since we are interested in as many tweets as we can get, we will specify the upper limit as 18,000, which tells the twitteR library the maximum number of tweets to retrieve for us.

    # read tweets from the two official hashtags, #opendataday and #odd18

    tweets_opendataday <- searchTwitteR("#opendataday", n = 18000)
    tweets_odd18 <- searchTwitteR("#odd18", n = 18000)

    # view lists of mined tweets from both hashtags

    tweets_opendataday
    tweets_odd18

Note: run each `searchTwitteR()` function separately and 15 minutes apart to avoid surpassing the limit.

In the R script snippet above, we assigned results of our search to the variables `tweets_opendataday` and `tweets_odd18` and called the two variables to view the entire list of tweets obtained. Lucky for us, the total number of tweets on either hashtag are within Twitter’s 15 minute request limit. Here’s the feedback we receive:

    # tweets mined on March 7, 2018

    #opendataday
     18000 tweets were requested but the API can only return 11458

    #odd18
     18000 tweets were requested but the API can only return 3497

Here’s a snippet of the list obtained from the #opendataday hashtag:

    [[1]]
    [1] "OpenDataAnd: Con motivo del pasado #OpenDataDay, @ODIHQ nos recuerda qu<U+00E9> es y para qu<U+00E9> sirven los #DatosAbiertos<U+2026> t.co/Fib4rSukbs"

    [[2]]
    [1] "johnfaig: RT @ODIHQ: Here's our list of seven weird and wonderful open datasets (nominated by you) t.co/H42bV5oIhw\n\n#opendataday #opendataday<U+2026>"

    [[3]]
    [1] "SurianoRodrigo: RT @CETGAPue: Desde el auditorio Ing. Antonio Osorio Garc<U+00ED>a de la @fi_buap, se lleva a cabo el BootCamp #OpenDataDay, al que asisten acad<U+00E9>m<U+2026>"

    [[4]]
    [1] "Carolrozu: RT @CETGAPue: Desde el auditorio Ing. Antonio Osorio Garc<U+00ED>a de la @fi_buap, se lleva a cabo el BootCamp #OpenDataDay, al que asisten acad<U+00E9>m<U+2026>"

    [[5]]
    [1] "Josefina_Buxade: RT @CETGAPue: Desde el auditorio Ing. Antonio Osorio Garc<U+00ED>a de la @fi_buap, se lleva a cabo el BootCamp #OpenDataDay, al que asisten acad<U+00E9>m<U+2026>"

Since the entire lists are long (~11,500 tweets on the #opendataday hashtag) and hard to comprehend, our best bet is to convert the lists to data frames. In R, data frames allow us to store data in tables and manipulate and analyse them easily. twitteR’s `twListToDF` function allows us to convert lists to data frames. After scraping data, it is always a good idea to save the original raw data as it provides a good base for any analysis work. We will write our data to a CSV file, so we can publish and it widely. The CSV format is machine-readable and easy to import into any spreadsheet application or advanced tools for analysis.

    # convert the list of mined tweets from each hashtag to a dataframe

    tweets_opendataday_df <- twListToDF(tweets_opendataday)
    tweets_odd18_df <- twListToDF(tweets_odd18)

    # save scraped data in CSV files

    write.csv(tweets_opendataday_df, file="data/opendataday_raw.csv")
    write.csv(tweets_odd18_df, file="data/odd18_raw.csv")

Here’s what the first five rows of our data frame look like:

text favorited favoriteCount replyToSN created truncated replyToSID id replyToUID statusSource screenName retweetCount isRetweet retweeted longitude latitude   1 Participan como panelistas de la mesa <U+201C>Datos ma<U+00F1>aneros, qu<U+00E9> son y para qu<U+00E9> sirven los #DatosAbiertos<U+201D>, Karla Ramos<U+2026> t.co/wFBYaUP68n FALSE 3 NA 05/03/18 16:29 TRUE NA 9.70698E+17 NA Twitter for iPhone CETGAPue 2 FALSE FALSE NA NA 2 RT @Transparen_Xal: A unos minutos de empezar el Open Data Day Xalapa#ODD18 #Xalapa t.co/VH3m0QGeOJ FALSE 0 NA 05/03/18 16:28 FALSE NA 9.70698E+17 NA Hootsuite AytoXalapa 1 TRUE FALSE NA NA 3 Nos encontramos ya en @ImacXalapa con @AytoXalapa para sumar esfuerzos a favor de la cultura de participaci<U+00F3>n ciuda<U+2026> t.co/VdIcF16Ub4 FALSE 0 NA 05/03/18 16:22 TRUE NA 9.70696E+17 NA Twitter for Android VERIVAI 1 FALSE FALSE NA NA 4 A unos minutos de empezar el Open Data Day Xalapa#ODD18 #Xalapa t.co/VH3m0QGeOJ FALSE 0 NA 05/03/18 16:20 FALSE NA 9.70696E+17 NA Twitter for iPhone Transparen_Xal 1 FALSE FALSE NA NA 5 El gobierno de @TonyGali promueve el uso de los #DatosAbiertos. Entra al portal t.co/Jz23xpJLAS y consult<U+2026> t.co/UoWP43R8Km FALSE 5 NA 05/03/18 16:09 TRUE NA 9.70693E+17 NA Twitter for iPhone CETGAPue 4 FALSE FALSE NA NA

For ease of analysis, and because the two data frames have the same columns, let’s merge the two datasets.

    # combine dataframes from the two hashtags

    alltweets_df <- rbind(tweets_opendataday_df, tweets_odd18_df)
    write.csv(alltweets_df, file="data/allopendatadaytweets.csv")

Analysing the Data

Data analysis in R is quite a joy. We will use R’s `dplyr` package to analyse our data and answer a few questions:

*   how many open data day attendees tweeted from android phones?

We can answer this using dplyr’s select() function, which as the name suggests, allows us to see only data we are interested in, in this case, tweets sent from the Twitter for Android app.

    # install and load dplyr

    install.packages(dplyr)
    library(dplyr)

    # find out number of open data day tweets from android phones

    android_tweets <- filter(alltweets_df, grepl("Twitter for Android", statusSource))
    tally(android_tweets)

    # the result
       n
    1 5180

5,180 of the 14,955 (34.6%) #opendataday and #odd18 tweets were sent from android phones.

*   Naturally, Open Data Day events cut across many topics and disciplines, and some events included hands-on workshop sessions or hackathons. Let’s find out which open data day tweets point to open source projects and resources that are available on GitHub.

     # open data day tweets that mention resources on GitHub

    github_resources <- filter(alltweets_df, grepl("github.com", statusSource))

    # the result
     n
    1 32

Only 32 #opendataday and #odd18 tweets contain GitHub links.

*   Not all open data day tweets are geotagged, but from the few that are, we can create a very basic map to show where people tweeted from. To do this, we will use the Leaflet library for R.

    # install and load leaflet

    install.packages(leaflet)
    library(leaflet)

    # create basic map
    map <- leaflet() %>%
      addTiles() %>%
      addCircles(data = alltweets_df, lat = ~ latitude, lng = ~ longitude)

    # view map
    map

♦  
figure 1: map showing where geotagged #opendataday and #odd18 tweets originated from

Sharing the Data

Due to Twitter’s terms of use, we can only share a stripped-down version of the raw data. Our final dataset contains tweet IDs and retweet count, and will be packaged alongside this R script, so you could download the tweets yourself.

    # filter out retweets and leave original tweets

      notretweets_df <- dplyr::filter(alltweets_df, grepl("FALSE", isRetweet))

    # strip down tweets data to comply with Twitter's terms of use.

      subsetoftweets <- select(notretweets_df, id, retweetCount)
      write.csv(subsetoftweets, file="data/subsetofopendatadaytweets.csv")

Packaging the Data and associated resources

Providing context when sharing data is important, and Frictionless Data’s Data Package format makes it possible. Using datapackage.r, we can infer a schema for the `all tweets` CSV file and publish it alongside the other resources.

    # specify filepath and infer schema
        filepath = '/data/subsetofopendatadaytweets.csv'

        schema = tableschema.r::infer(filepath)

Read more about the datapackage-r package in this post by Open Knowledge Greece.

Alternatively, we can use the Data Package Creator to package our data and associated resources.

♦  
figure 2: creating the data package on Data Package Creator

Read more about the data package creator in this post.

Publishing on Github

Once our data package is ready, we can simply publish it to GitHub. Find the open data day tweets data package here.

Conclusion

Data Packages are a great format for sharing data collections with contextual information i.e. we added metadata and a schema to our final dataset. Read more about Data Packages in Frictionless Data and reach out in our community chat on Gitter.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Friday, 16\. February 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Processing Tabular Data Packages in Go](http://okfnlabs.org/blog/2018/02/16/datapackages-in-go.html)

<div class="item-body">

<div class="item-snippet">Daniel Fireman was one of 2017’s Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Go programming language. You can read more about this in his grantee profile. In this post, Fireman will show you how to install and use the Go libraries for working with Tabular Data Packages. Our goal in this tutorial is to load a data package from</div>

<div class="item-content item-summary">

Daniel Fireman was one of 2017’s Frictionless Data Tool Fund grantees tasked with extending implementation of core Frictionless Data libraries in Go programming language. You can read more about this in his grantee profile.

In this post, Fireman will show you how to install and use the Go libraries for working with Tabular Data Packages.

Our goal in this tutorial is to load a data package from the web and read its metadata and contents.

Setup

For this tutorial, we will need the datapackage-go and tableschema-go packages, which provide all the functionality to deal with a Data Package’s metadata and its contents.

We are going to use the dep tool to manage the dependencies of our new project:

    $ cd $GOPATH/src/newdataproj
    $ dep init

The Periodic Table Data Package

A Data Package is a simple container format used to describe and package a collection of data. It consists of two parts:

*   Metadata that describes the structure and contents of the package
*   Resources such as data files that form the contents of the package

In this tutorial, we are using a Tabular Data Package containing the periodic table. The package descriptor (datapackage.json) and contents (data.csv) are stored on GitHub. This dataset includes the atomic number, symbol, element name, atomic mass, and the metallicity of the element. Here are the header and the first three rows:

atomic number symbol name atomic mass metal or nonmetal? 1 H Hydrogen 1.00794 nonmetal 2 He Helium 4.002602 noble gas 3 Li Lithium 6.941 alkali metal Inspecting Package Metadata

Let’s start off by creating the `main.go`, which loads the data package and inspects some of its metadata.

    package main

    import (
        "fmt"

        "github.com/frictionlessdata/datapackage-go/datapackage"
    )

    func main() {
        pkg, err := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
        if err != nil {
            panic(err)
        }
        fmt.Println("Package loaded successfully.")
    }

Before running the code, you need to tell the dep tool to update our project dependencies. Don’t worry; you won’t need to do it again in this tutorial.

    $ dep ensure
    $ go run main.go
    Package loaded successfully.

Now that you have loaded the periodic table Data Package, you have access to its `title` and `name` fields through the Package.Descriptor() function. To do so, let’s change our main function to (omitting error handling for the sake of brevity, but we know it is very important):

    func main() {
        pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
        fmt.Println("Name:", pkg.Descriptor()["name"])
        fmt.Println("Title:", pkg.Descriptor()["title"])
    }

And rerun the program:

    $ go run main.go
    Name: period-table
    Title: Periodic Table

And as you can see, the printed fields match the package descriptor. For more information about the Data Package structure, please take a look at the specification.

Quick Look At the Data

Now that you have loaded your Data Package, it is time to process its contents. The package content consists of one or more resources. You can access Resources via the Package.GetResource() method. Let’s print the periodic table `data` resource contents.

    func main() {
        pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
        res := pkg.GetResource("data")
        table, _ := res.ReadAll()
        for _, row := range table {
            fmt.Println(row)
        }
    }

    $ go run main.go
    [atomic number symbol name atomic mass metal or nonmetal?]
    [1 H Hydrogen 1.00794 nonmetal]
    [2 He Helium 4.002602 noble gas]
    [3 Li Lithium 6.941 alkali metal]
    [4 Be Beryllium 9.012182 alkaline earth metal]
    ...

The Resource.ReadAll() method loads the whole table in memory as raw strings and returns it as a Go `[][]string`. This can be quick useful to take a quick look or perform a visual sanity check at the data.

Processing the Data Package’s Content

Even though the string representation can be useful for a quick sanity check, you probably want to use actual language types to process the data. Don’t worry, you won’t need to fight the casting battle yourself. Data Package Go libraries provide a rich set of methods to deal with data loading in a very idiomatic way (very similar to encoding/json).

As an example, let’s change our `main` function to use actual types to store the periodic table and print the elements with atomic mass smaller than 10.

    package main

    import (
        "fmt"

        "github.com/frictionlessdata/datapackage-go/datapackage"
        "github.com/frictionlessdata/tableschema-go/csv"
    )

    type element struct {
        Number int     `tableheader:"atomic number"`
        Symbol string  `tableheader:"symbol"`
        Name   string  `tableheader:"name"`
        Mass   float64 `tableheader:"atomic mass"`
        Metal  string  `tableheader:"metal or nonmetal?"`
    }

    func main() {
        pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
        resource := pkg.GetResource("data")

        var elements []element
        resource.Cast(&elements, csv.LoadHeaders())
        for _, e := range elements {
            if e.Mass < 10 {
                fmt.Printf("%+v\n", e)
            }
        }
    }

    $ go run main.go
    {Number:1 Symbol:H Name:Hydrogen Mass:1.00794 Metal:nonmetal}
    {Number:2 Symbol:He Name:Helium Mass:4.002602 Metal:noble gas}
    {Number:3 Symbol:Li Name:Lithium Mass:6.941 Metal:alkali metal}
    {Number:4 Symbol:Be Name:Beryllium Mass:9.012182 Metal:alkaline earth metal}

In the example above, all rows in the table are loaded into memory. Then every row is parsed into an `element` object and appended to the slice. The `resource.Cast` call returns an error if the whole table cannot be successfully parsed.

If you don’t want to load all data in memory at once, you can lazily access each row using Resource.Iter and use Schema.CastRow to cast each row into an `element` object. That would change our main function to:

    func main() {
        pkg, _ := datapackage.Load("raw.githubusercontent.com/frictionlessdata/example-data-packages/62d47b454d95a95b6029214b9533de79401e953a/periodic-table/datapackage.json")
        resource := pkg.GetResource("data")

        iter, _ := resource.Iter(csv.LoadHeaders())
        sch, _ := resource.GetSchema()
        var e element
        for iter.Next() {
            sch.CastRow(iter.Row(), &e)
            if e.Mass < 10 {
                fmt.Printf("%+v\n", e)
            }
        }
    }

    $ go run main.go
    {Number:1 Symbol:H Name:Hydrogen Mass:1.00794 Metal:nonmetal}
    {Number:2 Symbol:He Name:Helium Mass:4.002602 Metal:noble gas}
    {Number:3 Symbol:Li Name:Lithium Mass:6.941 Metal:alkali metal}
    {Number:4 Symbol:Be Name:Beryllium Mass:9.012182 Metal:alkaline earth metal}

And our code is ready to deal with the growth of the periodic table in a very memory-efficient way :-)

We welcome your feedback and questions via our Frictionless Data Gitter chat or via GitHub issues on the datapackage-go repository.

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>

## Thursday, 15\. February 2018

<article class="item">

#### [Open Knowledge Foundation (OKFN) Labs News](http://okfnlabs.org/blog)

### [Frictionless Data Lib - A Design Pattern for Accessing Files and Datasets](http://okfnlabs.org/blog/2018/02/15/design-pattern-for-a-core-data-library.html)

<div class="item-body">

<div class="item-snippet">This document outlines a simple design pattern for a “core” data library "data". The pattern is focused on access and use of: individual files (streams) collections of files (“datasets”) Its primary operation is open: file = open('path/to/file.csv') dataset = open('path/to/files/') It defines a standardized “stream-plus-metadata” interface for file and dataset objects, along with method</div>

<div class="item-content item-summary">

This document outlines a simple design pattern for a “core” data library `"data"`.

The pattern is focused on access and use of:

*   individual files (streams)
*   collections of files (“datasets”)

Its primary operation is `open`:

    file = open('path/to/file.csv')
    dataset = open('path/to/files/')

It defines a standardized “stream-plus-metadata” interface for file and dataset objects, along with methods for creating these from file or dataset pointers such as file paths or urls.

    file = open('path/to/file.csv')
    file.stream()
    file.rows()
    file.descriptor
    file.descriptor.path
    ...

This pattern derives from many years experience working on data tools and projects like Frictionless Data. Specifically:

*   Data “plus”: when you work with data you always find yourself needing the data itself plus a little bit more – things like where the data came from on disk (or is going to), or how large it is. This pattern gives you that information in a standardized way.
*   Streams (and strings): streams are the standard way to access data (though strings are useful too) and you should get the same interface whether you’ve loaded data from online, on disk or inline; and, finally, we want both raw byte streams and (for tabular data) object/row streams aka iterators.
*   Building blocks: most data wrangling, even in simple cases, involves building data processing pipelines. Pipelines need a standard stream-plus-metadata interface to pass data between steps in the pipeline. For example, suppose you want to load a csv file and convert to JSON and write to stdout: that’s already three steps (load, convert, dump). Then suppose you want to delete the first 3 rows, delete the 2nd column. Now you have a more complex processing pipeline.

♦

Fig 1: data pipelines and the stream-plus-metadata pattern

The pattern leverages the Frictionless Data specs including Data Resource, Table Schema and Data Package. But it keeps metadata implicit rather than explicit and focuses on giving users the simplest most direct interface possible (put most crudely: `open` then `stream`). You can find more about the connection with the Frictionless Data tooling in the appendix.

Finally, we already have one working implementation of the pattern in javascript:

github.com/datahq/data.js

Work on a python implementation is underway (most of the code is already there in the python Data Package libraries).

Table of Contents

*   Overview of the Pattern
*   The Pattern in Detail
    *   `open` method
        *   File locators
    *   File
        *   Metadata: `descriptor`
        *   Accessing data
            *   `stream`
            *   `rows`
                *   Support for TableSchema and CSV Dialect
    *   Dataset
        *   `open` for datasets
            *   Dataset Locators
        *   `descriptor`
        *   `identifier` (optional)
        *   README
        *   `files`
        *   addFile
    *   Operators
*   Conclusion
*   Appendix: Why we need a pattern like this
    *   All data wrangling tools need to load and then pass around “file-like objects” as they process data
    *   A file is more than a byte stream: the stream may be structured and there is usually the need for associated metadata
    *   Tool authors find themselves inventing their own “stream-plus-metadata” objects … but they are all different
    *   Plus, many tools also need to access collection of files, i.e. datasets
    *   Having a common API pattern for files (stream-plus-metadata) and datasets would reduce duplication and support plug and play with tooling
*   Appendix: Design Principles
    *   Orient to the data wrangler workflow
    *   Zen - maximum viable simplicity
        *   Core objects should be kept as simple as possible (and no simpler)
    *   Use Streams
*   Appendix: Internal Library Structure Suggestions
    *   Library Components
    *   Streams
    *   Loaders/Parsers and Writers
*   Appendix: API with Data Package terminology
*   Appendix: Connection with Frictionless Data
    *   Recommendations for Frictionless Data community
    *   Why do it?
    *   Relation to Data Packages

Overview of the Pattern

The pattern is based on the following principles:

*   Data wrangler focused: focus on the core data wrangler workflow: open a file and do something with it
*   Zen-like: Simplicity and power. As simple as possible: does just what it needs and no more.
*   Use Streams: stream focused library, including object streams (aka iterators).

A minimal viable interface for the file case:

    // this example uses javascript but the example is generic

    // data.js is just an illustrative name for the library
    const data = require('data.js')

    // path can be local or remote
    // file is now a data.File object
    const file = data.open(pathOrUrl)

    // a byte stream
    file.stream()

    // if this file is tabular this will give me a row stream (iterator)
    file.rows()

    // descriptor for this file including info like size (if available)
    // the descriptor follows the Data Resource specification
    // (and if Tabular the Tabular Data Resource spec)
    file.descriptor

For datasets:

    // path or url to a directory (or datapackage.json)
    // dataset is a data.Dataset object
    // note: may rename to openDataset if need to disambiguate from open(file)
    const dataset = open(pathOrUrl)

    // list of files
    dataset.files

    // readme (if README.md existed)
    dataset.readme

    // any metadata (either inferred or from datapackage.json)
    // this follows the Data Package spec
    dataset.descriptor

These interfaces can then form the standard basis for lots of additional functionality e.g.

    infer(file) => inferred tableschema (and types) for the column

    writer(file) => stream (for saving to disk)

    validate(file) => validate a file (assumes it has a tableschema)

NOTE: here we have used `file` and `dataset` terminology. If you are more familiar with the package and resource of the Frictionless Data specs please mentally substitute file => resource and dataset => package.

The Pattern in Detail

Note: Support for Datasets is optional. Supporting datasets is an added layer of complexity and some implementors MAY choose to support files only. If so, they MUST indicate this clearly.

`open` method

The library MUST provide a method `open` which takes a locator to a file and returns a File object:

    open(path/to/file.csv, [options]) => File object

`options` is a dictionary of keyword argument list of options. The library MUST support an option `basePath`. `basePath` is for cases where you want to create a File with a path that is relative to a base directory / path e.g.

    file = open('data.csv', {basePath: '/my/base/path'})

Will open the file: `/my/base/path/data.csv`

This functionality is mainly useful when using Files as part of Datasets where it can be convenient for a File to have a path relative to the directory of the Dataset. (See also Data Package and Data Resource in the Frictionless Data specs).

File locators

Locators can be:

*   A file path
*   A URL
*   Raw data in JSON format
*   A Data Resource (in native language structure)

Implementors MUST support file paths, SHOULD support URLs and MAY support the last two.

    file = open('/path/to/file.csv')

    file = open('example.com/data.xls')

    // loading raw data
    file = open({
      name: 'mydata',
      data: { // can be any javascript - an object, an array or a string or ...
        a: 1,
        b: 2
      }
    })

    // Loading with a descriptor - this allows more fine-grained configuration
    // The descriptor should follow the Frictionless Data Resource model
    // specs.frictionlessdata.io/data-resource/
    file = open({
      // file or url path
      path: 'example.com/data.csv',
      // a Table Schema - specs.frictionlessdata.io/table-schema/
      schema: {
        fields: [
          ...
        ]
      }
      // CSV dialect - specs.frictionlessdata.io/csv-dialect/
      dialect: {
        // this is tab separated CSV/DSV
        delimiter: '\\t'
      }
    })

File

The File instance MUST have the following properties and methods

Metadata: `descriptor`

Main metadata is available via the `descriptor`:

    file.descriptor

The descriptor follows the Frictionless Data Data Resource spec.

The descriptor metadata is a combination of the metadata passed in at File creation (if you created the File with a descriptor object) and auto-inferred information from the File path. This is the info that SHOULD be auto-inferred:

    path: path this was instantiated with - may not be same as file.path (depending on basePath)
    pathType: remote | local
    name:   file name (without extension)
    format: the extension
    mediatype: mimetype based on file name and extension

In addition to this metadata there are certain properties which MAY be computed on demand and SHOULD be available as getters on the file object:

    // the full path to the file (using basepath)
    const path = file.path

    const size = file.size

    // md5 hash of the file
    const hash = file.hash

    // file encoding
    const encoding = file.encoding

Note: size, hash are not available for remote Files (those created from urls).

Accessing data

Accessing data in the file:

    // byte stream
    file.stream()

    // if file is tabular
    // crude rows - no type casting etc
    file.rows(cast=False, keyed=False, ...)

    // entire file as a buffer/string (be careful with large files!)
    file.buffer()

    // (optional)
    // if tabular return entire set of rows as an array
    file.array()

    // EXPERIMENTAL
    // file object packed into a stream
    // metadata is first line (\n separated)
    // motivation: way to send object over single stdin/stdout pipe
    file.packed()

`stream`

A raw byte stream:

    stream()

`rows`

Get the rows for this file as an object stream / iterator.

    file.rows(cast=False, keyed=False, ...) =>
      iterator with items [val1, val2, val3, ...]

*   `keyed`: if `false` (default) returns rows as arrays i.e. [val1, val2, val3]. If `true` returns rows as objects i.e.. `{col1: val1, col2: val2, ...}`.
*   `cast`: if `false` (default) returns values uncast. If true attempts to cast values either using best-effort or TableSchema if available
*   `addRowNumber`: default `false`. Add first value or column `_id` to resulting rows with row number. [OPTIONAL for implementors]

Note: this method assumes underlying data is tabular. Library should raise appropriate error if called on a non-tabular file. It is also up to implementors what tabular formats they support (there are many). At a minimum the library MUST support CSV. It SHOULD support JSON and it MAY (it is desirable) support Excel.

Support for TableSchema and CSV Dialect

The library SHOULD support Table Schema and CSV Dialect in the `rows` method using metadata provided when the file was `open`ed:

    // load a CSV with a non-standard dialect e.g. tab separated or semi-colon separated
    file = open({
      path: 'mydata.tsv'
      // Full support for specs.frictionlessdata.io/csv-dialect/
      dialect: {
        delimiter: '\\t' // for tabs or ';' for semi-colons etc
      }
    })
    file.rows() // use the dialect info in parsing the csv

    // open a CSV with a Table Schema
    file = open({
      path: 'mydata.csv'
      // Full support for Table Schema specs.frictionlessdata.io/table-schema/
      schema: {
        fields: [
          {
            name: 'Column 1',
            type: 'integer'
          },
          ...
        ]
      }
    })

Dataset

A collection of data files with optional metadata.

Under the hood it heavily uses Data Package formats and it natively supports Data Package formats including loading from `datapackage.json` files. However, it does not require knowledge or use of Data Packages.

A Dataset has two key properties:

    // metadata
    dataset.descriptor

    // files in the dataset
    dataset.files

`open` for datasets

The library MUST provide a method `openDataset` that takes a locator to a dataset and returns a Dataset object:

    openDataset(path/to/dataset/) => Dataset object

The library MAY overload the `open` method to support datasets as well as files:

    open(path/to/dataset/) => Dataset object

Note: overloading can be tricky as disambiguating locators for files from locators for datasets is not always trivial.

Dataset Locators

`path/to/dataset` - can be one of:

*   local path to Dataset
*   remote url to Dataset
*   descriptor object (i.e. datapackage.json)

`descriptor`

A Dataset MUST have a `descriptor` which holds the Dataset metadata. The descriptor MUST follow the Data Package spec.

The Dataset SHOULD have the convenience attribute `path` which is the path (remote or local) to this dataset.

`identifier` (optional)

A Dataset MAY have a `identifier` property that encapsulates the location (or origin) of this Dataset. The locator property MUST have the following structure:

    {
      name: <name>,   // computed from path
      owner: <owner>, // may be null
      path: <path>,   // computed path
      type: <type>,   // e.g. local, url, github, datahub, ... 
      original: <path>, // path (file or url) as originally supplied
      version: <version> // version as computed
    }

Note: the identifier is parsed from the locator passed into the open method. See the Data Package identifier spec frictionlessdata.io/specs/data-package-identifier/ and implementation in data.js library github.com/datahq/data.js#parsedatasetidentifier

README

The Dataset object MAY support a `readme` property which returns a string corresponding to the README for this Dataset (if it exists).

The readme content is taken from the README.md file located in the Dataset root directory, or, if that does not exist from the `readme` property on the descriptor. If neither of those exist the readme will be undefined or null.

`files`

A Dataset MUST have `files` property which returns an array of the Files contained in this Dataset:

    dataset.files => Array(<File>)

addFile

The library SHOULD implement an `addFile` method to add a `File` to a Dataset:

    dataset.addFile(file)

*   `file`: an already instantiated File object or a File descriptor

Operators

Finally, we discuss some operators. These SHOULD not be in the core library but it is useful to be aware of them:

*   `infer(file) => TableSchema`: infer the Table Schema for a CSV file or other tabular file
    *   `inferStructure(file)`: infer the structure i.e. CSV Dialect of a CSV or other tabular file. In addition to CSV dialect properties this may include things like `skipRows` i.e. number of rows to skip
*   `validate(file/dataset, metadataOnly=False)`: validate the data in a file e.g. against its schema
    *   `metadataOnly`: only validate the metadata e.g. against the Data Package or Data Resource schemas.
*   `write(file/dataset)`: write a File or Dataset to disk

Conclusion

In this document we’ve outlined a “Frictionless Data Library” pattern that standardizes the design of a “core” data library API focused on accessing files and datasets.

Almost all data wrangling work involves opening data streams and passing them between processes. Standardizing the API would have major benefits for tool creators and users, making it quicker and easier to develop tooling as well as making tooling more “plug and play”.

Appendix: Why we need a pattern like this All data wrangling tools need to load and then pass around “file-like objects” as they process data

All data tools need to access files/streams:

    data = open(path/to/file.csv)

And so every programming language and every tool have a method for opening a file path and returning a byte-stream.

But …

A file is more than a byte stream: the stream may be structured and there is usually the need for associated metadata

Often we need more than just a byte stream, for example:

*   We may want the stream to be structured: if it is a CSV file we’re opening we’d like to get a stream of row objects not just a stream of bytes
*   We may want file metadata to be available (where did the file come from, how big is the file, when was it last modified)
*   We may want schema information: not just the CSV file but type information on its columns (this would allow us to reliably cast the CSV data to proper types when reading)
*   And we may even want to add metadata ourselves (perhaps automatedly), for example guessing the types of the columns in a CSV

A file is more than a byte stream: the stream may be structured and there is usually the need for associated metadata, at a minimum the name and size of the file but also extending to things like a file schema.

Tool authors find themselves inventing their own “stream-plus-metadata” objects … but they are all different

Tool authors find themselves inventing their own file-like “stream-plus-metadata” objects to describe the files they open.

Note: Many languages have “file-like” object that usually consists of a stream plus some metadata (e.g. python `file` object, Node Streams etc). But this is not standardized and is often inadequate so tool makers end up wrapping or replacing it.

This is not just about opening files but about passing streams around with because most tools, even very simple ones, start to contain implicit mini data pipelines:

♦

These stream-plus-metadata objects contain implicit mini-metadata standards for describing files and collections of files (“datasets”). These mini-metadata standards look like Data Resource, Table Schema, Data Package etc.

But these stream-plus-metadata objects and their mini-metadata are all a little different across the various languages and tools.

Plus, many tools also need to access collection of files, i.e. datasets

Many tools want to access collections of files e.g. datasets:

    dataset = open(/path/to/dataset)

Datasets already require some structure to list their collection of files and usually require some additional metadata ranging from where the dataset was loaded from to items such as its license.

You can even have datasets without multiple files when the file you are using is implicitly a dataset. For example, an Excel file is really a dataset if you think of each sheets as a separate file stream or think of an sqlite database.

Having a common API pattern for files (stream-plus-metadata) and datasets would reduce duplication and support plug and play with tooling

Standardizing the structure of these stream-plus-metadata file objects (and dataset objects), and building standard libraries to create them from file/dataset pointers would:

*   Reduce repetition / allow for reuse across tools: at present, data wrangling tool write this themselves. They now have a common pattern and may even be able to use a common underlying library.
*   Support plug and play: new wrangling can operate on these standard file and dataset objects. For example, an inference library that given a file object returns an inferred schema, or a converter that converts xls => csv.

Appendix: Design Principles

The pattern is based on the following principles:

*   Data wrangler focused: focus on the core data wrangler workflow: open a file and do something with it
*   Zen-like: Simplicity and power. As simple as possible: does just what it needs and no more.
*   Use Streams: stream focused library, including object streams.

Orient to the data wrangler workflow

See motivation section above

*   Open => Read / Stream
*   [optional] Inspect
*   Check
*   Operate on
*   Write

Zen - maximum viable simplicity

As simple as possible. Does just what it needs and no more. Simple and powerful.

Zen =>

*   “thin” (vs fat) objects: all complex operators such as infer or dump operate on objects rather than becoming part of them
*   a single open method to get data (file or dataset)
*   hide metadata by default (data package, data resource etc are in the background)

Core objects should be kept as simple as possible (and no simpler)

=> Inversion of control where possible so that we don’t end up with “fat” core classes e.g.

A. save data to disk should be separate objects that operate on the main objects rather than built into them e.g.

    const writer = CSVWriter()
    writer.writer(dataLibFileObjectInstance, filePath, [options])

rather than e.g.

    dataLibFileObjectInstance.saveToCsv(filePath)

If there is a simple way to invert dependency (i.e. not have all different dumper in main lib) but have a simple save method that would be fine.

B. Similarly for parsers (though reading is so essential that read needs to be part of of the class)

C. infer, validate etc should operate on Files rather than be part of it …

    const tableschema = infer(fileObj)

Rather than

    fileObj.inferSchema()

Use Streams

Streams are the natural way to handle data and it scales to large datasets.

The library should be stream focused library, including object streams.

Appendix: Internal Library Structure Suggestions

These are some suggestions for how implementors could structure their library internally. They are entirely optional.

Library Components

In top level library just have Dataset and File (+ TabularFile)

    graph TD

    Dataset[Dataset/Package] --> File[File/Resource]
    File --> TabularFile
    TabularFile -.-> TableSchema
    TableSchema -.-> Field

    parsers((Parsers))
    dumpers((Writers))
    tools((Tools))

    tools --> infer
    infer --> validate

    classDef medium fill:lightblue,stroke:#333,stroke-width:4px;

    graph TD

    parsers((Parsers))
    dumpers((Writers))

    subgraph Parsers - Tabular
      csv["CSV parse(resource) -> row stream"]
      xls["XLS ..."]
    end

    subgraph Writers - Tabular
      ascii
      csvdump[CSV]
      xlsdump[XLS]
      markdown
    end

    parsers --> csv
    parsers --> xls

    dumpers --> ascii
    dumpers --> markdown
    dumpers --> xlsdump
    dumpers --> csvdump

Streams

    graph LR

    in1[File,URL,Stream] -- stream--> stream[Byte Stream + Meta]
    stream --parse--> objstream[Obj Stream+Meta]
    objstream --unparse--> stream2[Byte Stream + Meta]
    stream --write--> out
    stream2 --writer--> out[file/stream]

open => yields descriptor and file stream parse => yields file rows (internally uses parsers) writer => writers

    // aka write
    writer(File) => readable stream

    parser(File) => object stream

Loaders/Parsers and Writers

Loaders/Parsers and Writers should be be an extensible list.

Inversion of control is important: the core library does not depend directly on parsers (that way we can hot swap and/or extend the list at runtime).

Parsers:

    // file is a data.File object
    parse(file) => row stream

Writers are similar:

    // e.g. csv.js

    // dump to CSV file
    write(file, path) => null

Note we may want a writer for datasets as well e.g. a writer to datapackage.json or to sql or …

    write(dataset, destination ...)

Appendix: API with Data Package terminology

In progress

    // data.js is just an illustrative name for the library
    var data = require('data.js')

    // path can be local or remote
    const resource = data.open(pathOrUrl)

    // a byte stream
    resource.stream()

    // if this file is tabular this will give me a row stream (iterator)
    resource.rows()

For packages

    // path or url to a directory (or datapackage.json)
    const package = data.open(pathOrUrl)

    // list of files
    package.resources

    // readme (if README.md exists or there is a description in the metadata)
    package.readme

    // any metadata (either inferred or from datapackage.json)
    package.descriptor

Appendix: Connection with Frictionless Data

I’ve distilled this pattern out of the work of myself and others who have worked on Frictionless Data specs and tooling.

It is motivated by the following observations about the Data Package suite of libraries and their Table Schema, Data Resource and Data Package interfaces:

1.  These libraries contain functions and metadata that standardize operations that are common to almost all data wrangling tools because almost all data wrangling tools need to handle files/streams and datasets and the core metadata is designed around describing files and datasets – or inferring and validating that.
2.  BUT: by presenting the underlying metadata such as Data Resource, Data Package front and centre and hiding the common operations (e.g. open this file) they make a rather unnatural interface for data wranglers.
3.  Most data wranglers start from an immediate need: display this csv on the command line, convert this excel file to csv etc. At the simplest, most data wrangling tools need some function like `open(file) => file-like object` where the file-like object can be be used for other tasks

> Metaphorically: the current data package libraries put the skeleton (the metadata) “on the outside” and the “flesh” (the actual methods wranglers want to use) on the “inside” (they are implicit or hidden with the overall library)

What follows from this insight is that we should invert this:

*   “Put the flesh on the outside”: Create a simple interface that addresses the common needs of data wranglers and data wrangler tooling e.g. `open(file)`
*   Put the bones on the inside: leverage the Frictionless Data metadata structures but put them on the inside, out of sight but still available if needed.

Note: it may be appropriate to continue to have a dedicated Data Package or Table Schema library but keep it really simple

Here’s how I put this in the original issue github.com/frictionlessdata/tableschema-js/issues/78:

> People don’t care about Data Packages / Resources, they care about opening a data file and doing something with it
> 
> > Data Packages / Resources come up because they are a nicely agreed metadata structure for all the stuff that comes up in the background when you do that.
> 
> Put crudely: Most people are doing stuff with a file (or dataset), and they want to grab it and read it preferably in a structured way e.g. as a row iterator – sometimes inferring or specifying stuff a long the way e.g. encoding, formatting, field types.
> 
> => Our job is to help users to open that file (or dataset) and stream it as quickly as possible.

Recommendations for Frictionless Data community

Suggestions:

*   Users want to do stuff with data fast. This implies that a library like tabulator is more immediately appropriate to end users than data-package or table-schema
*   The current set of FD libraries is bewildering and confusing, especially for new users. There are several complementary libraries plus some of the API is pretty confusing (see appendix for more on this)

Recommendations:

*   Have a primary “gateway” library oriented around reading and writing data and datasets.
*   This can be based around a simplified Package and Resource interface and library
    *   Move auxiliary functionality to libraries e.g. infer,
    *   Make parsers / loaders (and writers) to a plugin model so this can be extended easily
*   Consider renaming Package and Resource to Dataset and File in the simple library as these are more accessible and common terms

Why do it?

*   Massively grow the potential audience: Create an interface non-DP fanatics can use and want to use (and DP ones too)
*   Ease of use: easier for us and others to use
*   Elegance: do it right - this is the elegant, functional, beautiful way to do this library

Relation to Data Packages

*   We use Data Package and Table Schema as the metadata model for data files and datasets
*   Data Package libraries already implement APIs a bit like this and support many features we want (e.g. infer)

</div>

<div class="item-footer">Published 00:00 • over a year ago</div>

</div>

</article>


## Welcome to GitHub Pages

You can use the [editor on GitHub](https://github.com/MarioBarbarino/MarioBarbarino.github.io/edit/master/index.md) to maintain and preview the content for your website in Markdown files.

Whenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.

### Markdown

Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/MarioBarbarino/MarioBarbarino.github.io/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://docs.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we’ll help you sort it out.
